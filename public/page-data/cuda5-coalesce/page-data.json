{"componentChunkName":"component---src-templates-blog-post-js","path":"/cuda5-coalesce/","result":{"data":{"site":{"siteMetadata":{"title":"FANG'S NOTEBOOK","author":"Fang Cabrera"}},"markdownRemark":{"id":"24b545e9-74b6-5be9-8d6a-f33611aa8b5e","excerpt":"This post talks about a key factor to CUDA kernel performace: accessing data in the globle memory. CUDA applications tend to process a‚Ä¶","html":"<p>This post talks about a key factor to CUDA kernel performace: accessing data in the globle memory.</p>\n<p>CUDA applications tend to process a massive amount of data from the global memory within a short period of time.</p>\n<p><strong>Tiling</strong> techniques are engineered that utilize <strong>shared memories</strong> to reduce the total amount of data that must be acessed from the global memory (read about tiling techniques here <a href=\"/cuda6-tiling\">The CUDA Parallel Programming Model - 6.Tiling</a>).</p>\n<p>I this post we talk about <strong>memory coalescing</strong> techniques that can more effectively move data from the global memory into <strong>shared memories and registers</strong>.</p>\n<p>Memory coalescing techniques are often used <em>in conjunction with tiling techniques</em> to allow CUDA devices to reach their performance potential by more efficiently utilizing the global memory bandwidth.</p>\n<h2>Global Memory Bandwidth</h2>\n<p>The global memory of a CUDA device is implemented with DRAMs.</p>\n<h4>DRAM is slow</h4>\n<p>Data bits are stored in DRAM cells that are small capacitors, where the presence or absence of a <em>tiny amount of electrical charge</em> distinguishes between 0 and 1.</p>\n<p>Reading data from a DRAM cell requires the small capacitor to use its tiny electrical charge to drive a highly capacitive line leading to a sensor and set off its detection mechanism that determines whether a sufficient amount of charge is present in the capacitor to qualify as a ‚Äú1‚Äù. This process takes 10 s of nanoseconds in modern DRAM chips. <strong>This is in sharp contrast with the sub-nanosecond clock cycle time of modern computing devices</strong>.</p>\n<h4>parallelism and memory access throughput</h4>\n<p>Because this is a very slow process relative to the desired data access speed (sub-nanosecond access per byte), modern DRAMs <strong>use parallelism to increase their rate of data access</strong>, commonly referred to as <em>memory access throughput</em>.</p>\n<h4>DRAM bursts</h4>\n<p>Each time a DRAM location is accessed, <strong>a range of consecutive locations that includes the requested location are actually accessed</strong>.</p>\n<p>Many sensors are provided in each DRAM chip and they work in parallel. Each senses the content of a bit within these consecutive locations.</p>\n<p>Once detected by the sensors, the data from all these consecutive locations can be transferred at very high-speed to the processor. These consecutive locations accessed and delivered are referred to as <strong>DRAM bursts</strong>.</p>\n<h4>motivation</h4>\n<p>If an application makes focused use of data from these bursts, the DRAMs can supply the data at a much higher rate than if a truly random sequence of locations were accessed.</p>\n<h2>Memory Coalescing</h2>\n<p>Current CUDA devices employ a technique that allows the programmers to achieve high global memory access efficiency by <strong>organizing memory accesses of threads into favorable patterns</strong>.</p>\n<h3>how?</h3>\n<ul>\n<li>This technique takes advantage of the fact that <strong>threads in a warp execute the same instruction at any given point in time</strong>.</li>\n<li>The most favorable access pattern is achieved when all threads in a warp access consecutive global memory locations.</li>\n<li>When all threads in a warp execute a load instruction, the hardware detects whether they access consecutive global memory locations. If that‚Äôs the case, the hardware combines (<strong>coalesces</strong>) all these accesses into a consolidated access to consecutive DRAM locations.</li>\n<li>For example, for a given load instruction of a warp, if thread 0 accesses global memory location N2, thread 1 location N+1, thread 2 location N+2, and so on, all these accesses will be coalesced into a single request for consecutive locations when accessing the DRAMs.</li>\n<li>Such coalesced access allows the DRAMs to deliver data as a burst.</li>\n</ul>\n<h3>matrix multiplication example</h3>\n<p>Recall from <a href=\"/cuda2-warp\">The CUDA Parallel Programming Model - 2. Warps</a> that multidimensional array elements in CUDA are placed into the linearly addressed memory space according to the <strong>row-major</strong> convention.</p>\n<p>Say we have a kernel that computes <code class=\"language-text\">M x N</code>, where both M and N are 2D row-major array.</p>\n<p>Each thread accesses a row of the M array (matrix A below) and a column of the N array (matrix B below).</p>\n<h3>IMPORTANT: hope this helps with understanding the patterns below</h3>\n<p>Coalesce happens amongst threads, not amongst different iterations of the loop within each thread‚Äôs execution.</p>\n<p>You might be thinking: for matrix M in the example below, each thread will read the whole row within its for-loop. But it‚Äôs not how the coalesce hardware works here.</p>\n<p>Because:</p>\n<ul>\n<li>if you look across the threads in matrix M, they don‚Äôt share row accesses at all.</li>\n<li>whereas for matrix N, each thread at iteration 0 combined will access the entire row 0.</li>\n</ul>\n<p>Since all threads within a warp executes the same instruction, they all execute the same iteration in the loop at any time. So it doesn‚Äôt matter if a thread reads through an entire row during its lifetime. What matters is that all the threads of a warp can be coalesced during each (collected) memory access.</p>\n<h4>patterns</h4>\n<p><figure class=\"gatsby-resp-image-figure\" style=\"\">\n    <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 590px;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/d3f4c1e8944826ac08d7b974ad9718b8/93f40/pattern.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 46.759847522236335%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAJABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAABQABA//EABUBAQEAAAAAAAAAAAAAAAAAAAEC/9oADAMBAAIQAxAAAAFLgnsA8vB//8QAGxAAAgIDAQAAAAAAAAAAAAAAAAECMQMREhP/2gAIAQEAAQUCzuRuXn1MdCr/xAAVEQEBAAAAAAAAAAAAAAAAAAAAEf/aAAgBAwEBPwFH/8QAFxEBAAMAAAAAAAAAAAAAAAAAAAECEv/aAAgBAgEBPwGasP/EABcQAAMBAAAAAAAAAAAAAAAAAAABMSD/2gAIAQEABj8CRXSvH//EABoQAAEFAQAAAAAAAAAAAAAAAAABETFBcfD/2gAIAQEAAT8hRXTDoKG4CQojw//aAAwDAQACAAMAAAAQa/8A/8QAFhEBAQEAAAAAAAAAAAAAAAAAAAFx/9oACAEDAQE/ELGn/8QAFxEAAwEAAAAAAAAAAAAAAAAAAAERUf/aAAgBAgEBPxBjdpWn/8QAGxABAAMAAwEAAAAAAAAAAAAAAQARMSFxocH/2gAIAQEAAT8QBo62LhyF6hzwuRXCr2k8DPmeFP/Z'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"access pattern\"\n        title=\"access pattern\"\n        src=\"/static/d3f4c1e8944826ac08d7b974ad9718b8/88218/pattern.jpg\"\n        srcset=\"/static/d3f4c1e8944826ac08d7b974ad9718b8/7237a/pattern.jpg 148w,\n/static/d3f4c1e8944826ac08d7b974ad9718b8/0cfdf/pattern.jpg 295w,\n/static/d3f4c1e8944826ac08d7b974ad9718b8/88218/pattern.jpg 590w,\n/static/d3f4c1e8944826ac08d7b974ad9718b8/93f40/pattern.jpg 787w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span>\n    <figcaption class=\"gatsby-resp-image-figcaption\">access pattern</figcaption>\n  </figure></p>\n<h4><strong>M: unfavorable data access pattern</strong></h4>\n<ul>\n<li>fig.(A) above illustrates the data access pattern of the M array</li>\n<li>threads in a warp read adjacent rows</li>\n<li>during iteration 0, threads in a warp read <strong>element 0</strong> of rows 0 through 31.</li>\n<li>during iteration 1, these same threads read <strong>element 1</strong> of rows 0 through 31.</li>\n<li><strong>None</strong> of the accesses will be coalesced.</li>\n</ul>\n<h4><strong>N: favorable data access pattern</strong></h4>\n<ul>\n<li>fig.(B) above illustrates the data access pattern of the N array</li>\n<li>each thread reads a column of N.</li>\n<li>during iteration 0, threads in warp 0 read <strong>element 1</strong> of columns 0 through 31.</li>\n<li>all these accesses will be coalesced.</li>\n</ul>\n<p>If the above doesn‚Äôt make sense to you üßê, read the post on the matrix application kernel here: <a href=\"/cuda7-matrixmult\">CUDA Programming Examples - 1. Matrix Multiplication</a></p>\n<h4>a coalesced access pattern ‚Äî e.g. N</h4>\n<p><figure class=\"gatsby-resp-image-figure\" style=\"\">\n    <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 590px;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/5e76a9b8f6899418b29b8cc01a01ba39/423f5/coalescedP.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 60.55194805194806%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAMABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAQFAv/EABUBAQEAAAAAAAAAAAAAAAAAAAIA/9oADAMBAAIQAxAAAAG0vhIq8RyH/8QAGxAAAgIDAQAAAAAAAAAAAAAAAgMBFAAQESH/2gAIAQEAAQUCL2ELMNNORG0ztg8//8QAFhEBAQEAAAAAAAAAAAAAAAAAEQAB/9oACAEDAQE/ATSb/8QAFhEAAwAAAAAAAAAAAAAAAAAAAAIR/9oACAECAQE/AVjFP//EABsQAAEFAQEAAAAAAAAAAAAAAAABERIhQQIQ/9oACAEBAAY/AlZcFn3LyhqMP//EABwQAAMAAQUAAAAAAAAAAAAAAAABESEQMWFxkf/aAAgBAQABPyHGQbhDpkI9tK+LSOaOjl8H/9oADAMBAAIAAwAAABDA7//EABYRAQEBAAAAAAAAAAAAAAAAABEAQf/aAAgBAwEBPxAFkL//xAAXEQEBAQEAAAAAAAAAAAAAAAABABEx/9oACAECAQE/EEuhyzf/xAAaEAEAAgMBAAAAAAAAAAAAAAABESEAEDFR/9oACAEBAAE/EDbKSTA+4foQhNJ3SysBZOAQtGuf/9k='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"a coalesced access pattern\"\n        title=\"a coalesced access pattern\"\n        src=\"/static/5e76a9b8f6899418b29b8cc01a01ba39/88218/coalescedP.jpg\"\n        srcset=\"/static/5e76a9b8f6899418b29b8cc01a01ba39/7237a/coalescedP.jpg 148w,\n/static/5e76a9b8f6899418b29b8cc01a01ba39/0cfdf/coalescedP.jpg 295w,\n/static/5e76a9b8f6899418b29b8cc01a01ba39/88218/coalescedP.jpg 590w,\n/static/5e76a9b8f6899418b29b8cc01a01ba39/77d57/coalescedP.jpg 885w,\n/static/5e76a9b8f6899418b29b8cc01a01ba39/5a917/coalescedP.jpg 1180w,\n/static/5e76a9b8f6899418b29b8cc01a01ba39/423f5/coalescedP.jpg 1232w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span>\n    <figcaption class=\"gatsby-resp-image-figcaption\">a coalesced access pattern</figcaption>\n  </figure></p>\n<ul>\n<li>The arrow in the top portion of the figure shows the access pattern of the kernel code.</li>\n</ul>\n<p>Recall the for-loop in the kernel:</p>\n<div class=\"gatsby-highlight\" data-language=\"c\"><pre class=\"language-c\"><code class=\"language-c\">__global__ <span class=\"token keyword\">void</span> MatrixMulKernel <span class=\"token punctuation\">(</span><span class=\"token keyword\">float</span><span class=\"token operator\">*</span> M<span class=\"token punctuation\">,</span> <span class=\"token keyword\">float</span><span class=\"token operator\">*</span> N<span class=\"token punctuation\">,</span> <span class=\"token keyword\">float</span><span class=\"token operator\">*</span> P<span class=\"token punctuation\">,</span> <span class=\"token keyword\">int</span> Width<span class=\"token punctuation\">)</span><span class=\"token punctuation\">{</span>\n    <span class=\"token comment\">// calculate the row index of the P element and M</span>\n    <span class=\"token keyword\">int</span> Row <span class=\"token operator\">=</span> blockIdx<span class=\"token punctuation\">.</span>y <span class=\"token operator\">*</span> blockDim<span class=\"token punctuation\">.</span>y <span class=\"token operator\">+</span> threadIdx<span class=\"token punctuation\">.</span>y<span class=\"token punctuation\">;</span>\n    <span class=\"token comment\">// calculate the col index of the P element and N</span>\n    <span class=\"token keyword\">int</span> Col <span class=\"token operator\">=</span> blockIdx<span class=\"token punctuation\">.</span>x <span class=\"token operator\">*</span> blockDim<span class=\"token punctuation\">.</span>x <span class=\"token operator\">+</span> threadIdx<span class=\"token punctuation\">.</span>x<span class=\"token punctuation\">;</span>\n\n    <span class=\"token keyword\">if</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>Row <span class=\"token operator\">&lt;</span> Width<span class=\"token punctuation\">)</span> <span class=\"token operator\">&amp;&amp;</span> <span class=\"token punctuation\">(</span>Col <span class=\"token operator\">&lt;</span> Width<span class=\"token operator\">></span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">{</span>\n        <span class=\"token keyword\">float</span> Pvalue <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span>\n        <span class=\"token comment\">//each thread computes one element of the block sub-matrix</span>\n        <span class=\"token keyword\">for</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> k <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> k <span class=\"token operator\">&lt;</span> Width<span class=\"token punctuation\">;</span> k<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span>\n            Pvalue <span class=\"token operator\">+=</span> M<span class=\"token punctuation\">[</span>Row<span class=\"token operator\">*</span>Width <span class=\"token operator\">+</span> k<span class=\"token punctuation\">]</span> <span class=\"token operator\">*</span> N<span class=\"token punctuation\">[</span>k<span class=\"token operator\">*</span>Width <span class=\"token operator\">+</span> Col<span class=\"token punctuation\">]</span><span class=\"token punctuation\">;</span>\n        <span class=\"token punctuation\">}</span>\n        P<span class=\"token punctuation\">[</span>Row <span class=\"token operator\">*</span> Width <span class=\"token operator\">+</span> Col<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> Pvalue<span class=\"token punctuation\">;</span>\n    <span class=\"token punctuation\">}</span>\n<span class=\"token punctuation\">}</span></code></pre></div>\n<ul>\n<li>Within a given iteration of the <code class=\"language-text\">k</code> loop, the <code class=\"language-text\">k*Width</code> value is the same across all threads. Recall that <code class=\"language-text\">Col=blockIdx.x*blockDim.x+threadIdx.x</code>.</li>\n<li>Since the value of blockIndx.x and blockDim.x are of the same value for all threads in the same block, the only part of <code class=\"language-text\">k*Width+Col</code> that varies across a thread block is threadIdx.x.</li>\n<li>Since <strong>adjacent threads</strong> have consecutive threadIdx.x values, their accessed elements will have consecutive addresses.</li>\n</ul>\n<p>In the pictured example, we are using only 1 block to calculate the entire P matrix, where:</p>\n<ul>\n<li>block size: 4 * 4</li>\n<li>warp size: 4</li>\n<li><code class=\"language-text\">Width = 4</code></li>\n<li><code class=\"language-text\">blockDim.x = 4</code></li>\n<li><code class=\"language-text\">blockIdx.x = 0</code></li>\n<li>\n<p>In <code class=\"language-text\">iteration 0</code>, the k value is 0. The index used by each thread for accessing N is</p>\n<div class=\"gatsby-highlight\" data-language=\"c\"><pre class=\"language-c\"><code class=\"language-c\">N<span class=\"token punctuation\">[</span>k <span class=\"token operator\">*</span> Width <span class=\"token operator\">+</span> Col<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> N <span class=\"token punctuation\">[</span>k <span class=\"token operator\">*</span> Width <span class=\"token operator\">+</span> blockIdx<span class=\"token punctuation\">.</span>x <span class=\"token operator\">*</span> blockDim<span class=\"token punctuation\">.</span>x <span class=\"token operator\">+</span> threadIdx<span class=\"token punctuation\">.</span>x<span class=\"token punctuation\">]</span>\n                 <span class=\"token operator\">=</span>N<span class=\"token punctuation\">[</span><span class=\"token number\">0</span> <span class=\"token operator\">*</span> <span class=\"token number\">4</span> <span class=\"token operator\">+</span> <span class=\"token number\">0</span> <span class=\"token operator\">*</span> <span class=\"token number\">4</span> <span class=\"token operator\">+</span> threadidx<span class=\"token punctuation\">.</span>x<span class=\"token punctuation\">]</span>\n                 <span class=\"token operator\">=</span>N<span class=\"token punctuation\">[</span>threadIdx<span class=\"token punctuation\">.</span>x<span class=\"token punctuation\">]</span></code></pre></div>\n<p>That is, within this thread <strong>block</strong>, the index for accessing N is simply the value of threadIdx.x. The N elements accessed by <code class=\"language-text\">T0, T1, T2, T3</code> are <code class=\"language-text\">N[0], N[1], N[2], and N[3]</code>. This is illustrated with the <code class=\"language-text\">‚ÄúLoad iteration 0‚Äù</code> box of the above figure.</p>\n<p>These elements are in consecutive locations in the global memory. The hardware detects that <strong>these accesses are made by threads in a warp and to consecutive locations in the global memory</strong>. It coalesces these accesses into a consolidated access. This allows the DRAMs to supply data at a high rate.</p>\n</li>\n<li>\n<p>In <code class=\"language-text\">iteration 1</code>, similarly:</p>\n<div class=\"gatsby-highlight\" data-language=\"c\"><pre class=\"language-c\"><code class=\"language-c\">N<span class=\"token punctuation\">[</span>k <span class=\"token operator\">*</span> Width <span class=\"token operator\">+</span> Col<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> N<span class=\"token punctuation\">[</span>k <span class=\"token operator\">*</span> Width <span class=\"token operator\">+</span> blockIdx<span class=\"token punctuation\">.</span>x <span class=\"token operator\">*</span> blockDim<span class=\"token punctuation\">.</span>x <span class=\"token operator\">+</span> threadIdx<span class=\"token punctuation\">.</span>x<span class=\"token punctuation\">]</span>\n                 <span class=\"token operator\">=</span> N<span class=\"token punctuation\">[</span><span class=\"token number\">1</span> <span class=\"token operator\">*</span> <span class=\"token number\">4</span> <span class=\"token operator\">+</span> <span class=\"token number\">0</span> <span class=\"token operator\">*</span> <span class=\"token number\">4</span> <span class=\"token operator\">+</span> threadidx<span class=\"token punctuation\">.</span>x<span class=\"token punctuation\">]</span>\n                 <span class=\"token operator\">=</span> N<span class=\"token punctuation\">[</span><span class=\"token number\">4</span> <span class=\"token operator\">+</span> threadIdx<span class=\"token punctuation\">.</span>x<span class=\"token punctuation\">]</span></code></pre></div>\n<p>The N elements accessed by <code class=\"language-text\">T0, T1, T2, T3</code> in this iteration are <code class=\"language-text\">N[5], N[6], N[7], and N[8]</code>, as shown with the ‚ÄúLoad iteration 1‚Äù box.</p>\n</li>\n</ul>\n<h4>an un-coalesced access pattern ‚Äî e.g. M</h4>\n<p><figure class=\"gatsby-resp-image-figure\" style=\"\">\n    <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 590px;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/7670edec83ba2cfd4fe413f0e10b2e42/a0f91/uncoalescedP.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 76.91056910569105%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAPABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAMEBf/EABUBAQEAAAAAAAAAAAAAAAAAAAEC/9oADAMBAAIQAxAAAAHehcuW0lGf/8QAGhAAAgIDAAAAAAAAAAAAAAAAAgMAARETFP/aAAgBAQABBQKIMyOEwarpVneuf//EABcRAQADAAAAAAAAAAAAAAAAAAABESH/2gAIAQMBAT8B1cP/xAAYEQACAwAAAAAAAAAAAAAAAAAAEQECMf/aAAgBAgEBPwGFbBn/xAAaEAACAgMAAAAAAAAAAAAAAAAAAQIREDEy/9oACAEBAAY/AhqUKxbZ0bP/xAAcEAEAAgEFAAAAAAAAAAAAAAABABEhEDFRkaH/2gAIAQEAAT8hUqOANbumBrPEQrbphRnyz//aAAwDAQACAAMAAAAQqy//xAAYEQADAQEAAAAAAAAAAAAAAAAAAREhYf/aAAgBAwEBPxCIrMOx/8QAFxEAAwEAAAAAAAAAAAAAAAAAAAERUf/aAAgBAgEBPxBmVqRp/8QAFxABAQEBAAAAAAAAAAAAAAAAAREAIf/aAAgBAQABPxAiFOmBl2gQWmpkCaiCcoVpA0cUuv/Z'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"an un-coalesced access pattern\"\n        title=\"an un-coalesced access pattern\"\n        src=\"/static/7670edec83ba2cfd4fe413f0e10b2e42/88218/uncoalescedP.jpg\"\n        srcset=\"/static/7670edec83ba2cfd4fe413f0e10b2e42/7237a/uncoalescedP.jpg 148w,\n/static/7670edec83ba2cfd4fe413f0e10b2e42/0cfdf/uncoalescedP.jpg 295w,\n/static/7670edec83ba2cfd4fe413f0e10b2e42/88218/uncoalescedP.jpg 590w,\n/static/7670edec83ba2cfd4fe413f0e10b2e42/77d57/uncoalescedP.jpg 885w,\n/static/7670edec83ba2cfd4fe413f0e10b2e42/5a917/uncoalescedP.jpg 1180w,\n/static/7670edec83ba2cfd4fe413f0e10b2e42/a0f91/uncoalescedP.jpg 1230w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span>\n    <figcaption class=\"gatsby-resp-image-figcaption\">an un-coalesced access pattern</figcaption>\n  </figure></p>\n<ul>\n<li>The arrow in the top portion of the figure shows that the kernel code for <strong>each thread</strong> accesses elements of a row in sequence</li>\n<li>block size: 4 * 4</li>\n<li>warp size: 4</li>\n<li><code class=\"language-text\">Width = 4</code></li>\n<li><code class=\"language-text\">blockDim.y = 4</code></li>\n<li><code class=\"language-text\">blockIdx.y = 0</code></li>\n<li>\n<p>The index used by each thread for accessing M is:</p>\n<div class=\"gatsby-highlight\" data-language=\"c\"><pre class=\"language-c\"><code class=\"language-c\">M<span class=\"token punctuation\">[</span>Row <span class=\"token operator\">*</span> Width <span class=\"token operator\">+</span> k<span class=\"token punctuation\">]</span>  <span class=\"token operator\">=</span>M<span class=\"token punctuation\">[</span><span class=\"token punctuation\">(</span>blockIdx<span class=\"token punctuation\">.</span>y <span class=\"token operator\">*</span> blockDim<span class=\"token punctuation\">.</span>y <span class=\"token operator\">+</span> threadIdx<span class=\"token punctuation\">.</span>y<span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> Width <span class=\"token operator\">+</span> k<span class=\"token punctuation\">]</span>\n\n                  <span class=\"token operator\">=</span>M<span class=\"token punctuation\">[</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span><span class=\"token number\">0</span> <span class=\"token operator\">*</span> <span class=\"token number\">4</span><span class=\"token operator\">+</span>  threadIdx<span class=\"token punctuation\">.</span>y<span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> <span class=\"token number\">4</span> <span class=\"token operator\">+</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span>\n\n                  <span class=\"token operator\">=</span>M<span class=\"token punctuation\">[</span>threadIdx<span class=\"token punctuation\">.</span>x <span class=\"token operator\">*</span> <span class=\"token number\">4</span><span class=\"token punctuation\">]</span></code></pre></div>\n</li>\n<li>The M elements accessed by <code class=\"language-text\">T0, T1, T2, T3</code> are <code class=\"language-text\">M[0], M[4], M[8], and M[12]</code>, as shown with the ‚ÄúLoad iteration 0‚Äù box in the above figure. These elements are not in consecutive locations in the global memory. The hardware cannot coalesce these accesses into a consolidated access.</li>\n</ul>\n<h2>Corner Turning</h2>\n<p>What if we have to iterate through data along the row direction?</p>\n<ul>\n<li>We can use the shared memory to enable memory coalescing ‚Äî this is the technique called corner turning</li>\n</ul>\n<h3>how?</h3>\n<ul>\n<li>Use a tiled algorithm</li>\n<li>Threads of a block can first cooperatively load the tiles into the shared memory</li>\n<li>Care must be taken to ensure that these tiles are loaded in a coalesced pattern</li>\n<li>Once the data is in shared memory, they can be accessed either on a row basis or a column basis with much less performance variation because the shared memories are implemented as intrinsically high-speed on-chip memory that does not require coalescing to achieve high data access rate.</li>\n<li>A tiled matrix multiplication kernel is shown below:\n<figure class=\"gatsby-resp-image-figure\" style=\"\">\n    <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 590px;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/01f05364f804fa8088510ebf2c481713/2d3f1/tiled.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 77.98232695139912%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAQABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAQAF/8QAFAEBAAAAAAAAAAAAAAAAAAAAAP/aAAwDAQACEAMQAAAB3URqP//EABQQAQAAAAAAAAAAAAAAAAAAACD/2gAIAQEAAQUCX//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABQQAQAAAAAAAAAAAAAAAAAAACD/2gAIAQEABj8CX//EABgQAAIDAAAAAAAAAAAAAAAAAAAQARFB/9oACAEBAAE/ISFpS//aAAwDAQACAAMAAAAQs8//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAZEAEBAQEBAQAAAAAAAAAAAAABABEhMVH/2gAIAQEAAT8QfYd7YfCTV2KGX//Z'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"tiled matrix multiplication kernel\"\n        title=\"tiled matrix multiplication kernel\"\n        src=\"/static/01f05364f804fa8088510ebf2c481713/88218/tiled.jpg\"\n        srcset=\"/static/01f05364f804fa8088510ebf2c481713/7237a/tiled.jpg 148w,\n/static/01f05364f804fa8088510ebf2c481713/0cfdf/tiled.jpg 295w,\n/static/01f05364f804fa8088510ebf2c481713/88218/tiled.jpg 590w,\n/static/01f05364f804fa8088510ebf2c481713/77d57/tiled.jpg 885w,\n/static/01f05364f804fa8088510ebf2c481713/5a917/tiled.jpg 1180w,\n/static/01f05364f804fa8088510ebf2c481713/2d3f1/tiled.jpg 1358w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span>\n    <figcaption class=\"gatsby-resp-image-figcaption\">tiled matrix multiplication kernel</figcaption>\n  </figure></li>\n</ul>","fields":{"readingTime":{"text":"9 min read"}},"frontmatter":{"title":"The CUDA Parallel Programming Model - 5. Memory Coalescing","date":"December 04, 2019"}}},"pageContext":{"isCreatedByStatefulCreatePages":false,"slug":"/cuda5-coalesce/","previous":{"fields":{"slug":"/cuda4-sync/"},"frontmatter":{"title":"The CUDA Parallel Programming Model - 4. Syncthreads Examples"}},"next":{"fields":{"slug":"/cuda6-memoryparallel/"},"frontmatter":{"title":"The CUDA Parallel Programming Model - 6. More About Memory"}}}}}