{
  "componentChunkName": "component---src-pages-index-js",
  "path": "/",
  "result": {
    "data": {
      "site": { "siteMetadata": { "title": "FANG'S NOTEBOOK" } },
      "allMarkdownRemark": {
        "edges": [
          {
            "node": {
              "id": "c7e6f56c-f6b0-524a-84ee-ce38ea5f8f6b",
              "excerpt": "Have you wondered if it’s possible to launch nested kernels (i.e. a kernel calls another kernel) in CUDA? Well, this is where dynamic parallelism comes into…",
              "fields": {
                "slug": "/cudaRandom-dynamicParallelism/",
                "readingTime": { "text": "3 min read" }
              },
              "frontmatter": {
                "date": "December 11, 2019",
                "title": "CUDA Dynamic Parallelism"
              }
            }
          },
          {
            "node": {
              "id": "2d3e0bf2-6127-599a-8c91-852a076ccb1d",
              "excerpt": "Unified Virtual Address Space (UVA) From CUDA 4.0 and on, UVA has been an important feature. It puts all CUDA execution, host and GPUs, in the same address…",
              "fields": {
                "slug": "/cudaRandom-UVA/",
                "readingTime": { "text": "3 min read" }
              },
              "frontmatter": {
                "date": "December 11, 2019",
                "title": "CUDA Unified Virtual Address Space & Unified Memory"
              }
            }
          },
          {
            "node": {
              "id": "d56853dd-b714-5edf-bf93-48ee304619af",
              "excerpt": "In the fifth post of the CUDA series (The CUDA Parallel Programming Model - 5. Memory Coalescing), I put up a note on the effect of memory alignment on memory…",
              "fields": {
                "slug": "/cudaRandom-memAlign/",
                "readingTime": { "text": "3 min read" }
              },
              "frontmatter": {
                "date": "December 11, 2019",
                "title": "Memory Alignment For CUDA"
              }
            }
          },
          {
            "node": {
              "id": "2d3d8705-fd42-51f6-b5d7-079386488924",
              "excerpt": "I briefly talked about how CUDA processors hide long-latency operations such as global memory accesses through their warp-scheduling mechanism in The CUDA…",
              "fields": {
                "slug": "/cudaRandom-latency/",
                "readingTime": { "text": "3 min read" }
              },
              "frontmatter": {
                "date": "December 10, 2019",
                "title": "Recap: GPU Latency Tolerance and Zero-Overhead Thread-Scheduling"
              }
            }
          },
          {
            "node": {
              "id": "d767ac8b-bb59-5ac7-b111-da455c2cd7c6",
              "excerpt": "Motivated by a CUDA puzzle I tried to solve today, I’d like to talk more about resource assignment.  A Puzzle problem Adding two big arrays element-wise…",
              "fields": {
                "slug": "/cudaRandom-resourceAssignment/",
                "readingTime": { "text": "3 min read" }
              },
              "frontmatter": {
                "date": "December 09, 2019",
                "title": "Several Things About CUDA Resource Assignment"
              }
            }
          },
          {
            "node": {
              "id": "53baaded-057f-5df7-a592-c0173552cd00",
              "excerpt": "In this post, I talk about what happens when blocks are assigned to SMs, as well as CUDA code optimization across GPU architecture. Before this, I thought it…",
              "fields": {
                "slug": "/cudaRandom/",
                "readingTime": { "text": "10 min read" }
              },
              "frontmatter": {
                "date": "December 08, 2019",
                "title": "Some CUDA Related Questions"
              }
            }
          },
          {
            "node": {
              "id": "fd5baa6f-a8e0-5317-96a7-4a1c8a207829",
              "excerpt": "I recently had to pack a project with Reprozip where all the dependencies can be nicely preserved. Reprozip uses ptrace and thus only works on Linux, which…",
              "fields": {
                "slug": "/install-reprozip/",
                "readingTime": { "text": "3 min read" }
              },
              "frontmatter": {
                "date": "December 08, 2019",
                "title": "Packing Files With Reprozip On MacOS Via Vagrant"
              }
            }
          },
          {
            "node": {
              "id": "de652e40-e1a9-5cdd-869f-5c6923eb60c9",
              "excerpt": "CUDA Variable Type Qualifiers  lifetime == kernel? If the lifetime of a variable is within a kernel execution, it must be declared within the kernel function…",
              "fields": {
                "slug": "/cudaProg2-Variables/",
                "readingTime": { "text": "5 min read" }
              },
              "frontmatter": {
                "date": "December 07, 2019",
                "title": "CUDA Programming - 2. CUDA Variable Type Qualifiers"
              }
            }
          },
          {
            "node": {
              "id": "0844d5dd-0ad9-5c49-9f9a-c3f9bd37eaf9",
              "excerpt": "Notation Matrix  Matrix  Output Matrix  row counter:  column counter:   is the element at  position in the vertical direction and  position in the horizontal…",
              "fields": {
                "slug": "/cudaProg1-matrixmult/",
                "readingTime": { "text": "3 min read" }
              },
              "frontmatter": {
                "date": "December 07, 2019",
                "title": "CUDA Programming - 1. Matrix Multiplication"
              }
            }
          },
          {
            "node": {
              "id": "363685f3-c574-5ebf-b468-fbe9b7f005f8",
              "excerpt": "In the last post, we saw how full concurrency can be achieved amongst streams. Here I’d like to talk about how CUDA operations from different streams may also…",
              "fields": {
                "slug": "/cuda9-stream2/",
                "readingTime": { "text": "5 min read" }
              },
              "frontmatter": {
                "date": "December 06, 2019",
                "title": "The CUDA Parallel Programming Model - 9. Interleave Operations by Stream"
              }
            }
          },
          {
            "node": {
              "id": "559d891c-acfe-5642-ab56-0c862f25476e",
              "excerpt": "In the previous posts, we have sometimes assumed that only one kernel is launched at a time. But this is not all that kernels can do. They can be launched…",
              "fields": {
                "slug": "/cuda8-Stream/",
                "readingTime": { "text": "6 min read" }
              },
              "frontmatter": {
                "date": "December 06, 2019",
                "title": "The CUDA Parallel Programming Model - 8. Concurrency by Stream"
              }
            }
          },
          {
            "node": {
              "id": "444cfd98-bb37-592d-972d-7ff27a50935c",
              "excerpt": "There’s an intrinsic tradeoff in the use of device memories in CUDA: the global memory is large but slow, whereas the shared memory is small but fast. (To recap…",
              "fields": {
                "slug": "/cuda7-tiling/",
                "readingTime": { "text": "7 min read" }
              },
              "frontmatter": {
                "date": "December 06, 2019",
                "title": "The CUDA Parallel Programming Model - 7.Tiling"
              }
            }
          },
          {
            "node": {
              "id": "ad729c81-c4a4-5f4d-b769-d5875d86e57e",
              "excerpt": "The compute-to-global-memory-access ratio has major implications on the performance of a CUDA kernel. Programs whose execution speed is limited by memory access…",
              "fields": {
                "slug": "/cuda6-memoryparallel/",
                "readingTime": { "text": "6 min read" }
              },
              "frontmatter": {
                "date": "December 05, 2019",
                "title": "The CUDA Parallel Programming Model - 6. More About Memory"
              }
            }
          },
          {
            "node": {
              "id": "24b545e9-74b6-5be9-8d6a-f33611aa8b5e",
              "excerpt": "This post talks about a key factor to CUDA kernel performace: accessing data in the globle memory. CUDA applications tend to process a massive amount of data…",
              "fields": {
                "slug": "/cuda5-coalesce/",
                "readingTime": { "text": "9 min read" }
              },
              "frontmatter": {
                "date": "December 04, 2019",
                "title": "The CUDA Parallel Programming Model - 5. Memory Coalescing"
              }
            }
          },
          {
            "node": {
              "id": "863e4c7d-c15f-5dc9-ada5-e86eb540e778",
              "excerpt": "This is the fourth post in a series about what I learnt in my GPU class at NYU this past fall. Here I talk about barrier synchronization, how CUDA ensures the…",
              "fields": {
                "slug": "/cuda4-sync/",
                "readingTime": { "text": "4 min read" }
              },
              "frontmatter": {
                "date": "December 03, 2019",
                "title": "The CUDA Parallel Programming Model - 4. Syncthreads Examples"
              }
            }
          },
          {
            "node": {
              "id": "d1d0a483-646c-56db-bae5-fd42b2f06372",
              "excerpt": "This is the third post in a series about what I learnt in my GPU class at NYU this past fall. Here I dive a bit deeper than the previous post into thread…",
              "fields": {
                "slug": "/cuda3-thread-divergence/",
                "readingTime": { "text": "4 min read" }
              },
              "frontmatter": {
                "date": "December 02, 2019",
                "title": "The CUDA Parallel Programming Model - 3.               \n More On Thread Divergence"
              }
            }
          },
          {
            "node": {
              "id": "fa2ee5aa-dd46-59d1-9984-6289fedad9b8",
              "excerpt": "This is the second post in a series about what I learnt in my GPU class at NYU this past fall. This will be mostly about warps, why using warps from a SIMD…",
              "fields": {
                "slug": "/cuda2-warp/",
                "readingTime": { "text": "9 min read" }
              },
              "frontmatter": {
                "date": "December 01, 2019",
                "title": "The CUDA Parallel Programming Model - 2. Warps"
              }
            }
          },
          {
            "node": {
              "id": "e92f1d62-ed13-5144-a5b9-ea2222379d8d",
              "excerpt": "I took the Graphics Processing Units course at NYU this past fall. This is the first post in a series about what I learnt. Buckle up for lots of technical…",
              "fields": {
                "slug": "/cuda1/",
                "readingTime": { "text": "6 min read" }
              },
              "frontmatter": {
                "date": "November 30, 2019",
                "title": "The CUDA Parallel Programming Model - 1. Concepts"
              }
            }
          }
        ]
      }
    },
    "pageContext": { "isCreatedByStatefulCreatePages": true }
  }
}
