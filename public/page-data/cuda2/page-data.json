{"componentChunkName":"component---src-templates-blog-post-js","path":"/cuda2/","webpackCompilationHash":"","result":{"data":{"site":{"siteMetadata":{"title":"FANG'S NOTEBOOK","author":"Fang Cabrera"}},"markdownRemark":{"id":"a2697a56-2f90-5d4e-a20d-1300ed18020e","excerpt":"This is the second post in a series about what I learnt in my GPU class at NYU this past fall. This will be mostly about  warps and SIMDâ€¦","html":"<p>This is the second post in a series about what I learnt in my GPU class at NYU this past fall. This will be mostly about <strong>warps and SIMD hardward</strong>.</p>\n<h2>Kernel threads hierarchy</h2>\n<p>Recall that launching a CUDA kernel will generate a grid of threads organized as a <strong>two-level</strong> hierarchy.</p>\n<ol>\n<li>\n<p>top level: a 1/2/3-dimensional array of blocks.</p>\n</li>\n<li>\n<p>bottom level: each block consists of a 1/2/3-dimensional array of threads.</p>\n</li>\n</ol>\n<h2>Synchronize threads?</h2>\n<p>Conceptually, threads in a block can execute in any order, just like blocks.</p>\n<p>When an algorithm needs to execute in <em>phases</em>, <strong>barrier synchronizations</strong> should be used to ensure that all threads have completed a common phase before they start the next one.</p>\n<p>But the correctness of executing a kernel should not depend on the synchrony amongst threads.</p>\n<h2>Warp</h2>\n<p>Due to hardware cost considerations, CUDA devices currently bundle multiple threads for execution, which leads to performance limitations.</p>","fields":{"readingTime":{"text":"1 min read"}},"frontmatter":{"title":"What I Learnt About The CUDA Parallel Programming Model - 2","date":"December 06, 2019"}}},"pageContext":{"isCreatedByStatefulCreatePages":false,"slug":"/cuda2/","previous":{"fields":{"slug":"/cuda1/"},"frontmatter":{"title":"What I Learnt About The CUDA Parallel Programming Model - 1"}},"next":null}}}