{
  "componentChunkName": "component---src-templates-blog-post-js",
  "path": "/cuda1/",
  "result": {
    "data": {
      "site": {
        "siteMetadata": { "title": "FANG'S NOTEBOOK", "author": "Fang Cabrera" }
      },
      "markdownRemark": {
        "id": "e92f1d62-ed13-5144-a5b9-ea2222379d8d",
        "excerpt": "I took the Graphics Processing Units course at NYU this past fall. This is the first post in a series about what I learnt. Buckle up for‚Ä¶",
        "html": "<p>I took the <a href=\"https://cs.nyu.edu/courses/fall19/CSCI-GA.3033-004/\"><em>Graphics Processing Units</em></a> course at NYU this past fall. This is the first post in a series about what I learnt. Buckle up for lots of technical details!</p>\n<h2>TABLE OF CONTENTS</h2>\n<ol>\n<li>\n<p>Concepts</p>\n<ol>\n<li>key abstractions</li>\n<li>granularity</li>\n</ol>\n</li>\n<li>\n<p>CUDA Architecture</p>\n<ol>\n<li>kernel execution</li>\n<li>thread organization</li>\n<li>blocks</li>\n<li>SMs</li>\n<li>warp</li>\n<li>execution picture</li>\n</ol>\n</li>\n<li>Thread ID</li>\n<li>\n<p>Memory Hierarchy</p>\n<ol>\n<li>between CPU and GPU</li>\n<li>global memory</li>\n<li>shared memory</li>\n<li>local memory</li>\n<li>registers</li>\n<li>constant and texture memory</li>\n<li>principle behind the design of memory hierarchy</li>\n<li>more on the difference between shared memory and registers in a CUDA device</li>\n</ol>\n</li>\n</ol>\n<h2>Some Concepts</h2>\n<h3>three key abstractions</h3>\n<ul>\n<li>a hierarchy of thread groups</li>\n<li>shared memories</li>\n<li>barrier synchronization</li>\n</ul>\n<h3>granularity</h3>\n<ul>\n<li>\n<p>In parallel computing, granularity means the amount of <strong>computation</strong> in relation to <strong>communication (or transfer) of data</strong>.</p>\n<ul>\n<li>fine-grained: individual tasks are small in terms of code size and execution time.</li>\n<li>coarse-grained: larger amounts of computation, infrequent data communication.</li>\n</ul>\n</li>\n<li>\n<p>CUDA abstraction:</p>\n<ul>\n<li><strong>fine-grained</strong> data parallelism and thread parallelism nested within <strong>roarse-grained</strong> data parallelism and task parallelism.</li>\n<li>programmers partition the problem into <strong>coarse sub-problems</strong> that can be solved independently in prallel by <strong>blocks of threads</strong> and each sub-problem into finer pieces that can be solved cooperatively in parallel by <strong>threads within the block</strong>.</li>\n</ul>\n</li>\n</ul>\n<h2>CUDA Architecture</h2>\n<h3>kernel execution</h3>\n<ul>\n<li>Executed in parallel by an array of threads, all of which run the same code.</li>\n<li>Each thread has an ID which is used to compute memory addresses and make control decisions.</li>\n</ul>\n<h3>thread organization - grid of blocks</h3>\n<ul>\n<li>Threads are arranged as a <strong>grid</strong> of <strong>blocks</strong>.</li>\n<li>Blocks in a grid are completely <strong>independent</strong> which means they can be executed in any order, in parallel or in series.</li>\n<li>The independence allows thread blocks to be scheduled in <em>any order</em> across <em>any number of cores</em>.</li>\n</ul>\n<h3>block</h3>\n<ul>\n<li>Threads from the same block have access to a <strong>shared memory</strong> .</li>\n<li>Execution of threads from the same block can be <strong>synchronized</strong> (to coordinate memory accesses).</li>\n</ul>\n<h3>SMs</h3>\n<p>The CUDA architecture is built around a scalable array of multithreaded Streaming Multiprocessors.</p>\n<p>Each SM has:</p>\n<ul>\n<li>a set of execution units</li>\n<li>a set of registers</li>\n<li>a chunk of shared memory.</li>\n</ul>\n<h3>üßêwarp</h3>\n<p>Warp is the <strong>basic unit of execution</strong> in an NVIDIA GPU.</p>\n<p>A warp is a collection of threads, 32 in current NVIDIA implementations.</p>\n<ul>\n<li>threads within a warp a executed simultaneously by an SM.</li>\n<li>multiple warps can be executed on an SM at once.</li>\n</ul>\n<p>The mapping between warps and thread blocks can affect the performance of the kernel.\n<strong>It‚Äôs usually good the keep the size of a block a multiple of 32</strong>.</p>\n<h3>picture the process of execution</h3>\n<ol>\n<li>CUDA program on the host CPU invokes a <strong>kernel grid</strong></li>\n<li>blocks in the grid are enumerated and distributed to SMs with available execution capacity</li>\n<li>the threads of a block execute concurrently on one SM</li>\n<li>as thread blocks terminate, new blocks are launched on the vacated SMs</li>\n</ol>\n<h2>Thread ID</h2>\n<p>TODO</p>\n<h2>Memory Hierarchy</h2>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 590px;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/bacd550fe0b1326f4e4032d2f73592d9/fb77c/memory.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 50.27027027027027%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAKABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAQDBf/EABYBAQEBAAAAAAAAAAAAAAAAAAIBA//aAAwDAQACEAMQAAAB7c1kYuYyX//EABoQAAEFAQAAAAAAAAAAAAAAAAEAAgMREhD/2gAIAQEAAQUCUjqfs8kA1QX/xAAWEQEBAQAAAAAAAAAAAAAAAAAAARH/2gAIAQMBAT8BjH//xAAXEQEAAwAAAAAAAAAAAAAAAAAAAhEx/9oACAECAQE/AZ4p/8QAFhABAQEAAAAAAAAAAAAAAAAAADEg/9oACAEBAAY/AlXH/8QAGRAAAwEBAQAAAAAAAAAAAAAAAAERIVFB/9oACAEBAAE/IfXBjZgUNEXCziFwR//aAAwDAQACAAMAAAAQf9//xAAWEQADAAAAAAAAAAAAAAAAAAAAARH/2gAIAQMBAT8QWsk//8QAGBEAAwEBAAAAAAAAAAAAAAAAAAERMVH/2gAIAQIBAT8QS6K6z//EABwQAQACAwADAAAAAAAAAAAAAAEAETFRkUHB0f/aAAgBAQABPxAG5AXuoIAINTEHD7Fm0cirMR4lL1T/2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"memory\"\n        title=\"memory\"\n        src=\"/static/bacd550fe0b1326f4e4032d2f73592d9/88218/memory.jpg\"\n        srcset=\"/static/bacd550fe0b1326f4e4032d2f73592d9/7237a/memory.jpg 148w,\n/static/bacd550fe0b1326f4e4032d2f73592d9/0cfdf/memory.jpg 295w,\n/static/bacd550fe0b1326f4e4032d2f73592d9/88218/memory.jpg 590w,\n/static/bacd550fe0b1326f4e4032d2f73592d9/77d57/memory.jpg 885w,\n/static/bacd550fe0b1326f4e4032d2f73592d9/5a917/memory.jpg 1180w,\n/static/bacd550fe0b1326f4e4032d2f73592d9/fb77c/memory.jpg 1295w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h3>between CPU and GPU</h3>\n<p>CPU and GPU has <strong>separate memory spaces</strong> => data must be moved from CPU to GPU before computation starts, as well as moved back to CPU once processing has completed.</p>\n<p>Host can <strong>write or read</strong> global memory and constant memory by calling API function.</p>\n<p>(Note that device can only read constant memory. But it‚Äôs short-latency and high-bandwidth)</p>\n<p>Shared memory and registers are on-chip memories that are not visible to the host.</p>\n<h3>global memory</h3>\n<ol>\n<li>accessible to all threads as well as the host (CPU)</li>\n<li><strong>host</strong> allocate and deallocate the global memory</li>\n<li>data is first initialized here for the GPU to work on</li>\n</ol>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 590px;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/297af877f673dc988df4be6f99c11c79/6d894/global.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 49.877750611246945%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAABYlAAAWJQFJUiTwAAACjElEQVQoz1WSTU8TURSG+69cuHGhGIOJidGNGxcuJPEzMTEKiR9RjAs1JqKCoMQYNaiJVKSAlHYG2lIoVVo6zLSFfswMHajQllYplk4fL6AmnpuTe86bm5Oc97mOwvoyxnKcH7U8mxSo2qv8EvffeqO+ymZjTWhFoa1RrZfYjoY47+RFHjsVul0qj/pjhLQVHFbJYHJOJTgXR9F1kstZQuoCYS2DWU5RQhV6mmklyddkBmvdANvGrtU49cDH3kvDHGxzs+eii96ROA6jlGNoMseAZOKdtZjUDD5JBp/HV4hZcZZqAeSIidOjMzRdYO6rn4XnXSRe9tLS7qL5psyx2xJNbaO88Szg0L4n6BqM0elUeTuh4QxHedqv0PkpiU8PkGWAPr/Ck48x8S5OxIjSqP6kWi5ztiNA83UPx9tlmlpHeT2WxJH/qTOXmydqaCg5BW0lRkRXdzJVjGD9mmHeUnb6b3qMXCX9x0M4/3SKE3fHudg5zZGbHl65xcDKRoFiRWdLGG4L47dENkRu17VGic2tdeo7WkloBWr1Mg27gV3f4kzHJCfv+bjaG+bwtbHdgcZqGjk8y1RMRTWzLOSzAkiSb4k0ZmUXyryRYkZNEEqkWK6Y/I1zj4M0i0H/rWwKKKMhC9fEkjDfIhg3GBw3GfLnUcR3suwg41GTz7LOyMw2FB/Jzg7iL3o4fWeIQzckjt7ycuDqF157xMDE9xTdg4s8c2bomxAIwgKSM033wBKBbJiMPch7/yJd/Sl6XAZRXaH+o8RGsUjLQx/7W93CPy/7Lo/srpxdjyEn3MiaRCAtMW14keNeJNFH825S1WGCGUloEl5tTICa/bfy/Q8RLjwJcKVnShD3MxzK8htsSKQ9zj7uQAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"global\"\n        title=\"global\"\n        src=\"/static/297af877f673dc988df4be6f99c11c79/799d3/global.png\"\n        srcset=\"/static/297af877f673dc988df4be6f99c11c79/00d96/global.png 148w,\n/static/297af877f673dc988df4be6f99c11c79/0b23c/global.png 295w,\n/static/297af877f673dc988df4be6f99c11c79/799d3/global.png 590w,\n/static/297af877f673dc988df4be6f99c11c79/2a3d6/global.png 885w,\n/static/297af877f673dc988df4be6f99c11c79/ae92f/global.png 1180w,\n/static/297af877f673dc988df4be6f99c11c79/6d894/global.png 1636w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h3>shared memory</h3>\n<ol>\n<li>on-chip</li>\n<li>each thread block has its own</li>\n<li>much faster than local or global memory</li>\n<li>requires special handling to get max performance</li>\n<li>only exists for the lifetime of the block</li>\n</ol>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 590px;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/9b39ccb8f5ff817f1518b632cd19541c/2b6a8/shared.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 38.8268156424581%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAABYlAAAWJQFJUiTwAAAB9ElEQVQoz12S/UtTURyH/bP6NX+QCpOg+gOCIAIpo9DKSqUiQqgoeqEif4hSVKKUcXUvuplrby6dy83tznRvXe+067ze6e509+k4Z1AHHh44fPicc/ieBsuyqFYt/l971SqHu4fWtsp8TyrMLxUE6l/mUipb22Yt01A0Ksj5HUI/sswmVvFHlplZyJHMl9F1QzSX2TV3RHQXV1CmsWOM5jtOTtzex8HxWw6OddqJpFYPCtc3TfwLGuNeGWkqjs0Tq9kb1fgVC1NW4hhKCtYSjI5InH3wlY53YS489dH+doarb2Y4c2+S2WS9UN0wcYZU7P4c9kAeyZvGITzmV8gl4+z8zqKvK1jFHLYxD02dTlqfB7j4zMellwFaXwQ42eUUN1QPChXNZNSnMjKd55M7zRfhz1MZbEGNzGKMcuEnuprG0laQxt00Xndw/sk3Lr8K0iY498hL000784eF+fUyg54C/a4sH52ZmvuFB6c10rEoe2syJXVFTGQJSXLR0uPmRl+Ya+KpV16Haj59d5I5uV6YLlR4PLRE74co9/tCPHwfEczROyCTWlapmluUjP3hbDPqjnCkzcapnglaul00d4kDuic42i4RTigHhcZ2hURWJ1ljs26dxXSRkmn9820yBYOBiUWGp2SGPEmGBUN11jaMWuYP/doe/bDbqx8AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"shared\"\n        title=\"shared\"\n        src=\"/static/9b39ccb8f5ff817f1518b632cd19541c/799d3/shared.png\"\n        srcset=\"/static/9b39ccb8f5ff817f1518b632cd19541c/00d96/shared.png 148w,\n/static/9b39ccb8f5ff817f1518b632cd19541c/0b23c/shared.png 295w,\n/static/9b39ccb8f5ff817f1518b632cd19541c/799d3/shared.png 590w,\n/static/9b39ccb8f5ff817f1518b632cd19541c/2b6a8/shared.png 716w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h3>local memory</h3>\n<ol>\n<li>on-chip</li>\n<li>only exists for the lifetime of the thread</li>\n<li>generally handled automatically by the <strong>compiler</strong></li>\n</ol>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 590px;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/b9c6f6fb2d801f35ae1c943a936b2618/ebbc9/local.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 40.97859327217125%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAABYlAAAWJQFJUiTwAAABoklEQVQoz42R3WtSYRzH/bOim15gyVaObmLQRTGCIgjqolFOU4Ne6Y2K7sShuKKXqy2G50zrMHLNQJc6t+jKVzI7OvXkEI/HT8fTmcpG0ANfvt/n5ffheX6PpdfrsTv+lf939Gss/dBqtVBVdbC4CytVGkTTBdYyRUOxTdPNHDNzNJWn8LNu1BhARVH4JVcHwK72F/gyvMXhmSUmPREmbixzbFbkuO5Wh8iYXTTmNneEQ1eX8Ic2hkBZltlR6ua9NTRNM+Kb8AZHromce7LKvddpLr2IYZ9L4JlPYvfFuan7hWef9TMCQXFzCGzUZIpfFvld+kazsEWj+B21EMcfeIvVJTF1R2L6UZSzDz9xRtf04yjnn65y8fkap++vcHQvsFqtojSb+nM1ul2VTqdjbAbFDFN3V3jwLsN1XwKnf50Zbxx3MInLzLdepTh1WyIgjAArlQrlctnsIYMeBoQMBy4vMOlexuYSGXcIWO0hTuh5wikwruukJ8zBKwt436eGwHa7Ta1W2/fLuXKdD4kc0nqej7r6Ln0dyaYi8SzZH9tGzR9xoSLV37KNMQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"local\"\n        title=\"local\"\n        src=\"/static/b9c6f6fb2d801f35ae1c943a936b2618/799d3/local.png\"\n        srcset=\"/static/b9c6f6fb2d801f35ae1c943a936b2618/00d96/local.png 148w,\n/static/b9c6f6fb2d801f35ae1c943a936b2618/0b23c/local.png 295w,\n/static/b9c6f6fb2d801f35ae1c943a936b2618/799d3/local.png 590w,\n/static/b9c6f6fb2d801f35ae1c943a936b2618/ebbc9/local.png 654w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<h3>registers</h3>\n<ol>\n<li>each thread can only access its own registers</li>\n<li>a kernel function typically uses registers to hold frequently accessed variables that are private to each thread</li>\n</ol>\n<h3>constant and texture memory</h3>\n<p>read-only, accessible by all threads</p>\n<ul>\n<li>constant memory: used to cache values that are shared by all functional units.</li>\n<li>texture memory is optimized for texturing operations provided by the hardware</li>\n</ul>\n<h3>principle behind the design of memory hierarchy</h3>\n<p>To improve <strong>compute-to-global-memory-access ratio</strong> and thus achieve high execution speed.</p>\n<h3>more on the difference between shared memory and registers in a CUDA device</h3>\n<p>Although both on-chip, they differ significantly in functionality and cost of access.</p>\n<p><figure class=\"gatsby-resp-image-figure\" style=\"\">\n    <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 590px;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/d5b4a56c399de51a63b5c390e8296064/43c17/shared%26registers.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 60.97826086956522%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAMABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAIBBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAe3lyUD/xAAWEAEBAQAAAAAAAAAAAAAAAAAQAkH/2gAIAQEAAQUCck//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAY/Al//xAAaEAACAgMAAAAAAAAAAAAAAAAAAREhMVGB/9oACAEBAAE/IXOuiqlghDDSM//aAAwDAQACAAMAAAAQ8M//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAbEAADAAIDAAAAAAAAAAAAAAAAAREhMUFRYf/aAAgBAQABPxBnvD0Q22pBq0jF30hluJSYWDY//9k='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"shared memory v.s. registers\"\n        title=\"shared memory v.s. registers\"\n        src=\"/static/d5b4a56c399de51a63b5c390e8296064/88218/shared%26registers.jpg\"\n        srcset=\"/static/d5b4a56c399de51a63b5c390e8296064/7237a/shared%26registers.jpg 148w,\n/static/d5b4a56c399de51a63b5c390e8296064/0cfdf/shared%26registers.jpg 295w,\n/static/d5b4a56c399de51a63b5c390e8296064/88218/shared%26registers.jpg 590w,\n/static/d5b4a56c399de51a63b5c390e8296064/77d57/shared%26registers.jpg 885w,\n/static/d5b4a56c399de51a63b5c390e8296064/43c17/shared%26registers.jpg 920w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span>\n    <figcaption class=\"gatsby-resp-image-figcaption\">shared memory v.s. registers</figcaption>\n  </figure></p>\n<h4>background knowledge on how processors access different memory types</h4>\n<h5>global memory</h5>\n<ul>\n<li>off the processor chip</li>\n<li>implemented with DRAM technology, which implies long access latencies and relatively low access bandwidths</li>\n</ul>\n<h5>registers</h5>\n<ul>\n<li>correspond to the Register File of the von Neumann model</li>\n<li>on-chip, which implies very short access latency and drastically higher access bandwidth compared with the global memory</li>\n<li>furthermore, when a variable is stored in a register, its accesses <strong>no longer consume off-chip global memory bandwidth</strong>. This reduced bandwidth consumption will be reflected as an increased compute-to-global-memory-access ratio.</li>\n</ul>\n<h5>how does ALU treat them differently</h5>\n<ul>\n<li>\n<p>each access to registers involves <strong>fewer instructions</strong> than an access to the global memory. Arithmetic instructions in most modern processors have ‚Äúbuilt-in‚Äù register operands.</p>\n<ul>\n<li>\n<p>For instance:</p>\n<div class=\"gatsby-highlight\" data-language=\"c\"><pre class=\"language-c\"><code class=\"language-c\">fadd r1<span class=\"token punctuation\">,</span> r2<span class=\"token punctuation\">,</span> r3</code></pre></div>\n</li>\n</ul>\n</li>\n<li>\n<p>meanwhile, if an operand value is in the global memory, the processor needs to perform a memory <strong>load</strong> operation to make the operand value available to the ALU</p>\n<ul>\n<li>\n<p>For instance:</p>\n<div class=\"gatsby-highlight\" data-language=\"c\"><pre class=\"language-c\"><code class=\"language-c\">load r2<span class=\"token punctuation\">,</span> r4<span class=\"token punctuation\">,</span> offset\nfadd r1<span class=\"token punctuation\">,</span> r2<span class=\"token punctuation\">,</span> r3</code></pre></div>\n</li>\n</ul>\n</li>\n</ul>\n<h4>shared v.s. global</h4>\n<ul>\n<li>When the processor accesses data that reside in the shared memory, it needs to perform a memory load operation, similar to accessing data in the global memory.</li>\n<li>However, because shared memory resides on-chip, it can be accessed with much lower latency and much higher throughput than the global memory.</li>\n</ul>\n<h4>shared v.s. registers</h4>\n<ul>\n<li>Shared memory has longer latency and lower bandwidth than registers because of the need to perform a <strong>load</strong> operation.</li>\n<li>In computer architecture terminology, the shared memory is a form of <em>scratchpad</em> memory.</li>\n</ul>",
        "fields": { "readingTime": { "text": "6 min read" } },
        "frontmatter": {
          "title": "The CUDA Parallel Programming Model - 1. Concepts",
          "date": "November 30, 2019"
        }
      }
    },
    "pageContext": {
      "isCreatedByStatefulCreatePages": false,
      "slug": "/cuda1/",
      "previous": null,
      "next": {
        "fields": { "slug": "/cuda2-warp/" },
        "frontmatter": {
          "title": "The CUDA Parallel Programming Model - 2. Warps"
        }
      }
    }
  }
}
