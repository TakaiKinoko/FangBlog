{
  "componentChunkName": "component---src-templates-blog-post-js",
  "path": "/cuda4-sync/",
  "webpackCompilationHash": "",
  "result": {
    "data": {
      "site": {
        "siteMetadata": { "title": "FANG'S NOTEBOOK", "author": "Fang Cabrera" }
      },
      "markdownRemark": {
        "id": "863e4c7d-c15f-5dc9-ada5-e86eb540e778",
        "excerpt": "This is the fourth post in a series about what I learnt in my GPU class at NYU this past fall. Here I collected several examples thatâ€¦",
        "html": "<p>This is the fourth post in a series about what I learnt in my GPU class at NYU this past fall. Here I collected several examples that showcase how the CUDA <code class=\"language-text\">__syncthreads()</code> command should (or should not) be used.</p>\n<h2>Example 1</h2>\n<div class=\"gatsby-highlight\" data-language=\"c\"><pre class=\"language-c\"><code class=\"language-c\">__shared__ <span class=\"token keyword\">float</span> partialSum<span class=\"token punctuation\">[</span>SIZE<span class=\"token punctuation\">]</span><span class=\"token punctuation\">;</span>\npartialSum<span class=\"token punctuation\">[</span>threadIdx<span class=\"token punctuation\">.</span>x<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> X<span class=\"token punctuation\">[</span>blockIdx<span class=\"token punctuation\">.</span>x <span class=\"token operator\">*</span> blockDim<span class=\"token punctuation\">.</span>x <span class=\"token operator\">+</span> threadIdx<span class=\"token punctuation\">.</span>x<span class=\"token punctuation\">]</span><span class=\"token punctuation\">;</span>\n<span class=\"token keyword\">unsigned</span> <span class=\"token keyword\">int</span> t <span class=\"token operator\">=</span> threadIdx<span class=\"token punctuation\">.</span>x<span class=\"token punctuation\">;</span>\n<span class=\"token keyword\">for</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">unsigned</span> <span class=\"token keyword\">int</span> stride <span class=\"token operator\">=</span> <span class=\"token number\">1</span><span class=\"token punctuation\">;</span> stride <span class=\"token operator\">&lt;</span> blockDim<span class=\"token punctuation\">.</span>x<span class=\"token punctuation\">;</span> stride <span class=\"token operator\">*</span><span class=\"token operator\">=</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">{</span>\n     <span class=\"token function\">__syncthreads</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n     <span class=\"token keyword\">if</span><span class=\"token punctuation\">(</span>t <span class=\"token operator\">%</span> <span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token operator\">*</span>stride<span class=\"token punctuation\">)</span> <span class=\"token operator\">==</span> <span class=\"token number\">0</span><span class=\"token punctuation\">)</span>\n          partialSum<span class=\"token punctuation\">[</span>t<span class=\"token punctuation\">]</span> <span class=\"token operator\">+</span><span class=\"token operator\">=</span> partialSum<span class=\"token punctuation\">[</span>t<span class=\"token operator\">+</span>stride<span class=\"token punctuation\">]</span><span class=\"token punctuation\">;</span>\n<span class=\"token punctuation\">}</span></code></pre></div>\n<p>The <code class=\"language-text\">__syncthreads()</code> statement in the for-loop ensures that all partial sums for the previous iteration have been generated and before any one of the threads is allowed to begin the current iteration. This way, all threads that enter the second iteration will be using the values produced in the first iteration.</p>",
        "fields": { "readingTime": { "text": "1 min read" } },
        "frontmatter": {
          "title": "The CUDA Parallel Programming Model - 4.               \n Syncthreads Examples",
          "date": "December 07, 2019"
        }
      }
    },
    "pageContext": {
      "isCreatedByStatefulCreatePages": false,
      "slug": "/cuda4-sync/",
      "previous": {
        "fields": { "slug": "/cuda3-thread-divergence/" },
        "frontmatter": {
          "title": "The CUDA Parallel Programming Model - 3.               \n More On Thread Divergence"
        }
      },
      "next": {
        "fields": { "slug": "/cuda5-coalesce/" },
        "frontmatter": {
          "title": "The CUDA Parallel Programming Model - 5.Memory Coalescing"
        }
      }
    }
  }
}
