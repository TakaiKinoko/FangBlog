{"componentChunkName":"component---src-templates-blog-post-js","path":"/cuda/","webpackCompilationHash":"","result":{"data":{"site":{"siteMetadata":{"title":"FANG'S NOTEBOOK","author":"Fang Cabrera"}},"markdownRemark":{"id":"1299c932-7966-5006-9240-d6c528cf05f1","excerpt":"Some Concepts three key abstractions a hierarchy of thread groups shared memories barrier synchronization granularity In parallel computingâ€¦","html":"<h2>Some Concepts</h2>\n<h3>three key abstractions</h3>\n<ul>\n<li>\n<p>a hierarchy of thread groups</p>\n</li>\n<li>\n<p>shared memories</p>\n</li>\n<li>\n<p>barrier synchronization</p>\n</li>\n</ul>\n<h3>granularity</h3>\n<ul>\n<li>\n<p>In parallel computing, granularity means the amount of <strong>computation</strong> in relation to <strong>communication (or transfer) of data</strong>.</p>\n<ul>\n<li>\n<p>fine-grained: individual tasks are small in terms of code size and execution time.</p>\n</li>\n<li>\n<p>coarse-grained: larger amounts of computation, infrequent data communication.</p>\n</li>\n</ul>\n</li>\n<li>\n<p>CUDA abstraction:</p>\n<ul>\n<li>\n<p><strong>fine-grained</strong> data parallelism and thread parallelism nested within <strong>roarse-grained</strong> data parallelism and task parallelism.</p>\n</li>\n<li>\n<p>programmers partition the problem into <strong>coarse sub-problems</strong> that can be solved independently in prallel by <strong>blocks of threads</strong> and each sub-problem into finer pieces that can be solved cooperatively in parallel by <strong>threads within the block</strong>.</p>\n</li>\n</ul>\n</li>\n</ul>\n<h3>kernel execution</h3>\n<ul>\n<li>\n<p>Executed in parallel by an array of threads, all of which run the same code.</p>\n</li>\n<li>\n<p>Each thread has an ID which is used to compute memory addresses and make control decisions.</p>\n</li>\n</ul>\n<h3>thread organization - grid of blocks</h3>\n<ul>\n<li>\n<p>Threads are arranged as a <strong>grid</strong> of <strong>blocks</strong>.</p>\n</li>\n<li>\n<p>Blocks in a grid are completely <strong>independent</strong> which means they can be executed in any order, in parallel or in series.</p>\n</li>\n<li>\n<p>The independence allows thread blocks to be scheduled in <em>any order</em> across <em>any number of cores</em>.</p>\n</li>\n</ul>\n<h3>block</h3>\n<ul>\n<li>\n<p>Threads from the same block have access to a <strong>shared memory</strong> .</p>\n</li>\n<li>\n<p>Execution of threads from the same block can be <strong>synchronized</strong> (to coordinate memory accesses).</p>\n</li>\n</ul>\n<h2>CUDA Architecture</h2>\n<h3>SMs</h3>\n<p>The CUDA architecture is built around a scalable array of multithreaded Streaming Multiprocessors. </p>\n<p>Each SM has:</p>\n<ul>\n<li>\n<p>a set of execution units</p>\n</li>\n<li>\n<p>a set of registers </p>\n</li>\n<li>\n<p>a chunch of shared memory.</p>\n</li>\n</ul>\n<h3>warp</h3>\n<p>Warp is the <strong>basic unit of execution</strong> in an NVIDIA GPU.</p>\n<p>A warp is a collection of threads, 32 in current NVIDIA implementations. </p>\n<ul>\n<li>\n<p>threads within a warp a executed simultaneously by an SM. </p>\n</li>\n<li>\n<p>multiple warps can be executed on an SM at once.</p>\n</li>\n</ul>","fields":{"readingTime":{"text":"2 min read"}},"frontmatter":{"title":"What I Learnt About The CUDA Parallel Programming Model","date":"December 06, 2019"}}},"pageContext":{"isCreatedByStatefulCreatePages":false,"slug":"/cuda/","previous":{"fields":{"slug":"/text-headings/"},"frontmatter":{"title":"Just text and links"}},"next":null}}}