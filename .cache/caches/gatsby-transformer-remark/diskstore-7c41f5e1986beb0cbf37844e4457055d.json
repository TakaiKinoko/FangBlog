{
  "expireTime": 9007200830838894000,
  "key": "transformer-remark-markdown-html-733c1564ca457b7679885fca6562969a-gatsby-remark-imagesgatsby-remark-responsive-iframegatsby-remark-prismjsgatsby-remark-copy-linked-filesgatsby-remark-smartypantsgatsby-remark-reading-time-",
  "val": "<p>Have you wondered if itâ€™s possible to launch nested kernels (i.e. a kernel calls another kernel) in CUDA? Well, this is where dynamic parallelism comes into play.</p>\n<p>Without dynamic parallelism, GPU is unable to create more work on itself dynamically depending on the data. Consequently data travels back and forth between the CPU and GPU many times. This was the case in older generations e.g. Fermi, see picture below:</p>\n<p><figure class=\"gatsby-resp-image-figure\" style=\"\">\n    <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 590px;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/50ab2214f80a3820049d777e1d662621/7a2f6/fermi.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 100.47281323877068%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAACXBIWXMAABYlAAAWJQFJUiTwAAACq0lEQVQ4y6WUS2/TQBSF7Rm/x/bEduI8nTRNnDZUolSFBlChLUSiou2CDQtWLJBYIbWqgA2PDa0qkJBY8VhVQurfPIwfpbUSpFQsjhx5PDfznXvPSLJmgbIARDwlSSpIliXwgEJRJai6DE0n0AwZQU0BcwkIkSb2SHq0Cq27DqN7F5JMC4u62Dx6yHB/x8H6Y4bRmGG4auDJCxf3dhlcj0wWVA2GsN6EabHJxVyqJoNSCVTJRMTv5HQJwcT3bqWBhTvbCIPSlGKKkPzPP5oq0/Hgt2Iw08x8IzQtxEoeDk8WsLxWTt9P9WuadMNEu9uDy+0UgZDMF9sxcXLWwWjL+9ugmQq6YQvx2iNELR+KJjyiNN1MKUE8rIJ7WfdlSb1kA8mtOH9Hc6XIJZRbfViGkZ/kAvngKEZ3wDHarGD/aABDfLN/1MftByEGS2W8+dqHbbt4fhhh71l0GbkPniCTBDlrAnMMfDrtYHFZjMh2gHff27AsCx9+RdjYDbC0Usbx7ya46+HVcQ1PX9azgvwcuZkgy1AUJfVRUTUMhjXhpQE/sNFbCFM7oi4XNphgtoFOj0NVVNQjG5WqnSPbXCD3YOpFZC8s4+PPGP2hj42dEK+/DEBkZYaxYTbm42vwfaeAzEsG3v9ooDfk2NzzhV8RKFELHS88z6eAV9uIb43RanppCjJkKnIrkLpVWMwU0gWSM9uQG8yFX4uga/olZIpaO8Tx6SKu54M9e1KYg7kEOSgil0MTb781MVzhWVLojBHktQ76N8doNhJk0WVVSeNniMuiN6jDcayrnVA3bdHRBjQxJhkySZHrc1V8PovFvPl5xmcsyLgnOnkDldBNryVKL5APTppizb1alksCeX5lC7VQeEWyyyHLMhWxYqkFV2uKGOxqqwsrv77+V38ASHzNIENPxjQAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"fermi\"\n        title=\"fermi\"\n        src=\"/static/50ab2214f80a3820049d777e1d662621/799d3/fermi.png\"\n        srcset=\"/static/50ab2214f80a3820049d777e1d662621/00d96/fermi.png 148w,\n/static/50ab2214f80a3820049d777e1d662621/0b23c/fermi.png 295w,\n/static/50ab2214f80a3820049d777e1d662621/799d3/fermi.png 590w,\n/static/50ab2214f80a3820049d777e1d662621/7a2f6/fermi.png 846w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span>\n    <figcaption class=\"gatsby-resp-image-figcaption\">fermi</figcaption>\n  </figure></p>\n<p>With dynamic parallelism, however:</p>\n<ul>\n<li>GPU can generate work on itself without involvement of CPU.</li>\n<li>Permits Dynamic Runtime decisions.</li>\n<li>Kernels can start new kernels</li>\n<li>Streams can spawn new streams.</li>\n</ul>\n<p><a href=\"./kepler.png\">!kepler</a></p>\n<h3>code example</h3>\n<div class=\"gatsby-highlight\" data-language=\"c\"><pre class=\"language-c\"><code class=\"language-c\">__global__ <span class=\"token function\">ChildKernel</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">void</span><span class=\"token operator\">*</span> data<span class=\"token punctuation\">)</span><span class=\"token punctuation\">{</span>\n    <span class=\"token comment\">// operate on data</span>\n<span class=\"token punctuation\">}</span>\n\n__global__ <span class=\"token function\">ParentKernel</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">void</span><span class=\"token operator\">*</span> data<span class=\"token punctuation\">)</span><span class=\"token punctuation\">{</span>\n    <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span>threadIdx<span class=\"token punctuation\">.</span>x <span class=\"token operator\">==</span> <span class=\"token number\">0</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span>\n        ChildKernel<span class=\"token operator\">&lt;&lt;</span><span class=\"token operator\">&lt;</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">32</span><span class=\"token operator\">>></span><span class=\"token operator\">></span><span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n        <span class=\"token function\">cudaThreadSynchronize</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token punctuation\">}</span>\n    <span class=\"token function\">syncthreads</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token comment\">//operate on data</span>\n<span class=\"token punctuation\">}</span></code></pre></div>"
}
