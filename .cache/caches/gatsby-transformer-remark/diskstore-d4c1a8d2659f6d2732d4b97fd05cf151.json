{
  "expireTime": 9007200830645242000,
  "key": "transformer-remark-markdown-html-154da6c4a341a3c23620f738f31bbd39-gatsby-remark-imagesgatsby-remark-responsive-iframegatsby-remark-prismjsgatsby-remark-copy-linked-filesgatsby-remark-smartypantsgatsby-remark-reading-time-",
  "val": "<p>In the previous posts, we have sometimes assumed that only one kernel is launched at a time. But this is not all that kernels can do. They can be launched <strong>sequentially</strong> or <strong>in parallel</strong>. Itâ€™s just that if multiple kernels are launched in parallel, CUDA streams must be used.</p>\n<h2>Motivation</h2>\n<p><strong>Heterogeneous computing</strong> is about efficiently using all processors in the system, including CPUs and GPUs. To do this, applications must <strong>execute functions concurrently</strong> on multiple processors.</p>\n<p>CUDA Applications manage concurrency by executing <strong>asynchronous</strong> commands in streams. </p>\n<ul>\n<li>\n<p>Each stream is a sequences of commands that execute <strong>in order</strong>. </p>\n</li>\n<li>\n<p>Different streams may execute their commands concurrently or <strong>out of order</strong> with respect to each other.</p>\n</li>\n</ul>\n<h2>What is a Stream?</h2>\n<p>In CUDA, stream refers to <strong>a single operation sequence</strong> on a GPU device.</p>\n<ul>\n<li>\n<p>Every CUDA kernel is invoked on an independent stream</p>\n</li>\n<li>\n<p>If only one kernel is invoked, the default stream, stream0, is used</p>\n</li>\n<li>\n<p>When you execute asynchronous CUDA commands without specifying a stream, the runtime uses the default stream</p>\n</li>\n<li>\n<p>If n kernels are invoked in parallel, n streams need to be used</p>\n</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"c\"><pre class=\"language-c\"><code class=\"language-c\">  kernel<span class=\"token operator\">&lt;&lt;</span><span class=\"token operator\">&lt;</span> blocks<span class=\"token punctuation\">,</span> threads<span class=\"token punctuation\">,</span> bytes <span class=\"token operator\">>></span><span class=\"token operator\">></span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>    <span class=\"token comment\">// default stream</span>\n  kernel<span class=\"token operator\">&lt;&lt;</span><span class=\"token operator\">&lt;</span> blocks<span class=\"token punctuation\">,</span> threads<span class=\"token punctuation\">,</span> bytes<span class=\"token punctuation\">,</span> <span class=\"token number\">0</span> <span class=\"token operator\">>></span><span class=\"token operator\">></span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span> <span class=\"token comment\">// also default stream</span></code></pre></div>\n<p>In CUDA, we can run multiple kernels on different streams concurrently. Typically, we can improve performance by increasing number of concurrent streams by setting a higher degree of parallelism.</p>\n<h2>Asynchronous Commands</h2>\n<h3>side note on non-blocking</h3>\n<p>Asynchronous commands return control to the calling host thread before the device has finished the requested task (they are <strong>non-blocking</strong>). These commands are:</p>\n<ul>\n<li>Kernel launches</li>\n<li>Memory copies between two addresses to the same device memory</li>\n<li>Memory copies from host to device of a memory block of 64 KB or less</li>\n<li>Memory copies performed by functions with the Async suffix</li>\n<li>Memory set function calls</li>\n</ul>\n<p>CUDA 7 introduced the per-thread default stream, that has two effects:</p>\n<ol>\n<li>it gives each host thread its own default stream. This means that commands issued to the default stream by different host threads can run concurrently. </li>\n<li>these default streams are regular streams. This means that commands in the default stream may run concurrently with commands in non-default streams.</li>\n</ol>\n<p>To learn more about how to invoke these options in <code class=\"language-text\">nvcc</code>, check out the <em>CUDA C Programming Guide</em>.</p>\n<h2>Between Streams</h2>\n<p>The concurrent streams are <strong>independent</strong>, which means that streams neither communicate nor have any inter-stream dependency.</p>\n<p>However, different streams may have different execution times (<strong>asynchronous</strong>), and we cannot guarantee that all streams will complete at the same time. To ensure all streams are synchronized, use a synchronization barrier.</p>\n<h2>Synchronize Streams</h2>\n<p>There are two types of stream synchronization in CUDA. A programmer can place the synchronization barrier explicitly, to synchronize tasks such as memory operations. Some functions are implicitly synchronized, which means one or all streams must complete before proceeding to the next section.</p>\n<h3>explicit synchronization</h3>\n<p>Invoking multiple concurrent kernels requires slightly more programming than invoking one kernel at a time:</p>\n<ul>\n<li>Kernels need to be assigned to different streams and invoked asynchronously</li>\n<li>Kernel launch and memory transfer functions need to be assigned to the same stream</li>\n<li>pinned memory must be used</li>\n<li>Asynchronous memory transfer API functions must be used</li>\n<li>the synchronization barrier cudaStreamSynchronize() must be used to ensure all tasks are synchronized</li>\n</ul>\n<h3>implicit synchronization</h3>\n<p>The following operations are implicitly synchronized; therefore, no barrier is needed:</p>\n<ul>\n<li>\n<p>page-locked memory allocation </p>\n<ul>\n<li>cudaMallocHost</li>\n<li>cudaHostAlloc</li>\n</ul>\n</li>\n<li>\n<p>device memory allocation</p>\n<ul>\n<li>cudaMalloc</li>\n</ul>\n</li>\n<li>\n<p>Non-asynchronized memory operation</p>\n<ul>\n<li>cudaMemcpy</li>\n</ul>\n</li>\n<li>\n<p>Memory configuration</p>\n<ul>\n<li>cudaDeviceSetCacheConfig</li>\n</ul>\n</li>\n</ul>"
}
