{
  "expireTime": 9007200830651626000,
  "key": "transformer-remark-markdown-html-241e96b710f8f060064d5ea5e63f9799-gatsby-remark-imagesgatsby-remark-responsive-iframegatsby-remark-prismjsgatsby-remark-copy-linked-filesgatsby-remark-smartypantsgatsby-remark-reading-time-",
  "val": "<p>In the last post, we saw how full concurrency can be achieved amongst streams. Here I’d like to talk about how CUDA operations from different streams may also be <strong>interleaved</strong>, which is another programming model often used to effect concurrency.</p>\n<h3>Review CUDA Streams</h3>\n<p>A stream in CUDA is a sequence of operations that execute on the device in the order in which they are issued by the host code.</p>\n<p>While operations within a stream are guaranteed to execute in the prescribed order, operations in different streams can be <strong>interleaved</strong> and, when possible, they can even run <strong>concurrently</strong>.</p>\n<h3>Review CUDA Asynchronous Commands</h3>\n<p>Being asynchronous means that CUDA can perform these operations simultaneously</p>\n<ul>\n<li>\n<p>CUDA Kernel &#x3C;&#x3C;&#x3C;>>></p>\n</li>\n<li>\n<p>cudaMemcpyAsync (HostToDevice)</p>\n</li>\n<li>\n<p>cudaMemcpyAsync (DeviceToHost)</p>\n</li>\n<li>\n<p>Operations on the CPU</p>\n</li>\n</ul>\n<h3>🌟 What Can Happen Simultaneously 🌟</h3>\n<ul>\n<li>\n<p>computation on the device </p>\n<ul>\n<li>lots of CUDA kernels on GPU</li>\n</ul>\n</li>\n<li>\n<p>data transfers between the host and device </p>\n<ul>\n<li><code class=\"language-text\">cudaMemcpyAsyncs</code> both HostToDevice and DeviceToHost</li>\n</ul>\n</li>\n<li>\n<p>computation on the CPU</p>\n</li>\n</ul>\n<h2>No Concurrency</h2>\n<p>The two examples below are completely synchronous:</p>\n<h3>example 1</h3>\n<div class=\"gatsby-highlight\" data-language=\"c\"><pre class=\"language-c\"><code class=\"language-c\"><span class=\"token function\">cudaMemcpy</span><span class=\"token punctuation\">(</span>d_a<span class=\"token punctuation\">,</span> a<span class=\"token punctuation\">,</span> numBytes<span class=\"token punctuation\">,</span> cudaMemcpyHostToDevice<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\nincrement<span class=\"token operator\">&lt;&lt;</span><span class=\"token operator\">&lt;</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span>N<span class=\"token operator\">>></span><span class=\"token operator\">></span><span class=\"token punctuation\">(</span>d_a<span class=\"token punctuation\">)</span>\n<span class=\"token function\">cudaMemcpy</span><span class=\"token punctuation\">(</span>a<span class=\"token punctuation\">,</span> d_a<span class=\"token punctuation\">,</span> numBytes<span class=\"token punctuation\">,</span> cudaMemcpyDeviceToHost<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre></div>\n<ul>\n<li>\n<p>From the perspective of the <strong>device</strong>:</p>\n<ul>\n<li>all three operations are issued to the same (default) stream, thus will execute in the order that they were issued</li>\n</ul>\n</li>\n<li>\n<p>From the perspective of the <strong>host</strong>:</p>\n<ul>\n<li>the implicit data transfers are blocking or synchronous transfers</li>\n<li>the kernel launch is asynchronous</li>\n<li>😳<strong>Since the host-to-device data transfer on the first line is synchronous, the CPU thread will not reach the kernel call on the second line until the host-to-device transfer is complete</strong>. Once the kernel is issued, the CPU thread moves to the third line, but the transfer on that line cannot begin due to the device-side order of execution.</li>\n</ul>\n</li>\n</ul>\n<h3>example 2</h3>\n<div class=\"gatsby-highlight\" data-language=\"c\"><pre class=\"language-c\"><code class=\"language-c\">cudaMemcpy <span class=\"token punctuation\">(</span> dev1<span class=\"token punctuation\">,</span> host1<span class=\"token punctuation\">,</span> size<span class=\"token punctuation\">,</span> H2D <span class=\"token punctuation\">)</span> <span class=\"token punctuation\">;</span>\nkernel2 <span class=\"token operator\">&lt;&lt;</span><span class=\"token operator\">&lt;</span> grid<span class=\"token punctuation\">,</span> block<span class=\"token punctuation\">,</span> <span class=\"token number\">0</span> <span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token punctuation\">(</span> …<span class=\"token punctuation\">,</span> dev2<span class=\"token punctuation\">,</span> … <span class=\"token punctuation\">)</span> <span class=\"token punctuation\">;</span>\nkernel3 <span class=\"token operator\">&lt;&lt;</span><span class=\"token operator\">&lt;</span> grid<span class=\"token punctuation\">,</span> block<span class=\"token punctuation\">,</span> <span class=\"token number\">0</span> <span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token punctuation\">(</span> …<span class=\"token punctuation\">,</span> dev3<span class=\"token punctuation\">,</span> … <span class=\"token punctuation\">)</span> <span class=\"token punctuation\">;</span>\ncudaMemcpy <span class=\"token punctuation\">(</span> host4<span class=\"token punctuation\">,</span> dev4<span class=\"token punctuation\">,</span> size<span class=\"token punctuation\">,</span> D2H <span class=\"token punctuation\">)</span> <span class=\"token punctuation\">;</span></code></pre></div>\n<p>All CUDA operations in the default stream are synchronous.</p>\n<h2>Overlap Device and Host Computation</h2>\n<p>The asynchronous behavior of kernel launches from the host’s perspective makes <strong>overlapping device and host computation</strong> very simple.</p>\n<p>In the modified code below, an independent CPU computation is added:</p>\n<div class=\"gatsby-highlight\" data-language=\"c\"><pre class=\"language-c\"><code class=\"language-c\"><span class=\"token function\">cudaMemcpy</span><span class=\"token punctuation\">(</span>d_a<span class=\"token punctuation\">,</span> a<span class=\"token punctuation\">,</span> numBytes<span class=\"token punctuation\">,</span> cudaMemcpyHostToDevice<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\nincrement<span class=\"token operator\">&lt;&lt;</span><span class=\"token operator\">&lt;</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span>N<span class=\"token operator\">>></span><span class=\"token operator\">></span><span class=\"token punctuation\">(</span>d_a<span class=\"token punctuation\">)</span>\n<span class=\"token function\">myCpuFunction</span><span class=\"token punctuation\">(</span>b<span class=\"token punctuation\">)</span>\n<span class=\"token function\">cudaMemcpy</span><span class=\"token punctuation\">(</span>a<span class=\"token punctuation\">,</span> d_a<span class=\"token punctuation\">,</span> numBytes<span class=\"token punctuation\">,</span> cudaMemcpyDeviceToHost<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre></div>\n<p>As soon as the <code class=\"language-text\">increment()</code> kernel is launched on the device the CPU thread executes myCpuFunction(), <strong>overlapping its execution on the CPU with the kernel execution on the GPU</strong>.</p>\n<p>Whether the host function or device kernel completes first doesn’t affect the subsequent device-to-host transfer, which will begin only after the kernel completes.  </p>\n<p>From the perspective of the device, nothing has changed from the previous example; the device is completely unaware of myCpuFunction().</p>\n<h4>special note</h4>\n<p>GPU kernels are asynchronous with host by default.</p>\n<h2>Asynchronous with Streams</h2>\n<div class=\"gatsby-highlight\" data-language=\"c\"><pre class=\"language-c\"><code class=\"language-c\">cudaStream_t stream1<span class=\"token punctuation\">,</span> stream2<span class=\"token punctuation\">,</span> stream3<span class=\"token punctuation\">,</span> stream4 <span class=\"token punctuation\">;</span>\ncudaStreamCreate <span class=\"token punctuation\">(</span> <span class=\"token operator\">&amp;</span>stream1<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">;</span>\n<span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span>\ncudaMalloc <span class=\"token punctuation\">(</span> <span class=\"token operator\">&amp;</span>dev1<span class=\"token punctuation\">,</span> size <span class=\"token punctuation\">)</span> <span class=\"token punctuation\">;</span>\ncudaMallocHost <span class=\"token punctuation\">(</span> <span class=\"token operator\">&amp;</span>host1<span class=\"token punctuation\">,</span> size <span class=\"token punctuation\">)</span> <span class=\"token punctuation\">;</span> <span class=\"token comment\">// pinned memory required on host</span>\n…\ncudaMemcpyAsync <span class=\"token punctuation\">(</span> dev1<span class=\"token punctuation\">,</span> host1<span class=\"token punctuation\">,</span> size<span class=\"token punctuation\">,</span> H2D<span class=\"token punctuation\">,</span> stream1 <span class=\"token punctuation\">)</span> <span class=\"token punctuation\">;</span> \nkernel2 <span class=\"token operator\">&lt;&lt;</span><span class=\"token operator\">&lt;</span> grid<span class=\"token punctuation\">,</span> block<span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> stream2 <span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token punctuation\">(</span> …<span class=\"token punctuation\">,</span> dev2<span class=\"token punctuation\">,</span> … <span class=\"token punctuation\">)</span> <span class=\"token punctuation\">;</span>\nkernel3 <span class=\"token operator\">&lt;&lt;</span><span class=\"token operator\">&lt;</span> grid<span class=\"token punctuation\">,</span> block<span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> stream3 <span class=\"token operator\">>></span><span class=\"token operator\">></span> <span class=\"token punctuation\">(</span> …<span class=\"token punctuation\">,</span> dev3<span class=\"token punctuation\">,</span> … <span class=\"token punctuation\">)</span> <span class=\"token punctuation\">;</span>\ncudaMemcpyAsync <span class=\"token punctuation\">(</span> host4<span class=\"token punctuation\">,</span> dev4<span class=\"token punctuation\">,</span> size<span class=\"token punctuation\">,</span> D2H<span class=\"token punctuation\">,</span> stream4 <span class=\"token punctuation\">)</span> <span class=\"token punctuation\">;</span>\nsome_CPU_method <span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span></code></pre></div>\n<p>The use of different, non-default streams make the code above fully asynchronous/concurrent. The four streams are potentially overlapped.</p>\n<p>Note that: data used by concurrent operations should be independent.</p>\n<h2>Overlap Computation and Data Transfers</h2>\n<h3>requirements</h3>\n<ul>\n<li>\n<p>The kernel execution and the data transfer to be overlapped must <strong>both occur in different, non-default streams</strong>.</p>\n</li>\n<li>\n<p>Must <code class=\"language-text\">cudaMemcpyAsync</code> with host from ‘pinned’ memory</p>\n<ul>\n<li>Page-locked memory</li>\n<li>Allocated using cudaMallocHost() or cudaHostAlloc()</li>\n</ul>\n</li>\n<li>\n<p>Sufficient resources must be available</p>\n<ul>\n<li>cudaMemcpyAsyncs in different directions</li>\n<li>Device resources (SMEM, registers, blocks, etc.) </li>\n<li>The device must be capable of “concurrent copy and execution”</li>\n</ul>\n</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"c\"><pre class=\"language-c\"><code class=\"language-c\"><span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> i <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> i <span class=\"token operator\">&lt;</span> nStreams<span class=\"token punctuation\">;</span> <span class=\"token operator\">++</span>i<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span>\n  <span class=\"token keyword\">int</span> offset <span class=\"token operator\">=</span> i <span class=\"token operator\">*</span> streamSize<span class=\"token punctuation\">;</span>\n  <span class=\"token function\">cudaMemcpyAsync</span><span class=\"token punctuation\">(</span><span class=\"token operator\">&amp;</span>d_a<span class=\"token punctuation\">[</span>offset<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token operator\">&amp;</span>a<span class=\"token punctuation\">[</span>offset<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> streamBytes<span class=\"token punctuation\">,</span> cudaMemcpyHostToDevice<span class=\"token punctuation\">,</span> stream<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n  kernel<span class=\"token operator\">&lt;&lt;</span><span class=\"token operator\">&lt;</span>streamSize<span class=\"token operator\">/</span>blockSize<span class=\"token punctuation\">,</span> blockSize<span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> stream<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token operator\">>></span><span class=\"token operator\">></span><span class=\"token punctuation\">(</span>d_a<span class=\"token punctuation\">,</span> offset<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n  <span class=\"token function\">cudaMemcpyAsync</span><span class=\"token punctuation\">(</span><span class=\"token operator\">&amp;</span>a<span class=\"token punctuation\">[</span>offset<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token operator\">&amp;</span>d_a<span class=\"token punctuation\">[</span>offset<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> streamBytes<span class=\"token punctuation\">,</span> cudaMemcpyDeviceToHost<span class=\"token punctuation\">,</span> stream<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token punctuation\">}</span></code></pre></div>\n<p>In the modified code, we break up the array of size N into chunks of streamSize elements. Since the kernel operates independently on all elements, each of the chunks can be processed independently. </p>\n<p><a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/8ed12f93dd65d2f535290fa80bf564f7/840ba/compute&data.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-wrapper\"\n    style=\"position: relative; display: block;  max-width: 590px; margin-left: auto; margin-right: auto;\"\n  >\n    <span\n      class=\"gatsby-resp-image-background-image\"\n      style=\"padding-bottom: 10.409556313993175%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAACCAYAAABYBvyLAAAACXBIWXMAABYlAAAWJQFJUiTwAAAAbklEQVQI14XDsQrCMBRA0X62m4tzQRCcXKSTIKhPHAy0aR5qh9okL+LvXPEDxAOnUlWuzqF9x7irKac5gy4425pNuCCPkeFZSHEipfR3FUJARPDek19v7N5isqIcZmhcsp+2NDfHsY+YZfJ3/v0DWhaEIPkWZXoAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n    ></span>\n    <img\n        class=\"gatsby-resp-image-image\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;box-shadow:inset 0px 0px 0px 400px white;\"\n        alt=\"overlap computetion and data transfers\"\n        title=\"\"\n        src=\"/static/8ed12f93dd65d2f535290fa80bf564f7/d7711/compute&data.png\"\n        srcset=\"/static/8ed12f93dd65d2f535290fa80bf564f7/a695b/compute&data.png 148w,\n/static/8ed12f93dd65d2f535290fa80bf564f7/2f273/compute&data.png 295w,\n/static/8ed12f93dd65d2f535290fa80bf564f7/d7711/compute&data.png 590w,\n/static/8ed12f93dd65d2f535290fa80bf564f7/5e66f/compute&data.png 885w,\n/static/8ed12f93dd65d2f535290fa80bf564f7/840ba/compute&data.png 1172w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n      />\n  </span>\n  </a></p>\n<p>The number of (non-default) streams used is <code class=\"language-text\">nStreams=N/streamSize</code>. In the picture above there are 4 streams.</p>\n<h2>Measure Performance Gain</h2>\n<p><a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/abac83195ebc6cd43dd0e4a6ca8bf8c5/b1073/interleave.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-wrapper\"\n    style=\"position: relative; display: block;  max-width: 590px; margin-left: auto; margin-right: auto;\"\n  >\n    <span\n      class=\"gatsby-resp-image-background-image\"\n      style=\"padding-bottom: 49.193548387096776%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAABYlAAAWJQFJUiTwAAACH0lEQVQoz02SW08UQRBGe6Z7emd67tfd2V1QWVgElxjigwgS0BcVEU24hEhM1Gff/fnH2oVEHk6qeqb7S9VXpVymGM09ug2P4aaSKMwU4x1F/1wRJgrPU/ha8O/xHkf5rpQicoaTsw2UHXgMYrXC5R5htjx7RNl9bow8EkHt/4/mMSKo5Y5zlvGkFUFr6UdjxuMpLjK0raZpNHXtS/QpOkPaCLUh6zSuybBlLVTYqsEWBQMpygT3wipJLNNpx3A4ou8NNzeGyyvL1bXm5tZweJuzuCjY+1Lw6jqmPz/FnPzGnnwneP+L8OAraaIpKkecGlQQGGnLZxAq4lhLrtHaiCeBXNCCj5PoEn9FmmnyQpPlmlQoK0vdOqbrDVkZoCI3IEkyabVlIh5UbU6cFOJJSdt1jPqStUnK+tOKyXpNUVsaESwKs6J5EI/F70xyNRwl0nLP1nzG64M5bw57jo5mHB/P+Hi2zdsPW+y9O2Xxcpf9vSnDcUIhlSxE7DA39FVALhS1IFHVreXZrJUKhlJhwOczxSfh4pvi/EKx/zFi48855d1P7OwFqVhQFop1eTwrZYhLwfpBUFBlFUlblQylI4kTbCjeZZYwCmXXQtkxQ14qdi632P57R3X1g2S2TVp6xCKWlcv/mkRWLq9kKM5F4lcohhsq8adqQmkpJXaO1cACvRLPnMeTXZ/+aEG6uUtVLSdr6YYy+XHGfGeNsh7wDx/W9WHF5jZtAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n    ></span>\n    <img\n        class=\"gatsby-resp-image-image\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;box-shadow:inset 0px 0px 0px 400px white;\"\n        alt=\"interleave\"\n        title=\"\"\n        src=\"/static/abac83195ebc6cd43dd0e4a6ca8bf8c5/d7711/interleave.png\"\n        srcset=\"/static/abac83195ebc6cd43dd0e4a6ca8bf8c5/a695b/interleave.png 148w,\n/static/abac83195ebc6cd43dd0e4a6ca8bf8c5/2f273/interleave.png 295w,\n/static/abac83195ebc6cd43dd0e4a6ca8bf8c5/d7711/interleave.png 590w,\n/static/abac83195ebc6cd43dd0e4a6ca8bf8c5/5e66f/interleave.png 885w,\n/static/abac83195ebc6cd43dd0e4a6ca8bf8c5/466de/interleave.png 1180w,\n/static/abac83195ebc6cd43dd0e4a6ca8bf8c5/76253/interleave.png 1770w,\n/static/abac83195ebc6cd43dd0e4a6ca8bf8c5/b1073/interleave.png 1984w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n      />\n  </span>\n  </a></p>\n<p><a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/c4b7386740970ea3d09914e128fd5e5f/ed8b6/moreinterleave.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-wrapper\"\n    style=\"position: relative; display: block;  max-width: 590px; margin-left: auto; margin-right: auto;\"\n  >\n    <span\n      class=\"gatsby-resp-image-background-image\"\n      style=\"padding-bottom: 49.14914914914915%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAABYlAAAWJQFJUiTwAAACQklEQVQozz2SSU8bQRCFe1Z7Vo/Hy3iMNzzGdoKx2Q0OxAnIKISIAxJCicIxp0SJxDHHHPK3v9SAlcNT9VL16vXrUn6skQ41kkyj2lXUM0UyUNT6L+soVWiaQtfXMNZYrw1DQylFXHdYfsxQdlGjGCgKvsKR6JU0vEgXvERX9qaVIy9WKCHXhEzlUC/N8ugHDmmziioUijSSJu1Wl3I5xjINTEPHks62qZ5RKGgUirrc+2z2ujSTBj0pzvotOt3mM5HjmBiSqwJhns8Trq9Tzs5jBochg7lHd9HAnZ5TnL2j2MmkQJ4f26zu65x+v6K6eEuj16TTqVFPSqLQwvUMlOt67O13WF1tcXHZZ385Y2d+zGjUYTQdk2Yd3KgoVhhEgcZDT/F0W+BhNWGjFqOJFa6ce6GBLznKtk3abY2DA1uU2gwvNkhuL/Gnx/jlgChShJFBWDZppAE77SaDqMZ+ErLTj8mGKVHFfr7PoWp1V4y20JRBEIpsxyaILRrnE0qDTAg1ompeYIlXDp8eUm5/9jj+dka2uhEfu8ShohybYomFajQDFm+6vL05Yrw7Z7i5yeT1kPEgZTRsiaoEP7T/q0gHOuf3Jnd/K8z/zKh9ucTefoVTKUiOEIYlh8m2w510Pfn1gerJKdVWWbp5OOKvoRtYlqiWvJoULEXJMjD4PDb4/Wjw9CPm6+MhR7t7lMrioec72DI6UahxtNDpvJ9gJ33M9XzlA+y4Yrj4GAkmFZ2pYCwfMA5NtmVOZ+LzljTLP+UfHuT+rpCbE/UAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n    ></span>\n    <img\n        class=\"gatsby-resp-image-image\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;box-shadow:inset 0px 0px 0px 400px white;\"\n        alt=\"moreinterleave\"\n        title=\"\"\n        src=\"/static/c4b7386740970ea3d09914e128fd5e5f/d7711/moreinterleave.png\"\n        srcset=\"/static/c4b7386740970ea3d09914e128fd5e5f/a695b/moreinterleave.png 148w,\n/static/c4b7386740970ea3d09914e128fd5e5f/2f273/moreinterleave.png 295w,\n/static/c4b7386740970ea3d09914e128fd5e5f/d7711/moreinterleave.png 590w,\n/static/c4b7386740970ea3d09914e128fd5e5f/5e66f/moreinterleave.png 885w,\n/static/c4b7386740970ea3d09914e128fd5e5f/466de/moreinterleave.png 1180w,\n/static/c4b7386740970ea3d09914e128fd5e5f/76253/moreinterleave.png 1770w,\n/static/c4b7386740970ea3d09914e128fd5e5f/ed8b6/moreinterleave.png 1998w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n      />\n  </span>\n  </a></p>\n<p>To read more about concurrency and streams:\n<a href=\"https://developer.download.nvidia.com/CUDA/training/StreamsAndConcurrencyWebinar.pdf\">https://developer.download.nvidia.com/CUDA/training/StreamsAndConcurrencyWebinar.pdf</a>\n<a href=\"https://devblogs.nvidia.com/how-overlap-data-transfers-cuda-cc/\">https://devblogs.nvidia.com/how-overlap-data-transfers-cuda-cc/</a></p>"
}
