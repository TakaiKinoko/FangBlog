{
  "expireTime": 9007200830736169000,
  "key": "transformer-remark-markdown-html-2cb2cf43b13e07d044cb625ff58b9bbf-gatsby-remark-imagesgatsby-remark-responsive-iframegatsby-remark-prismjsgatsby-remark-copy-linked-filesgatsby-remark-smartypantsgatsby-remark-reading-time-",
  "val": "<p>Motivated by a CUDA puzzle I tried to solve today, I’d like to talk more about resource assignment. </p>\n<h2>A Puzzle</h2>\n<h3>problem</h3>\n<p>Adding two big arrays element-wise.</p>\n<h3>settings</h3>\n<ul>\n<li>Suppose a GPU has 8 SMs</li>\n<li>Each SM has 32 SPs</li>\n<li>Warp size is 16, instead of 32</li>\n<li>kernel adds one element from each array together</li>\n</ul>\n<h3>questions</h3>\n<p>Say if we were to compute everything sequentially, it takes time <code class=\"language-text\">t</code> to finish.</p>\n<h4>scenario 1</h4>\n<p>Assume that 256 threads are enough to keep all SPs in the SM busy all the time. What is the amount of time it’d take to perform the computation for <strong>one block of 1024</strong> threads? </p>\n<h4>scenario 2</h4>\n<p>What if we use <strong>two blocks</strong> of <strong>512 threads</strong> each?</p>\n<h2>CUDA Resource Assignment</h2>\n<p>In order to see how the two scenarios differ, I thougth it was necessary to take another look at how resources are assigned to blocks and warps.</p>\n<h3>sequence of events</h3>\n<ol>\n<li>kernel launch</li>\n<li>CUDA runtime generates the corresponding grid of threads.</li>\n<li>threads are assigned to execution resources on a block-to-block basis.</li>\n</ol>\n<h3>blocks and SMs</h3>\n<p>Multiple blocks can be assigned to each SM. But each device sets a limit on the number of blocks that can be assigned to each SM.</p>\n<p>Say that a device allows 8 blocks to be assigned to each SM. In situations where there’s a <strong>shortage of one or more types of resources</strong> needed for the simultaneous execution of 8 blocks, the CUDA runtime automatically reduces the number of blocks assigned to each SM until their combined resource usage falls below the limit.</p>\n<p>The runtime system:</p>\n<ul>\n<li>maintains a list of blocks that need to execute</li>\n<li>assigns new block to SMs as previously assigned blocks complete execution.</li>\n</ul>\n<p><img src=\"./blocks.jpg\" alt=\"blocks to SM\"></p>\n<h2>Puzzle Solution</h2>\n<h3>scenario 1</h3>\n<p>First, a block is assigned to <strong>one</strong> SM, not more. So if one block contains enough threads to occupy all this SM’s capacity, then there’ll be lots of warps in queue for any of the SPs to free up.</p>\n<p>So even though there are 1024 threads in the block, the number of threads actually get executed here is capped by how much the SM can handle at any time. Here it’s 256.</p>\n<p>Thus the time is <code class=\"language-text\">t/256</code>.</p>\n<h3>scenario 2</h3>\n<p>When we run 1024 threads on two different blocks, two SMs can service them. Hence the amount of time will be half of that taken for just one SM of 256 threads. </p>\n<p>Thus the time is <code class=\"language-text\">t/512</code>.</p>"
}
