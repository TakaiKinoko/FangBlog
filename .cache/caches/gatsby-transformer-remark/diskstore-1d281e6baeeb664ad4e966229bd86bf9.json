{
  "expireTime": 9007200830432997000,
  "key": "transformer-remark-markdown-html-eb260de4803a774de264b601d58fe2df-gatsby-remark-imagesgatsby-remark-responsive-iframegatsby-remark-prismjsgatsby-remark-copy-linked-filesgatsby-remark-smartypantsgatsby-remark-reading-time-",
  "val": "<h2>TABLE OF CONTENTS</h2>\n<ol>\n<li>\n<p>Concepts</p>\n<ol>\n<li>key abstractions</li>\n<li>granularity</li>\n</ol>\n</li>\n<li>\n<p>CUDA Architecture</p>\n<ol>\n<li>kernel execution</li>\n<li>thread organization</li>\n<li>blocks</li>\n<li>SMs</li>\n<li>warp</li>\n<li>execution picture</li>\n</ol>\n</li>\n<li>Thread ID</li>\n<li>Memory Hierarchy</li>\n</ol>\n<h2>Some Concepts</h2>\n<h3>three key abstractions</h3>\n<ul>\n<li>\n<p>a hierarchy of thread groups</p>\n</li>\n<li>\n<p>shared memories</p>\n</li>\n<li>\n<p>barrier synchronization</p>\n</li>\n</ul>\n<h3>granularity</h3>\n<ul>\n<li>\n<p>In parallel computing, granularity means the amount of <strong>computation</strong> in relation to <strong>communication (or transfer) of data</strong>.</p>\n<ul>\n<li>\n<p>fine-grained: individual tasks are small in terms of code size and execution time.</p>\n</li>\n<li>\n<p>coarse-grained: larger amounts of computation, infrequent data communication.</p>\n</li>\n</ul>\n</li>\n<li>\n<p>CUDA abstraction:</p>\n<ul>\n<li>\n<p><strong>fine-grained</strong> data parallelism and thread parallelism nested within <strong>roarse-grained</strong> data parallelism and task parallelism.</p>\n</li>\n<li>\n<p>programmers partition the problem into <strong>coarse sub-problems</strong> that can be solved independently in prallel by <strong>blocks of threads</strong> and each sub-problem into finer pieces that can be solved cooperatively in parallel by <strong>threads within the block</strong>.</p>\n</li>\n</ul>\n</li>\n</ul>\n<h2>CUDA Architecture</h2>\n<h3>kernel execution</h3>\n<ul>\n<li>\n<p>Executed in parallel by an array of threads, all of which run the same code.</p>\n</li>\n<li>\n<p>Each thread has an ID which is used to compute memory addresses and make control decisions.</p>\n</li>\n</ul>\n<h3>thread organization - grid of blocks</h3>\n<ul>\n<li>\n<p>Threads are arranged as a <strong>grid</strong> of <strong>blocks</strong>.</p>\n</li>\n<li>\n<p>Blocks in a grid are completely <strong>independent</strong> which means they can be executed in any order, in parallel or in series.</p>\n</li>\n<li>\n<p>The independence allows thread blocks to be scheduled in <em>any order</em> across <em>any number of cores</em>.</p>\n</li>\n</ul>\n<h3>block</h3>\n<ul>\n<li>\n<p>Threads from the same block have access to a <strong>shared memory</strong> .</p>\n</li>\n<li>\n<p>Execution of threads from the same block can be <strong>synchronized</strong> (to coordinate memory accesses).</p>\n</li>\n</ul>\n<h3>SMs</h3>\n<p>The CUDA architecture is built around a scalable array of multithreaded Streaming Multiprocessors. </p>\n<p>Each SM has:</p>\n<ul>\n<li>\n<p>a set of execution units</p>\n</li>\n<li>\n<p>a set of registers </p>\n</li>\n<li>\n<p>a chunch of shared memory.</p>\n</li>\n</ul>\n<h3>üßêwarp</h3>\n<p>Warp is the <strong>basic unit of execution</strong> in an NVIDIA GPU.</p>\n<p>A warp is a collection of threads, 32 in current NVIDIA implementations. </p>\n<ul>\n<li>\n<p>threads within a warp a executed simultaneously by an SM. </p>\n</li>\n<li>\n<p>multiple warps can be executed on an SM at once.</p>\n</li>\n</ul>\n<p>The mapping between warps and thread blocks can affect the performance of the kernel.\n<strong>It‚Äôs usually good the keep the size of a block a multiple of 32</strong>.</p>\n<h3>picture the process of execution</h3>\n<ol>\n<li>\n<p>CUDA program on the host CPU invokes a <strong>kernel grid</strong></p>\n</li>\n<li>\n<p>blocks in the grid are enumerated and distributed to SMs with available execution capacity</p>\n</li>\n<li>\n<p>the threads of a block execute concurrently on one SM </p>\n</li>\n<li>\n<p>as thread blocks terminate, new blocks are launched on the vacated SMs</p>\n</li>\n</ol>\n<h2>Thread ID</h2>\n<p>TODO</p>\n<h2>Memory Hierarchy</h2>\n<h3>between CPU and GPU</h3>\n<p>CPU and GPU has <strong>separate memory spaces</strong> => data must be moved from CPU to GPU before computation starts, as well as moved back to CPU once processing has completed.</p>\n<h3>global memory</h3>\n<ol>\n<li>\n<p>accessible to all threads as well as the host (CPU)</p>\n</li>\n<li>\n<p><strong>host</strong> allocate and deallocate the global memory</p>\n</li>\n<li>\n<p>data is first initialized here for the GPU to work on <img src=\"./global.jpg\" alt=\"global\"></p>\n</li>\n</ol>\n<p><a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/ba26bdec003901409af2a4291fe43d6a/ee5f8/sandy-miller.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-wrapper\"\n    style=\"position: relative; display: block;  max-width: 590px; margin-left: auto; margin-right: auto;\"\n  >\n    <span\n      class=\"gatsby-resp-image-background-image\"\n      style=\"padding-bottom: 66.66666666666666%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAANABQDASIAAhEBAxEB/8QAGAAAAwEBAAAAAAAAAAAAAAAAAAMFAgT/xAAVAQEBAAAAAAAAAAAAAAAAAAABAP/aAAwDAQACEAMQAAAB2tXAVInA/wD/xAAZEAEBAAMBAAAAAAAAAAAAAAACAQAREgP/2gAIAQEAAQUC23U4cV6r9UEr1ZdZ/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAGhAAAgMBAQAAAAAAAAAAAAAAAAECETEh4f/aAAgBAQAGPwLTsi44Ui2j0//EABoQAQEBAQADAAAAAAAAAAAAAAERACExYXH/2gAIAQEAAT8hvLwPZzFeHBaqMqM+uoVL70cLn//aAAwDAQACAAMAAAAQ1C//xAAXEQADAQAAAAAAAAAAAAAAAAAAAREh/9oACAEDAQE/EFJpEf/EABYRAQEBAAAAAAAAAAAAAAAAAAEQIf/aAAgBAgEBPxB1n//EABsQAQADAQADAAAAAAAAAAAAAAEAESFBMVHR/9oACAEBAAE/EOCQoMe2LdZy1YeqRSTdlNsbgknlIvsCrJd4p//Z'); background-size: cover; display: block;\"\n    ></span>\n    <img\n        class=\"gatsby-resp-image-image\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;box-shadow:inset 0px 0px 0px 400px white;\"\n        alt=\"Image by Sandy Miller\"\n        title=\"\"\n        src=\"/static/ba26bdec003901409af2a4291fe43d6a/1697e/sandy-miller.jpg\"\n        srcset=\"/static/ba26bdec003901409af2a4291fe43d6a/a2cfd/sandy-miller.jpg 148w,\n/static/ba26bdec003901409af2a4291fe43d6a/3348f/sandy-miller.jpg 295w,\n/static/ba26bdec003901409af2a4291fe43d6a/1697e/sandy-miller.jpg 590w,\n/static/ba26bdec003901409af2a4291fe43d6a/6a00a/sandy-miller.jpg 885w,\n/static/ba26bdec003901409af2a4291fe43d6a/46304/sandy-miller.jpg 1180w,\n/static/ba26bdec003901409af2a4291fe43d6a/07574/sandy-miller.jpg 1770w,\n/static/ba26bdec003901409af2a4291fe43d6a/ee5f8/sandy-miller.jpg 5184w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n      />\n  </span>\n  </a></p>\n<h3>shared memory</h3>"
}
