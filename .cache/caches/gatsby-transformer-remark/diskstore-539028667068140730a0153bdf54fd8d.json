{
  "expireTime": 9007200830484798000,
  "key": "transformer-remark-markdown-html-8e2f18592d5425eec78a1dcd06ab10dc-gatsby-remark-imagesgatsby-remark-responsive-iframegatsby-remark-prismjsgatsby-remark-copy-linked-filesgatsby-remark-smartypantsgatsby-remark-reading-time-",
  "val": "<p>This post talks about a key factor to CUDA kernel performace: accessing data in the globle memory.</p>\n<p>CUDA applications tend to process a massive amount of data from the global memory within a short period of time. </p>\n<p><strong>Tiling</strong> techniques are engineered that utilize <strong>shared memories</strong> to reduce the total amount of data that must be acessed from the global memory (read about tiling techniques here <a href=\"/cuda6-tiling\">The CUDA Parallel Programming Model - 6.Tiling</a>). </p>\n<p>I this post we talk about <strong>memory coalescing</strong> techniques that can more effectively move data from the global memory into <strong>shared memories and registers</strong>. </p>\n<p>Memory coalescing techniques are often used <em>in conjunction with tiling techniques</em> to allow CUDA devices to reach their performance potential by more efficiently utilizing the global memory bandwidth.</p>\n<h2>Global Memory Bandwidth</h2>\n<p>The global memory of a CUDA device is implemented with DRAMs.</p>\n<h4>DRAM is slow</h4>\n<p>Data bits are stored in DRAM cells that are small capacitors, where the presence or absence of a <em>tiny amount of electrical charge</em> distinguishes between 0 and 1. </p>\n<p>Reading data from a DRAM cell requires the small capacitor to use its tiny electrical charge to drive a highly capacitive line leading to a sensor and set off its detection mechanism that determines whether a sufficient amount of charge is present in the capacitor to qualify as a ‚Äú1‚Äù. This process takes 10 s of nanoseconds in modern DRAM chips. <strong>This is in sharp contrast with the sub-nanosecond clock cycle time of modern computing devices</strong>. </p>\n<h4>parallelism and memory access throughput</h4>\n<p>Because this is a very slow process relative to the desired data access speed (sub-nanosecond access per byte), modern DRAMs <strong>use parallelism to increase their rate of data access</strong>, commonly referred to as <em>memory access throughput</em>.</p>\n<h4>DRAM bursts</h4>\n<p>Each time a DRAM location is accessed, <strong>a range of consecutive locations that includes the requested location are actually accessed</strong>. </p>\n<p>Many sensors are provided in each DRAM chip and they work in parallel. Each senses the content of a bit within these consecutive locations. </p>\n<p>Once detected by the sensors, the data from all these consecutive locations can be transferred at very high-speed to the processor. These consecutive locations accessed and delivered are referred to as <strong>DRAM bursts</strong>. </p>\n<h4>motivation</h4>\n<p>If an application makes focused use of data from these bursts, the DRAMs can supply the data at a much higher rate than if a truly random sequence of locations were accessed.</p>\n<h2>Memory Coalescing</h2>\n<p>Current CUDA devices employ a technique that allows the programmers to achieve high global memory access efficiency by <strong>organizing memory accesses of threads into favorable patterns</strong>. </p>\n<h3>how?</h3>\n<ul>\n<li>\n<p>This technique takes advantage of the fact that <strong>threads in a warp execute the same instruction at any given point in time</strong>. </p>\n</li>\n<li>\n<p>The most favorable access pattern is achieved when all threads in a warp access consecutive global memory locations. </p>\n</li>\n<li>\n<p>When all threads in a warp execute a load instruction, the hardware detects whether they access consecutive global memory locations. If that‚Äôs the case, the hardware combines (<strong>coalesces</strong>) all these accesses into a consolidated access to consecutive DRAM locations. </p>\n</li>\n<li>\n<p>For example, for a given load instruction of a warp, if thread 0 accesses global memory location N2, thread 1 location N+1, thread 2 location N+2, and so on, all these accesses will be coalesced into a single request for consecutive locations when accessing the DRAMs. </p>\n</li>\n<li>\n<p>Such coalesced access allows the DRAMs to deliver data as a burst.</p>\n</li>\n</ul>\n<h3>how to effectively use the coalescing hardware?</h3>\n<p>Recall from <a href=\"/cuda2-warp\">The CUDA Parallel Programming Model - 2. Warps</a> that multidimensional array elements in CUDA are placed into the linearly addressed memory space according to the <strong>row-major</strong> convention. </p>\n<h4>matrix multiplication example</h4>\n<p>Say we have a kernel that computes <code class=\"language-text\">M x N</code>, where both M and N are 2D row-major array.</p>\n<p>Each thread accesses a row of the M array (matrix A below) and a column of the N array (matrix B below).</p>\n<p><a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/d3f4c1e8944826ac08d7b974ad9718b8/e6b24/pattern.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-wrapper\"\n    style=\"position: relative; display: block;  max-width: 590px; margin-left: auto; margin-right: auto;\"\n  >\n    <span\n      class=\"gatsby-resp-image-background-image\"\n      style=\"padding-bottom: 46.759847522236335%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAJABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAABQABA//EABUBAQEAAAAAAAAAAAAAAAAAAAEC/9oADAMBAAIQAxAAAAFLgnsA8vB//8QAGxAAAgIDAQAAAAAAAAAAAAAAAAECMQMREhP/2gAIAQEAAQUCzuRuXn1MdCr/xAAVEQEBAAAAAAAAAAAAAAAAAAAAEf/aAAgBAwEBPwFH/8QAGBEAAgMAAAAAAAAAAAAAAAAAAAECEmH/2gAIAQIBAT8BcSmn/8QAFxAAAwEAAAAAAAAAAAAAAAAAAAExIP/aAAgBAQAGPwJFdK8f/8QAGhAAAQUBAAAAAAAAAAAAAAAAAAERMUFx8P/aAAgBAQABPyFFdMOgobgJCiPD/9oADAMBAAIAAwAAABBr/wD/xAAWEQEBAQAAAAAAAAAAAAAAAAAAAXH/2gAIAQMBAT8Qsaf/xAAWEQEBAQAAAAAAAAAAAAAAAAABEQD/2gAIAQIBAT8QRbdWf//EABsQAQACAwEBAAAAAAAAAAAAAAEAESExoXHB/9oACAEBAAE/EAaI5sWHUL1DrhdRXCr6zgZ8zhT/2Q=='); background-size: cover; display: block;\"\n    ></span>\n    <img\n        class=\"gatsby-resp-image-image\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;box-shadow:inset 0px 0px 0px 400px white;\"\n        alt=\"access pattern\"\n        title=\"\"\n        src=\"/static/d3f4c1e8944826ac08d7b974ad9718b8/1697e/pattern.jpg\"\n        srcset=\"/static/d3f4c1e8944826ac08d7b974ad9718b8/a2cfd/pattern.jpg 148w,\n/static/d3f4c1e8944826ac08d7b974ad9718b8/3348f/pattern.jpg 295w,\n/static/d3f4c1e8944826ac08d7b974ad9718b8/1697e/pattern.jpg 590w,\n/static/d3f4c1e8944826ac08d7b974ad9718b8/e6b24/pattern.jpg 787w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n      />\n  </span>\n  </a></p>\n<h5><strong>M: unfavorable data access pattern</strong></h5>\n<ul>\n<li>\n<p>fig.(A) above illustrates the data access pattern of the M array</p>\n</li>\n<li>\n<p>threads in a warp read adjacent rows</p>\n</li>\n<li>\n<p>during iteration 0, threads in a warp read element 0 of rows 0 through 31. </p>\n</li>\n<li>\n<p>during iteration 1, these same threads read element 1 of rows 0 through 31. </p>\n</li>\n<li>\n<p><strong>None</strong> of the accesses will be coalesced.</p>\n</li>\n</ul>\n<h5><strong>N: favorable data access pattern</strong></h5>\n<ul>\n<li>\n<p>fig.(B) above illustrates the data access pattern of the N array</p>\n</li>\n<li>\n<p>each thread reads a column of N. </p>\n</li>\n<li>\n<p>during iteration 0, threads in warp 0 read element 1 of columns 0 through 31. </p>\n</li>\n<li>\n<p>all these accesses will be coalesced.</p>\n</li>\n</ul>\n<p>üßêüßêüßê</p>"
}
