{
  "expireTime": 9007200830433240000,
  "key": "transformer-remark-markdown-html-c93f97b4d45907fbf4bf9bd13c36c882-gatsby-remark-imagesgatsby-remark-responsive-iframegatsby-remark-prismjsgatsby-remark-copy-linked-filesgatsby-remark-smartypantsgatsby-remark-reading-time-",
  "val": "<h2>TABLE OF CONTENTS</h2>\n<ol>\n<li>\n<p>Concepts</p>\n<ol>\n<li>key abstractions</li>\n<li>granularity</li>\n</ol>\n</li>\n<li>\n<p>CUDA Architecture</p>\n<ol>\n<li>kernel execution</li>\n<li>thread organization</li>\n<li>blocks</li>\n<li>SMs</li>\n<li>warp</li>\n<li>execution picture</li>\n</ol>\n</li>\n<li>Thread ID</li>\n<li>Memory Hierarchy</li>\n</ol>\n<h2>Some Concepts</h2>\n<h3>three key abstractions</h3>\n<ul>\n<li>\n<p>a hierarchy of thread groups</p>\n</li>\n<li>\n<p>shared memories</p>\n</li>\n<li>\n<p>barrier synchronization</p>\n</li>\n</ul>\n<h3>granularity</h3>\n<ul>\n<li>\n<p>In parallel computing, granularity means the amount of <strong>computation</strong> in relation to <strong>communication (or transfer) of data</strong>.</p>\n<ul>\n<li>\n<p>fine-grained: individual tasks are small in terms of code size and execution time.</p>\n</li>\n<li>\n<p>coarse-grained: larger amounts of computation, infrequent data communication.</p>\n</li>\n</ul>\n</li>\n<li>\n<p>CUDA abstraction:</p>\n<ul>\n<li>\n<p><strong>fine-grained</strong> data parallelism and thread parallelism nested within <strong>roarse-grained</strong> data parallelism and task parallelism.</p>\n</li>\n<li>\n<p>programmers partition the problem into <strong>coarse sub-problems</strong> that can be solved independently in prallel by <strong>blocks of threads</strong> and each sub-problem into finer pieces that can be solved cooperatively in parallel by <strong>threads within the block</strong>.</p>\n</li>\n</ul>\n</li>\n</ul>\n<h2>CUDA Architecture</h2>\n<h3>kernel execution</h3>\n<ul>\n<li>\n<p>Executed in parallel by an array of threads, all of which run the same code.</p>\n</li>\n<li>\n<p>Each thread has an ID which is used to compute memory addresses and make control decisions.</p>\n</li>\n</ul>\n<h3>thread organization - grid of blocks</h3>\n<ul>\n<li>\n<p>Threads are arranged as a <strong>grid</strong> of <strong>blocks</strong>.</p>\n</li>\n<li>\n<p>Blocks in a grid are completely <strong>independent</strong> which means they can be executed in any order, in parallel or in series.</p>\n</li>\n<li>\n<p>The independence allows thread blocks to be scheduled in <em>any order</em> across <em>any number of cores</em>.</p>\n</li>\n</ul>\n<h3>block</h3>\n<ul>\n<li>\n<p>Threads from the same block have access to a <strong>shared memory</strong> .</p>\n</li>\n<li>\n<p>Execution of threads from the same block can be <strong>synchronized</strong> (to coordinate memory accesses).</p>\n</li>\n</ul>\n<h3>SMs</h3>\n<p>The CUDA architecture is built around a scalable array of multithreaded Streaming Multiprocessors. </p>\n<p>Each SM has:</p>\n<ul>\n<li>\n<p>a set of execution units</p>\n</li>\n<li>\n<p>a set of registers </p>\n</li>\n<li>\n<p>a chunch of shared memory.</p>\n</li>\n</ul>\n<h3>üßêwarp</h3>\n<p>Warp is the <strong>basic unit of execution</strong> in an NVIDIA GPU.</p>\n<p>A warp is a collection of threads, 32 in current NVIDIA implementations. </p>\n<ul>\n<li>\n<p>threads within a warp a executed simultaneously by an SM. </p>\n</li>\n<li>\n<p>multiple warps can be executed on an SM at once.</p>\n</li>\n</ul>\n<p>The mapping between warps and thread blocks can affect the performance of the kernel.\n<strong>It‚Äôs usually good the keep the size of a block a multiple of 32</strong>.</p>\n<h3>picture the process of execution</h3>\n<ol>\n<li>\n<p>CUDA program on the host CPU invokes a <strong>kernel grid</strong></p>\n</li>\n<li>\n<p>blocks in the grid are enumerated and distributed to SMs with available execution capacity</p>\n</li>\n<li>\n<p>the threads of a block execute concurrently on one SM </p>\n</li>\n<li>\n<p>as thread blocks terminate, new blocks are launched on the vacated SMs</p>\n</li>\n</ol>\n<h2>Thread ID</h2>\n<p>TODO</p>\n<h2>Memory Hierarchy</h2>\n<h3>between CPU and GPU</h3>\n<p>CPU and GPU has <strong>separate memory spaces</strong> => data must be moved from CPU to GPU before computation starts, as well as moved back to CPU once processing has completed.</p>\n<h3>global memory</h3>\n<ol>\n<li>\n<p>accessible to all threads as well as the host (CPU)</p>\n</li>\n<li>\n<p><strong>host</strong> allocate and deallocate the global memory</p>\n</li>\n<li>\n<p>data is first initialized here for the GPU to work on </p>\n</li>\n</ol>\n<p><a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/297af877f673dc988df4be6f99c11c79/ff9ab/global.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-wrapper\"\n    style=\"position: relative; display: block;  max-width: 590px; margin-left: auto; margin-right: auto;\"\n  >\n    <span\n      class=\"gatsby-resp-image-background-image\"\n      style=\"padding-bottom: 49.877750611246945%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAABYlAAAWJQFJUiTwAAACjElEQVQoz1WSTU8TURSG+69cuHGhGIOJidGNGxcuJPEzMTEKiR9RjAs1JqKCoMQYNaiJVKSAlHYG2lIoVVo6zLSFfswMHajQllYplk4fL6AmnpuTe86bm5Oc97mOwvoyxnKcH7U8mxSo2qv8EvffeqO+ymZjTWhFoa1RrZfYjoY47+RFHjsVul0qj/pjhLQVHFbJYHJOJTgXR9F1kstZQuoCYS2DWU5RQhV6mmklyddkBmvdANvGrtU49cDH3kvDHGxzs+eii96ROA6jlGNoMseAZOKdtZjUDD5JBp/HV4hZcZZqAeSIidOjMzRdYO6rn4XnXSRe9tLS7qL5psyx2xJNbaO88Szg0L4n6BqM0elUeTuh4QxHedqv0PkpiU8PkGWAPr/Ck48x8S5OxIjSqP6kWi5ztiNA83UPx9tlmlpHeT2WxJH/qTOXmydqaCg5BW0lRkRXdzJVjGD9mmHeUnb6b3qMXCX9x0M4/3SKE3fHudg5zZGbHl65xcDKRoFiRWdLGG4L47dENkRu17VGic2tdeo7WkloBWr1Mg27gV3f4kzHJCfv+bjaG+bwtbHdgcZqGjk8y1RMRTWzLOSzAkiSb4k0ZmUXyryRYkZNEEqkWK6Y/I1zj4M0i0H/rWwKKKMhC9fEkjDfIhg3GBw3GfLnUcR3suwg41GTz7LOyMw2FB/Jzg7iL3o4fWeIQzckjt7ycuDqF157xMDE9xTdg4s8c2bomxAIwgKSM033wBKBbJiMPch7/yJd/Sl6XAZRXaH+o8RGsUjLQx/7W93CPy/7Lo/srpxdjyEn3MiaRCAtMW14keNeJNFH825S1WGCGUloEl5tTICa/bfy/Q8RLjwJcKVnShD3MxzK8htsSKQ9zj7uQAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n    ></span>\n    <img\n        class=\"gatsby-resp-image-image\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;box-shadow:inset 0px 0px 0px 400px white;\"\n        alt=\"global\"\n        title=\"\"\n        src=\"/static/297af877f673dc988df4be6f99c11c79/d7711/global.png\"\n        srcset=\"/static/297af877f673dc988df4be6f99c11c79/a695b/global.png 148w,\n/static/297af877f673dc988df4be6f99c11c79/2f273/global.png 295w,\n/static/297af877f673dc988df4be6f99c11c79/d7711/global.png 590w,\n/static/297af877f673dc988df4be6f99c11c79/5e66f/global.png 885w,\n/static/297af877f673dc988df4be6f99c11c79/466de/global.png 1180w,\n/static/297af877f673dc988df4be6f99c11c79/ff9ab/global.png 1636w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n      />\n  </span>\n  </a></p>\n<h3>shared memory</h3>\n<ol>\n<li>\n<p>each thread block has its own </p>\n</li>\n<li>\n<p>much faster than local or global memory</p>\n</li>\n<li>\n<p>requires special handling to get max performance</p>\n</li>\n<li>\n<p>only exists for the lifetime of the block </p>\n</li>\n</ol>\n<p><a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/9b39ccb8f5ff817f1518b632cd19541c/ce4da/shared.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-wrapper\"\n    style=\"position: relative; display: block;  max-width: 590px; margin-left: auto; margin-right: auto;\"\n  >\n    <span\n      class=\"gatsby-resp-image-background-image\"\n      style=\"padding-bottom: 38.8268156424581%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAABYlAAAWJQFJUiTwAAAB9ElEQVQoz12S/UtTURyH/bP6NX+QCpOg+gOCIAIpo9DKSqUiQqgoeqEif4hSVKKUcXUvuplrby6dy83tznRvXe+067ze6e509+k4Z1AHHh44fPicc/ieBsuyqFYt/l971SqHu4fWtsp8TyrMLxUE6l/mUipb22Yt01A0Ksj5HUI/sswmVvFHlplZyJHMl9F1QzSX2TV3RHQXV1CmsWOM5jtOTtzex8HxWw6OddqJpFYPCtc3TfwLGuNeGWkqjs0Tq9kb1fgVC1NW4hhKCtYSjI5InH3wlY53YS489dH+doarb2Y4c2+S2WS9UN0wcYZU7P4c9kAeyZvGITzmV8gl4+z8zqKvK1jFHLYxD02dTlqfB7j4zMellwFaXwQ42eUUN1QPChXNZNSnMjKd55M7zRfhz1MZbEGNzGKMcuEnuprG0laQxt00Xndw/sk3Lr8K0iY498hL000784eF+fUyg54C/a4sH52ZmvuFB6c10rEoe2syJXVFTGQJSXLR0uPmRl+Ya+KpV16Haj59d5I5uV6YLlR4PLRE74co9/tCPHwfEczROyCTWlapmluUjP3hbDPqjnCkzcapnglaul00d4kDuic42i4RTigHhcZ2hURWJ1ljs26dxXSRkmn9820yBYOBiUWGp2SGPEmGBUN11jaMWuYP/doe/bDbqx8AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n    ></span>\n    <img\n        class=\"gatsby-resp-image-image\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;box-shadow:inset 0px 0px 0px 400px white;\"\n        alt=\"global\"\n        title=\"\"\n        src=\"/static/9b39ccb8f5ff817f1518b632cd19541c/d7711/shared.png\"\n        srcset=\"/static/9b39ccb8f5ff817f1518b632cd19541c/a695b/shared.png 148w,\n/static/9b39ccb8f5ff817f1518b632cd19541c/2f273/shared.png 295w,\n/static/9b39ccb8f5ff817f1518b632cd19541c/d7711/shared.png 590w,\n/static/9b39ccb8f5ff817f1518b632cd19541c/ce4da/shared.png 716w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n      />\n  </span>\n  </a></p>\n<h3>local memory</h3>\n<ol>\n<li>\n<p>only exists for the lifetime of the thread</p>\n</li>\n<li></li>\n</ol>"
}
