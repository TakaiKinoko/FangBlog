{
  "expireTime": 9007200830489378000,
  "key": "transformer-remark-markdown-html-a67d4ced8b7f7b64133139af81c556e4-gatsby-remark-imagesgatsby-remark-responsive-iframegatsby-remark-prismjsgatsby-remark-copy-linked-filesgatsby-remark-smartypantsgatsby-remark-reading-time-",
  "val": "<p>This post talks about a key factor to CUDA kernel performace: accessing data in the globle memory.</p>\n<p>CUDA applications tend to process a massive amount of data from the global memory within a short period of time.</p>\n<p><strong>Tiling</strong> techniques are engineered that utilize <strong>shared memories</strong> to reduce the total amount of data that must be acessed from the global memory (read about tiling techniques here <a href=\"/cuda6-tiling\">The CUDA Parallel Programming Model - 6.Tiling</a>).</p>\n<p>I this post we talk about <strong>memory coalescing</strong> techniques that can more effectively move data from the global memory into <strong>shared memories and registers</strong>.</p>\n<p>Memory coalescing techniques are often used <em>in conjunction with tiling techniques</em> to allow CUDA devices to reach their performance potential by more efficiently utilizing the global memory bandwidth.</p>\n<h2>Global Memory Bandwidth</h2>\n<p>The global memory of a CUDA device is implemented with DRAMs.</p>\n<h4>DRAM is slow</h4>\n<p>Data bits are stored in DRAM cells that are small capacitors, where the presence or absence of a <em>tiny amount of electrical charge</em> distinguishes between 0 and 1.</p>\n<p>Reading data from a DRAM cell requires the small capacitor to use its tiny electrical charge to drive a highly capacitive line leading to a sensor and set off its detection mechanism that determines whether a sufficient amount of charge is present in the capacitor to qualify as a ‚Äú1‚Äù. This process takes 10 s of nanoseconds in modern DRAM chips. <strong>This is in sharp contrast with the sub-nanosecond clock cycle time of modern computing devices</strong>.</p>\n<h4>parallelism and memory access throughput</h4>\n<p>Because this is a very slow process relative to the desired data access speed (sub-nanosecond access per byte), modern DRAMs <strong>use parallelism to increase their rate of data access</strong>, commonly referred to as <em>memory access throughput</em>.</p>\n<h4>DRAM bursts</h4>\n<p>Each time a DRAM location is accessed, <strong>a range of consecutive locations that includes the requested location are actually accessed</strong>.</p>\n<p>Many sensors are provided in each DRAM chip and they work in parallel. Each senses the content of a bit within these consecutive locations.</p>\n<p>Once detected by the sensors, the data from all these consecutive locations can be transferred at very high-speed to the processor. These consecutive locations accessed and delivered are referred to as <strong>DRAM bursts</strong>.</p>\n<h4>motivation</h4>\n<p>If an application makes focused use of data from these bursts, the DRAMs can supply the data at a much higher rate than if a truly random sequence of locations were accessed.</p>\n<h2>Memory Coalescing</h2>\n<p>Current CUDA devices employ a technique that allows the programmers to achieve high global memory access efficiency by <strong>organizing memory accesses of threads into favorable patterns</strong>.</p>\n<h3>how?</h3>\n<ul>\n<li>\n<p>This technique takes advantage of the fact that <strong>threads in a warp execute the same instruction at any given point in time</strong>.</p>\n</li>\n<li>\n<p>The most favorable access pattern is achieved when all threads in a warp access consecutive global memory locations.</p>\n</li>\n<li>\n<p>When all threads in a warp execute a load instruction, the hardware detects whether they access consecutive global memory locations. If that‚Äôs the case, the hardware combines (<strong>coalesces</strong>) all these accesses into a consolidated access to consecutive DRAM locations.</p>\n</li>\n<li>\n<p>For example, for a given load instruction of a warp, if thread 0 accesses global memory location N2, thread 1 location N+1, thread 2 location N+2, and so on, all these accesses will be coalesced into a single request for consecutive locations when accessing the DRAMs.</p>\n</li>\n<li>\n<p>Such coalesced access allows the DRAMs to deliver data as a burst.</p>\n</li>\n</ul>\n<h3>matrix multiplication example</h3>\n<p>Recall from <a href=\"/cuda2-warp\">The CUDA Parallel Programming Model - 2. Warps</a> that multidimensional array elements in CUDA are placed into the linearly addressed memory space according to the <strong>row-major</strong> convention.</p>\n<p>Say we have a kernel that computes <code class=\"language-text\">M x N</code>, where both M and N are 2D row-major array.</p>\n<p>Each thread accesses a row of the M array (matrix A below) and a column of the N array (matrix B below).</p>\n<h4>hope this helps with understanding the patterns below</h4>\n<p>Coalesce happens amongst threads, not amongst different iterations of the loop within each thread‚Äôs execution.</p>\n<p>You might be thinking: for matrix M in the example below, each thread will read the whole row within its for-loop. But it‚Äôs not how the coalesce hardware works here. Because if you look across the threads in matrix M, they don‚Äôt share row accesses at all. Whereas for </p>\n<h4>patterns</h4>\n<p><a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/d3f4c1e8944826ac08d7b974ad9718b8/e6b24/pattern.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-wrapper\"\n    style=\"position: relative; display: block;  max-width: 590px; margin-left: auto; margin-right: auto;\"\n  >\n    <span\n      class=\"gatsby-resp-image-background-image\"\n      style=\"padding-bottom: 46.759847522236335%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAJABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAABQABA//EABUBAQEAAAAAAAAAAAAAAAAAAAEC/9oADAMBAAIQAxAAAAFLgnsA8vB//8QAGxAAAgIDAQAAAAAAAAAAAAAAAAECMQMREhP/2gAIAQEAAQUCzuRuXn1MdCr/xAAVEQEBAAAAAAAAAAAAAAAAAAAAEf/aAAgBAwEBPwFH/8QAGBEAAgMAAAAAAAAAAAAAAAAAAAECEmH/2gAIAQIBAT8BcSmn/8QAFxAAAwEAAAAAAAAAAAAAAAAAAAExIP/aAAgBAQAGPwJFdK8f/8QAGhAAAQUBAAAAAAAAAAAAAAAAAAERMUFx8P/aAAgBAQABPyFFdMOgobgJCiPD/9oADAMBAAIAAwAAABBr/wD/xAAWEQEBAQAAAAAAAAAAAAAAAAAAAXH/2gAIAQMBAT8Qsaf/xAAWEQEBAQAAAAAAAAAAAAAAAAABEQD/2gAIAQIBAT8QRbdWf//EABsQAQACAwEBAAAAAAAAAAAAAAEAESExoXHB/9oACAEBAAE/EAaI5sWHUL1DrhdRXCr6zgZ8zhT/2Q=='); background-size: cover; display: block;\"\n    ></span>\n    <img\n        class=\"gatsby-resp-image-image\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;box-shadow:inset 0px 0px 0px 400px white;\"\n        alt=\"access pattern\"\n        title=\"\"\n        src=\"/static/d3f4c1e8944826ac08d7b974ad9718b8/1697e/pattern.jpg\"\n        srcset=\"/static/d3f4c1e8944826ac08d7b974ad9718b8/a2cfd/pattern.jpg 148w,\n/static/d3f4c1e8944826ac08d7b974ad9718b8/3348f/pattern.jpg 295w,\n/static/d3f4c1e8944826ac08d7b974ad9718b8/1697e/pattern.jpg 590w,\n/static/d3f4c1e8944826ac08d7b974ad9718b8/e6b24/pattern.jpg 787w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n      />\n  </span>\n  </a></p>\n<h4><strong>M: unfavorable data access pattern</strong></h4>\n<ul>\n<li>fig.(A) above illustrates the data access pattern of the M array</li>\n<li>threads in a warp read adjacent rows</li>\n<li>during iteration 0, threads in a warp read <strong>element 0</strong> of rows 0 through 31.</li>\n<li>during iteration 1, these same threads read <strong>element 1</strong> of rows 0 through 31.</li>\n<li><strong>None</strong> of the accesses will be coalesced.</li>\n</ul>\n<h4><strong>N: favorable data access pattern</strong></h4>\n<ul>\n<li>\n<p>fig.(B) above illustrates the data access pattern of the N array</p>\n</li>\n<li>\n<p>each thread reads a column of N.</p>\n</li>\n<li>\n<p>during iteration 0, threads in warp 0 read <strong>element 1</strong> of columns 0 through 31.</p>\n</li>\n<li>\n<p>all these accesses will be coalesced.</p>\n</li>\n</ul>\n<p>If the above doesn‚Äôt make sense to you üßê, read the post on the matrix application kernel here: <a href=\"/cuda7-matrixmult\">CUDA Programming Examples - 1. Matrix Multiplication</a></p>\n<h4>a coalesced access pattern ‚Äî e.g. N</h4>\n<p><a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/5e76a9b8f6899418b29b8cc01a01ba39/0bfc5/coalescedP.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-wrapper\"\n    style=\"position: relative; display: block;  max-width: 590px; margin-left: auto; margin-right: auto;\"\n  >\n    <span\n      class=\"gatsby-resp-image-background-image\"\n      style=\"padding-bottom: 60.55194805194806%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAMABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAQFAv/EABUBAQEAAAAAAAAAAAAAAAAAAAIA/9oADAMBAAIQAxAAAAG0vhIq8RyH/8QAGxAAAgIDAQAAAAAAAAAAAAAAAgMBFAAQESH/2gAIAQEAAQUCL2ELMNNORG0ztg8//8QAFhEBAQEAAAAAAAAAAAAAAAAAEQAB/9oACAEDAQE/ATRm/8QAGBEAAwEBAAAAAAAAAAAAAAAAAAECEjH/2gAIAQIBAT8Blq+Gkf/EABsQAAEFAQEAAAAAAAAAAAAAAAABERIhQQIQ/9oACAEBAAY/AlZcFn3LyhqMP//EABwQAAMAAQUAAAAAAAAAAAAAAAABESEQMWFxkf/aAAgBAQABPyHGQbhDp0I9tK+LSOaOjl8H/9oADAMBAAIAAwAAABDAz//EABcRAAMBAAAAAAAAAAAAAAAAAAABEVH/2gAIAQMBAT8QasiD/8QAFhEAAwAAAAAAAAAAAAAAAAAAECFB/9oACAECAQE/EHEA/8QAGhABAQEAAwEAAAAAAAAAAAAAAREhABAxUf/aAAgBAQABPxA6ylLB+8LkIRcT3pZmA0vIQ1HXP//Z'); background-size: cover; display: block;\"\n    ></span>\n    <img\n        class=\"gatsby-resp-image-image\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;box-shadow:inset 0px 0px 0px 400px white;\"\n        alt=\"a coalesced access pattern\"\n        title=\"\"\n        src=\"/static/5e76a9b8f6899418b29b8cc01a01ba39/1697e/coalescedP.jpg\"\n        srcset=\"/static/5e76a9b8f6899418b29b8cc01a01ba39/a2cfd/coalescedP.jpg 148w,\n/static/5e76a9b8f6899418b29b8cc01a01ba39/3348f/coalescedP.jpg 295w,\n/static/5e76a9b8f6899418b29b8cc01a01ba39/1697e/coalescedP.jpg 590w,\n/static/5e76a9b8f6899418b29b8cc01a01ba39/6a00a/coalescedP.jpg 885w,\n/static/5e76a9b8f6899418b29b8cc01a01ba39/46304/coalescedP.jpg 1180w,\n/static/5e76a9b8f6899418b29b8cc01a01ba39/0bfc5/coalescedP.jpg 1232w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n      />\n  </span>\n  </a></p>\n<ul>\n<li>The arrow in the top portion of the figure shows the access pattern of the kernel code. </li>\n</ul>\n<p>Recall the for-loop in the kernel: </p>\n<div class=\"gatsby-highlight\" data-language=\"c\"><pre class=\"language-c\"><code class=\"language-c\">__global__ <span class=\"token keyword\">void</span> MatrixMulKernel <span class=\"token punctuation\">(</span><span class=\"token keyword\">float</span><span class=\"token operator\">*</span> M<span class=\"token punctuation\">,</span> <span class=\"token keyword\">float</span><span class=\"token operator\">*</span> N<span class=\"token punctuation\">,</span> <span class=\"token keyword\">float</span><span class=\"token operator\">*</span> P<span class=\"token punctuation\">,</span> <span class=\"token keyword\">int</span> Width<span class=\"token punctuation\">)</span><span class=\"token punctuation\">{</span>\n    <span class=\"token comment\">// calculate the row index of the P element and M</span>\n    <span class=\"token keyword\">int</span> Row <span class=\"token operator\">=</span> blockIdx<span class=\"token punctuation\">.</span>y <span class=\"token operator\">*</span> blockDim<span class=\"token punctuation\">.</span>y <span class=\"token operator\">+</span> threadIdx<span class=\"token punctuation\">.</span>y<span class=\"token punctuation\">;</span>\n    <span class=\"token comment\">// calculate the col index of the P element and N</span>\n    <span class=\"token keyword\">int</span> Col <span class=\"token operator\">=</span> blockIdx<span class=\"token punctuation\">.</span>x <span class=\"token operator\">*</span> blockDim<span class=\"token punctuation\">.</span>x <span class=\"token operator\">+</span> threadIdx<span class=\"token punctuation\">.</span>x<span class=\"token punctuation\">;</span>\n\n    <span class=\"token keyword\">if</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>Row <span class=\"token operator\">&lt;</span> Width<span class=\"token punctuation\">)</span> <span class=\"token operator\">&amp;&amp;</span> <span class=\"token punctuation\">(</span>Col <span class=\"token operator\">&lt;</span> Width<span class=\"token operator\">></span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">{</span>\n        <span class=\"token keyword\">float</span> Pvalue <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span>\n        <span class=\"token comment\">//each thread computes one element of the block sub-matrix</span>\n        <span class=\"token keyword\">for</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> k <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> k <span class=\"token operator\">&lt;</span> Width<span class=\"token punctuation\">;</span> k<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span>\n            Pvalue <span class=\"token operator\">+</span><span class=\"token operator\">=</span> M<span class=\"token punctuation\">[</span>Row<span class=\"token operator\">*</span>Width <span class=\"token operator\">+</span> k<span class=\"token punctuation\">]</span> <span class=\"token operator\">*</span> N<span class=\"token punctuation\">[</span>k<span class=\"token operator\">*</span>Width <span class=\"token operator\">+</span> Col<span class=\"token punctuation\">]</span><span class=\"token punctuation\">;</span>\n        <span class=\"token punctuation\">}</span>\n        P<span class=\"token punctuation\">[</span>Row <span class=\"token operator\">*</span> Width <span class=\"token operator\">+</span> Col<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> Pvalue<span class=\"token punctuation\">;</span>\n    <span class=\"token punctuation\">}</span>\n<span class=\"token punctuation\">}</span></code></pre></div>\n<ul>\n<li>\n<p>Within a given iteration of the <code class=\"language-text\">k</code> loop, the <code class=\"language-text\">k*Width</code> value is the same across all threads. Recall that <code class=\"language-text\">Col=blockIdx.x*blockDim.x+threadIdx.x</code>. </p>\n</li>\n<li>\n<p>Since the value of blockIndx.x and blockDim.x are of the same value for all threads in the same block, the only part of <code class=\"language-text\">k*Width+Col</code> that varies across a thread block is threadIdx.x. </p>\n</li>\n<li>\n<p>Since <strong>adjacent threads</strong> have consecutive threadIdx.x values, their accessed elements will have consecutive addresses.</p>\n</li>\n</ul>\n<h4>an un-coalesced access pattern ‚Äî e.g. M</h4>\n<p><a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/7670edec83ba2cfd4fe413f0e10b2e42/b88af/uncoalescedP.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-wrapper\"\n    style=\"position: relative; display: block;  max-width: 590px; margin-left: auto; margin-right: auto;\"\n  >\n    <span\n      class=\"gatsby-resp-image-background-image\"\n      style=\"padding-bottom: 76.91056910569105%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAPABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAMEBf/EABUBAQEAAAAAAAAAAAAAAAAAAAEC/9oADAMBAAIQAxAAAAHehcuW0lGf/8QAGhAAAgIDAAAAAAAAAAAAAAAAAgMAARETFP/aAAgBAQABBQKIMyOEwarpVneuf//EABcRAQADAAAAAAAAAAAAAAAAAAABESH/2gAIAQMBAT8B1cP/xAAYEQACAwAAAAAAAAAAAAAAAAAAEQECMf/aAAgBAgEBPwGFbBn/xAAaEAACAgMAAAAAAAAAAAAAAAAAAQIREDEy/9oACAEBAAY/AhqUKxbZ0bP/xAAcEAEAAgEFAAAAAAAAAAAAAAABABEhEDFRkaH/2gAIAQEAAT8hUqOANbumBrPEQrbphRnyz//aAAwDAQACAAMAAAAQqy//xAAYEQADAQEAAAAAAAAAAAAAAAAAAREhYf/aAAgBAwEBPxCIrMOx/8QAFxEBAQEBAAAAAAAAAAAAAAAAEQEAUf/aAAgBAgEBPxCqNJh3f//EABcQAQEBAQAAAAAAAAAAAAAAAAERACH/2gAIAQEAAT8QIhTpg5doEFpqZAmognKFaQNHFLr/2Q=='); background-size: cover; display: block;\"\n    ></span>\n    <img\n        class=\"gatsby-resp-image-image\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;box-shadow:inset 0px 0px 0px 400px white;\"\n        alt=\"an un-coalesced access pattern\"\n        title=\"\"\n        src=\"/static/7670edec83ba2cfd4fe413f0e10b2e42/1697e/uncoalescedP.jpg\"\n        srcset=\"/static/7670edec83ba2cfd4fe413f0e10b2e42/a2cfd/uncoalescedP.jpg 148w,\n/static/7670edec83ba2cfd4fe413f0e10b2e42/3348f/uncoalescedP.jpg 295w,\n/static/7670edec83ba2cfd4fe413f0e10b2e42/1697e/uncoalescedP.jpg 590w,\n/static/7670edec83ba2cfd4fe413f0e10b2e42/6a00a/uncoalescedP.jpg 885w,\n/static/7670edec83ba2cfd4fe413f0e10b2e42/46304/uncoalescedP.jpg 1180w,\n/static/7670edec83ba2cfd4fe413f0e10b2e42/b88af/uncoalescedP.jpg 1230w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n      />\n  </span>\n  </a></p>\n<ul>\n<li>The arrow in the top portion of the figure shows that the kernel code for <strong>each thread</strong> accesses elements of a row in sequence</li>\n</ul>"
}
