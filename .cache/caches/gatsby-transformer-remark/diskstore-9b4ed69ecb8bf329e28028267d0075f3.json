{
  "expireTime": 9007200830739191000,
  "key": "transformer-remark-markdown-html-6765fd15afecbce35c9fd4fc3cc601ec-gatsby-remark-imagesgatsby-remark-responsive-iframegatsby-remark-prismjsgatsby-remark-copy-linked-filesgatsby-remark-smartypantsgatsby-remark-reading-time-",
  "val": "<p>There’s an intrinsic tradeoff in the use of device memories in CUDA: the <strong>global memory</strong> is large but slow, whereas the <strong>shared memory</strong> is small but fast.</p>\n<p>(To recap on the memory hierarchy: <a href=\"/cuda1\">The CUDA Parallel Programming Model - 1. Concepts</a>, on how to specify memories for variables: <a href=\"/cudaProg2-Variables\">CUDA Programming - 2. CUDA Variable Type Qualifiers</a>.)</p>\n<p>A common strategy to reduce memory traffic is to partition the <strong>data</strong> into subsets called <strong>tiles</strong> so that <strong>each tile fits into the shared memory</strong>. An important criterion is that kernel computation on these tiles can be performed independently of each other. Note that <strong>not</strong> all data structures can be partitioned into tiles given an arbitrary kernel function.</p>\n<p>The term “tile” draws on the analogy that a large wall (i.e., the global memory data) can be covered by tiles (i.e., subsets that each can fit into the shared memory).</p>\n<p>To illustrate the concept of tiling, we use the example from <a href=\"/cudaProg1-matrixmult\">CUDA Programming - 1. Matrix Multiplication</a>, assumes that we use four 2×2 blocks to compute the P matrix. The picture below highlights the computation performed by the four threads of block(0,0).\n<figure class=\"gatsby-resp-image-figure\" style=\"\">\n    <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 590px;\"\n    >\n      <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 97.9320531757755%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAUABQDASIAAhEBAxEB/8QAGQABAAIDAAAAAAAAAAAAAAAAAAEFAgMG/8QAFgEBAQEAAAAAAAAAAAAAAAAAAAEC/9oADAMBAAIQAxAAAAHoKTfjm2g1IkJB/8QAGxAAAQQDAAAAAAAAAAAAAAAAAAECAwQUIDL/2gAIAQEAAQUCL3ZlKTP1/8QAFREBAQAAAAAAAAAAAAAAAAAAASD/2gAIAQMBAT8BRj//xAAWEQADAAAAAAAAAAAAAAAAAAABAiD/2gAIAQIBAT8BDR//xAAdEAABAgcAAAAAAAAAAAAAAAABAAIQERIgMkGh/9oACAEBAAY/Ak2GPE00AzGxb//EABsQAAICAwEAAAAAAAAAAAAAAAABEbEQIDFh/9oACAEBAAE/IStePQJ26EPrGiP/2gAMAwEAAgADAAAAEAAIAP/EABURAQEAAAAAAAAAAAAAAAAAAAEg/9oACAEDAQE/EBR//8QAFREBAQAAAAAAAAAAAAAAAAAAASD/2gAIAQIBAT8QFSP/xAAgEAADAAAFBQAAAAAAAAAAAAAAATERIUFx8VFhkbHR/9oACAEBAAE/EDs57hays5l9HMJR4lN+Ba7iWVfnYayrnUS2n//Z'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"block\"\n        title=\"block\"\n        src=\"/static/8a6ac4a5ad16d10fb7057c2042b31ad6/88218/block.jpg\"\n        srcset=\"/static/8a6ac4a5ad16d10fb7057c2042b31ad6/7237a/block.jpg 148w,\n/static/8a6ac4a5ad16d10fb7057c2042b31ad6/0cfdf/block.jpg 295w,\n/static/8a6ac4a5ad16d10fb7057c2042b31ad6/88218/block.jpg 590w,\n/static/8a6ac4a5ad16d10fb7057c2042b31ad6/e1ec8/block.jpg 677w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        loading=\"lazy\"\n      />\n    </span>\n    <figcaption class=\"gatsby-resp-image-figcaption\">block</figcaption>\n  </figure></p>"
}
