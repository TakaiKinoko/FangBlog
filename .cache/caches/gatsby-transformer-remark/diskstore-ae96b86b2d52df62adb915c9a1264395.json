{
  "expireTime": 9007200830484657000,
  "key": "transformer-remark-markdown-html-1a75f20dfefa9627f08d4cb92f1f9d59-gatsby-remark-imagesgatsby-remark-responsive-iframegatsby-remark-prismjsgatsby-remark-copy-linked-filesgatsby-remark-smartypantsgatsby-remark-reading-time-",
  "val": "<p>This post talks about a key factor to CUDA kernel performace: accessing data in the globle memory.</p>\n<p>CUDA applications tend to process a massive amount of data from the global memory within a short period of time. </p>\n<p><strong>Tiling</strong> techniques are engineered that utilize <strong>shared memories</strong> to reduce the total amount of data that must be acessed from the global memory (read about tiling techniques here <a href=\"/cuda6-tiling\">The CUDA Parallel Programming Model - 6.Tiling</a>). </p>\n<p>I this post we talk about <strong>memory coalescing</strong> techniques that can more effectively move data from the global memory into <strong>shared memories and registers</strong>. </p>\n<p>Memory coalescing techniques are often used <em>in conjunction with tiling techniques</em> to allow CUDA devices to reach their performance potential by more efficiently utilizing the global memory bandwidth.</p>\n<h2>Global Memory Bandwidth</h2>\n<p>The global memory of a CUDA device is implemented with DRAMs.</p>\n<h4>DRAM is slow</h4>\n<p>Data bits are stored in DRAM cells that are small capacitors, where the presence or absence of a <em>tiny amount of electrical charge</em> distinguishes between 0 and 1. </p>\n<p>Reading data from a DRAM cell requires the small capacitor to use its tiny electrical charge to drive a highly capacitive line leading to a sensor and set off its detection mechanism that determines whether a sufficient amount of charge is present in the capacitor to qualify as a ‚Äú1‚Äù. This process takes 10 s of nanoseconds in modern DRAM chips. <strong>This is in sharp contrast with the sub-nanosecond clock cycle time of modern computing devices</strong>. </p>\n<h4>parallelism and memory access throughput</h4>\n<p>Because this is a very slow process relative to the desired data access speed (sub-nanosecond access per byte), modern DRAMs <strong>use parallelism to increase their rate of data access</strong>, commonly referred to as <em>memory access throughput</em>.</p>\n<h4>DRAM bursts</h4>\n<p>Each time a DRAM location is accessed, <strong>a range of consecutive locations that includes the requested location are actually accessed</strong>. </p>\n<p>Many sensors are provided in each DRAM chip and they work in parallel. Each senses the content of a bit within these consecutive locations. </p>\n<p>Once detected by the sensors, the data from all these consecutive locations can be transferred at very high-speed to the processor. These consecutive locations accessed and delivered are referred to as <strong>DRAM bursts</strong>. </p>\n<h4>motivation</h4>\n<p>If an application makes focused use of data from these bursts, the DRAMs can supply the data at a much higher rate than if a truly random sequence of locations were accessed.</p>\n<h2>Memory Coalescing</h2>\n<p>Current CUDA devices employ a technique that allows the programmers to achieve high global memory access efficiency by <strong>organizing memory accesses of threads into favorable patterns</strong>. </p>\n<h3>how?</h3>\n<ul>\n<li>\n<p>This technique takes advantage of the fact that <strong>threads in a warp execute the same instruction at any given point in time</strong>. </p>\n</li>\n<li>\n<p>The most favorable access pattern is achieved when all threads in a warp access consecutive global memory locations. </p>\n</li>\n<li>\n<p>When all threads in a warp execute a load instruction, the hardware detects whether they access consecutive global memory locations. If that‚Äôs the case, the hardware combines (<strong>coalesces</strong>) all these accesses into a consolidated access to consecutive DRAM locations. </p>\n</li>\n<li>\n<p>For example, for a given load instruction of a warp, if thread 0 accesses global memory location N2, thread 1 location N+1, thread 2 location N+2, and so on, all these accesses will be coalesced into a single request for consecutive locations when accessing the DRAMs. </p>\n</li>\n<li>\n<p>Such coalesced access allows the DRAMs to deliver data as a burst.</p>\n</li>\n</ul>\n<h3>how to effectively use the coalescing hardware?</h3>\n<p>Recall from <a href=\"/cuda2-warp\">The CUDA Parallel Programming Model - 2. Warps</a> that multidimensional array elements in CUDA are placed into the linearly addressed memory space according to the <strong>row-major</strong> convention. </p>\n<h4>matrix multiplication example</h4>\n<p>Say we have a kernel that computes <code class=\"language-text\">M x N</code>, where both M and N are 2D row-major array.</p>\n<p>Each thread accesses a row of the M array (picture A below) and a column of the N array (picture B below).</p>\n<h5><strong>M: unfavorable data access pattern</strong> üßêüßêüßê</h5>\n<ul>\n<li>\n<p>the picture below illustrates the data access pattern of the M array</p>\n</li>\n<li>\n<p>threads in a warp read adjacent rows</p>\n</li>\n<li>\n<p>during iteration 0, threads in a warp read element 0 of rows 0 through 31. </p>\n</li>\n<li>\n<p>during iteration 1, these same threads read element 1 of rows 0 through 31. </p>\n</li>\n<li>\n<p><strong>None</strong> of the accesses will be coalesced.</p>\n</li>\n</ul>\n<h5><strong>N: favorable data access pattern</strong></h5>\n<ul>\n<li>\n<p>each thread reads a column of N. </p>\n</li>\n<li>\n<p>during iteration 0, threads in warp 0 read element 1 of columns 0 through 31. </p>\n</li>\n<li>\n<p>all these accesses will be coalesced.</p>\n</li>\n</ul>"
}
