{
  "expireTime": 9007200830587630000,
  "key": "transformer-remark-markdown-html-70e17c5fc4912142a67e47a10f984928-gatsby-remark-imagesgatsby-remark-responsive-iframegatsby-remark-prismjsgatsby-remark-copy-linked-filesgatsby-remark-smartypantsgatsby-remark-reading-time-",
  "val": "<h2>Table Of Content</h2>\n<ul>\n<li>\n<p>Part 0. review CUDA physical and virtual architecture</p>\n<h2>Part 1. Block Assignment</h2>\n</li>\n</ul>\n<h2>Part 0. Review</h2>\n<h3>CUDA’s Physical Architecture</h3>\n<h4>SMs</h4>\n<p>CUDA-capable GPU cards are composed of one or more <strong>Streaming Multiprocessors (SMs)</strong>, which are an <em>abstraction</em> of the underlying hardware.</p>\n<h4>cores</h4>\n<p>Each SM has a set of <strong>Streaming Processors (SPs)</strong>, also called CUDA cores, which <strong>share a cache of shared memory</strong> that is faster than the GPU’s global memory but that can only be accessed by the threads running on the SPs the that SM. These streaming processors are the “cores” that execute instructions.</p>\n<h4>cores/SM</h4>\n<p>The numbers of SPs/cores in an SM and the number of SMs depend on your device.</p>\n<p>It is important to realize, however, that regardless of GPU model, there <strong>are many more CUDA cores in a GPU than in a typical multicore CPU</strong>: hundreds or thousands more. For example, the Kepler Streaming Multiprocessor design, dubbed SMX, contains 192 single-precision CUDA cores, 64 double-precision units, 32 special function units, and 32 load/store units.</p>\n<h4>warps, SM and cores</h4>\n<p>CUDA <strong>cores are grouped together</strong> to perform instructions in a <strong>warp</strong> of threads. Warp simply means a group of threads that are scheduled together to execute the same instructions in lockstep.</p>\n<p>Depending on the model of GPU, the <strong>cores</strong> may be double or quadruple pumped so that they <strong>execute one instruction on two or four threads</strong> in as many clock cycles.</p>\n<p><em>For instance, Tesla devices use a group of 8 quadpumped cores to execute a single warp. If there are less than 32 threads scheduled in the warp, it will still take as long to execute the instructions.</em></p>\n<p>All CUDA cards to date use a warp size of 32.</p>\n<p>Each <strong>SM</strong> has at least one <strong>warp scheduler</strong>, which is responsible for executing 32 threads.</p>\n<h4>tiling</h4>\n<p>The programmer is responsible for ensuring that the threads are being assigned efficiently for code that is designed to run on the GPU. The assignment of threads is done virtually in the code using what is sometimes referred to as a ‘tiling’ scheme of blocks of threads that form a grid. Programmers define a kernel function that will be executed on the CUDA card using a particular tiling scheme.</p>\n<h3>CUDA’s Virtual Architecture</h3>\n<p>When programming in CUDA we work with blocks of threads and grids of blocks. What is the relationship between this virtual architecture and the CUDA card’s physical architecture?</p>\n<h4>block</h4>\n<p>When kernels are launched, each <strong>block</strong> in a grid is assigned to a <strong>Streaming Multiprocessor</strong>. This allows threads in a block to use <strong>shared</strong> memory. If a block doesn’t use the full resources of the SM then multiple blocks may be assigned at once. If all of the SMs are busy then the extra blocks will have to wait until a SM becomes free.</p>\n<h4>threads</h4>\n<p><em>Once a block is assigned to an SM</em>, it’s threads are split into warps by the warp scheduler and executed on the CUDA cores.</p>\n<p>Since the same instructions are executed on each thread in the warp simultaneously it’s generally a bad idea to have conditionals in kernel code. This type of code is sometimes called <strong>divergent</strong>: when some threads in a warp are unable to execute the same instruction as other threads in a warp, those threads are diverged and do no work.</p>\n<h4>warp’s context switch</h4>\n<p>Due to the hugely increased number of registers, a <strong>warp’s context (it’s registers, program counter etc.) stays on chip for the life of the warp</strong>. This means there is no additional cost to switching between warps vs executing the next step of a given warp. This allows the GPU to switch to hide some of it’s memory latency by switching to a new warp while it waits for a costly read.</p>\n<h2>Part 1. Block Assignment</h2>\n<ol>\n<li>\n<p>Before a block is assgined to an SM, it’s given all the resources it needs beforehands.</p>\n<p>These resources include:</p>\n<ul>\n<li>\n<p>shared memory</p>\n</li>\n<li>\n<p>registers</p>\n</li>\n<li>\n<p>a slot in the SM scheduler</p>\n</li>\n</ul>\n<p><a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/be4180c5f4948a409943a556e2c0cda1/02b03/block.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-wrapper\"\n    style=\"position: relative; display: block;  max-width: 590px; margin-left: auto; margin-right: auto;\"\n  >\n    <span\n      class=\"gatsby-resp-image-background-image\"\n      style=\"padding-bottom: 88.43930635838151%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAASCAYAAABb0P4QAAAACXBIWXMAABYlAAAWJQFJUiTwAAAEUUlEQVQ4y3WU+09TBxTH+2/shy2Lmg11Iz62mcUlywIRcUoBnfOXxZhlm0Md4w1RQECXbFnELYoRyeRRHgPkISAPobaUtrSlL4q2lEKBOtxchpqhqaXw2WlhWWa2m/vN9957Ts79nvM99ypgmZWVZRxTASweP56Fx3gePBF+ssZ/I3L/n7H1Z78sEg4/RsEqkZMu/TR1PU5Ms2AOCObBNPcPos8EpvkXILFRYfNsiOXwohSUI7S8gtvfjT9QxZS/Cu/M1Sgi9/5AJdOBWib8/Xhmapicr2dyThXFVECF734DvsB1xv1mQuFnawXDItHmzEKv243T/iF2617uuhIJLqUSeqrk6dIn+Pxn0Krfw2xKYNyRgNedyKPfkwg9O0Twz3h889UERZgi0nBoBSbtxcxoEvFqDuLTpkQxPZyCfzgJv/koXs95ZgwH8OlTmRmV2Ggqs6ZU5s2HuG/Zyz1PvdQJR2a4KraAoyODgZzNtGbtpCNrBx2ZgpwddGfHYr6wh/HhQnTFsbRJvDtnOx0Z22nL3E5X7k50ha9j11dH7F1XKC3ftRQyrU5gWpOCS60UJcn4NMJ39uPUfiIjOIdfu48ZrZJ7g0rcd5RyLeqlC1dvHM4JFcur6wrDolBT/zU3c7fSmf02jv79OHr20S4qu3N3YOs/gtVQRGf+Nrrz38JQHc+kMZWuM7voyn8HdcUuPHONYkp4zZTnUtF3rxiXdi9TMpNp10csTB3GPpTMrE2J13UMj7sU74gYNqDE7zjI/cmPcQwmMykz9Y4mcNfXsD5DaVmYAdtZLt9M5NKNZK73HKLdcIRLbUqutB+gbuAoJvc31A8eoOpmMrW9KXRJvPpWMjV9B6np3YPeVc9KpOXIUq+Ky626AgprdnP6ahzftyZS1Z9ESV0cBVff53LPYYbkhSWqD8i5HEdxbTyVA0mUNuwhryKeUtW7aJx1UjBqyop8erLt7gr6LCdR27MZtGZgdOdx25pNv+Ur9O6zOGevoR3/THCKPnMaBnc6Wlcag7ZT9JqOYZvulC8lhGJuMcgfT54yNlCDqSEPa0uRcD6OthIMDUUMV+cKF2JsKmGkNpOx5gJMjQU4bpzB2piFvfU0JlUGVmOfrM1zFJqJX/HOPsBYeZyR/Fexn98miMVW9iadGZtRpW2m8USM8Gs0nnyD5lNbuJ23BWtZbDTXUrYNXc7LWLp+jO6zom1ojIePljDd+gn1leMY60WBQF+Tw51r6VibCjCL4jXkYWnMR1uVjqE2VzqRvNo8hio+x6zrXVM4sbAsChcwVX6Ks+gV7GVbcJTFYCuJoSN9Iz+nbaApbSNNX26g4YsNtJzYRG/mJpySYy+NwXFuK7bTL2HuvBDdZ0Uw+Fw2XH4/6nYsrd8x1vEDY+0XBeUYW8vRNV9gpLkcQ0t5lHXr12s5F6P5lpZvMRm1ojAYcTks7gTxPhQs8i/4Hv0/Xsz1/rYkP9hF/gJ+F2IFO4iVVAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n    ></span>\n    <img\n        class=\"gatsby-resp-image-image\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;box-shadow:inset 0px 0px 0px 400px white;\"\n        alt=\"block\"\n        title=\"\"\n        src=\"/static/be4180c5f4948a409943a556e2c0cda1/d7711/block.png\"\n        srcset=\"/static/be4180c5f4948a409943a556e2c0cda1/a695b/block.png 148w,\n/static/be4180c5f4948a409943a556e2c0cda1/2f273/block.png 295w,\n/static/be4180c5f4948a409943a556e2c0cda1/d7711/block.png 590w,\n/static/be4180c5f4948a409943a556e2c0cda1/02b03/block.png 692w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n      />\n  </span>\n  </a></p>\n</li>\n<li>\n<p>Why not give blocks SPs too?</p>\n<ul>\n<li>There are way more blocks than SPs!</li>\n</ul>\n</li>\n<li>\n<p>Why don’t we assign these resources after the block is assigned?</p>\n<p>First, more on CUDA runtime:</p>\n<h3>Notes On CUDA Runtime</h3>\n<ul>\n<li>\n<p>The runtime system:</p>\n</li>\n<li>\n<p>maintains a list of blocks to be executed</p>\n</li>\n<li>\n<p>assigns new blocks to SM as they compute previously assigned blocks</p>\n</li>\n<li>\n<p>CUDA runtime automatically reduces the number of blocks assgined to each SM until resource usage is under limit.</p>\n</li>\n</ul>\n<p>Then, coming back to the point that we have <strong>zero-cost context switch for warps</strong> now, preassign resources to blocks (which in terms of execution is a bunch of warps) will make it that when warps are scheduled, their resources are already on-chip. So effectively we can achieve <strong>zero-cost scheduling</strong>.</p>\n</li>\n</ol>\n<h2>Part 3. Can CUDA Code Be Optimized For All GPUs?</h2>\n<p>This assumes that warp size is the same.</p>\n<h3>first, what gets built by nvcc?</h3>\n<p>NVCC generates three types of code:</p>\n<ul>\n<li>\n<p>Host object code (compiled with g++) — <strong>not</strong> forward-compatible</p>\n</li>\n<li>\n<p>Device object code — <strong>not</strong> forward-compatible</p>\n</li>\n<li>\n<p>Device assembly code (PTX) — forward-compatible</p>\n</li>\n</ul>\n<p>Compiler produces a <strong>fat binary</strong> which includes all three types of code.</p>\n<p>More on this at: <a href=\"http://on-demand.gputechconf.com/gtc/2013/webinar/cuda-toolkit-as-build-tool.pdf\">http://on-demand.gputechconf.com/gtc/2013/webinar/cuda-toolkit-as-build-tool.pdf</a></p>\n<h3>fat binaries</h3>\n<p>When a CUDA fat binary is run on a given GPU, a few scenarios can take place:</p>\n<ul>\n<li>\n<p>If the fat binary includes object code compiled for the device architecture, that code is run directly.</p>\n</li>\n<li>\n<p>If the fat binary includes PTX assembly which the GPU understands, that code is Just-In-Time compiled and run on the GPU. (This will result in startup lag due to JIT compilation)</p>\n</li>\n<li>\n<p>If neither version are compatible with the GPU, the application doesn’t run.</p>\n</li>\n</ul>\n<p>Newer GPUs can run older PTX assembly code, but binary code from one architecture can’t fun on another.</p>\n<h3>compute capability and device architecture</h3>\n<p>Compute capability:</p>\n<ul>\n<li>\n<p>defines the <strong>computing features</strong> supported by a given GPU generation</p>\n</li>\n<li>\n<p>language features (i.e. double precision floats, various functions)</p>\n</li>\n<li>\n<p>device features (size of shared memory, max thread block size)</p>\n</li>\n</ul>\n<p>GPU architecture:</p>\n<ul>\n<li>\n<p>binary code is architecture-specific, and changes with each GPU generation</p>\n</li>\n<li>\n<p>version of ther object code</p>\n</li>\n<li>\n<p>different architecutres use different optimizations, etc.</p>\n</li>\n</ul>\n<h3>CUDA code optimized for one GPU might run inefficiently when executed on another</h3>\n<p>Optimizations can be applied at various levels</p>\n<ul>\n<li>overlapping data transfers with computation </li>\n<li>fine-tuning floating-point operation sequences</li>\n<li>…</li>\n</ul>\n<p>Now coming back to the question, we can try to answer from device features perspective. For instance, we don’t know beforehand:</p>\n<ul>\n<li>\n<p>size of shared memory </p>\n<ul>\n<li>you might be under or over utilizing it</li>\n</ul>\n</li>\n<li>\n<p>architecture of shared memory (e.g. does it come with L1 cache or are they separate) </p>\n<ul>\n<li>see the section below on shared memory</li>\n</ul>\n</li>\n<li>\n<p>is there an L2 cache?</p>\n<ul>\n<li>see the section below on shared memory</li>\n</ul>\n</li>\n<li>\n<p>max thread block size</p>\n</li>\n<li>\n<p>number of SMs available</p>\n</li>\n<li>\n<p>how many cores there are on each SM</p>\n</li>\n</ul>\n<h3>more on shared memory</h3>\n<p>Shared memory benefits meny problems, but it’s not appropriate for all problems.</p>\n<ul>\n<li>\n<p>Some algorithms map naturally to shared memory</p>\n</li>\n<li>\n<p>Others require a cache</p>\n</li>\n<li>\n<p>Others require a combination of both.</p>\n</li>\n</ul>\n<h4>L1 cache</h4>\n<p>The L1 cache helps <strong>caching temporary resgister</strong> spills of complex programs.</p>\n<p>In older generations, GPUs spilled registers directly to DRAM, increasing access latency. </p>\n<p>With L1 cache, performance scales gracefully with increased temporary register usage.</p>\n<h4>L2 cache</h4>\n<h4>Unified Cache Architecture</h4>\n<p>This unified design <strong>allows the L1 cache to leverage resources</strong>, increasing its bandwidth, and allows it to be reconfigured to grow larger when shared memory allocations are not using all of the shared memory capacity.</p>\n<p>Furthermore, a configurable L1 cache can be as large as 64KB in size, combined with a 32KB per SM shared memory allocation, or it can be reduced to 32KB of L1 cache, allowing 64KB of allocation to be used for shared memory. </p>\n<p>Combining the L1 data cache with the shared memory <strong>reduces latency</strong> and provides <strong>higher bandwidth</strong> than the L1 cache implementation used previously.</p>"
}
