{
  "expireTime": 9007200830647052000,
  "key": "transformer-remark-markdown-html-a52506ef7cd7764d37f85856328bdb1c-gatsby-remark-imagesgatsby-remark-responsive-iframegatsby-remark-prismjsgatsby-remark-copy-linked-filesgatsby-remark-smartypantsgatsby-remark-reading-time-",
  "val": "<p>In the last post, we saw how full concurrency can be achieved amongst streams. Here Iâ€™d like to talk about how CUDA operations from different streams may also be <strong>interleaved</strong>, which is another programming model often used to effect concurrency.</p>\n<h3>Review CUDA Asynchronous Commands</h3>\n<p>Being asynchronous means that CUDA can perform these operations simultaneously</p>\n<ul>\n<li>CUDA Kernel &#x3C;&#x3C;&#x3C;>>></li>\n<li>cudaMemcpyAsync (HostToDevice)</li>\n<li>cudaMemcpyAsync (DeviceToHost)</li>\n<li>Operations on the CPU</li>\n</ul>\n<h3>What Can Happen Simultaneously</h3>\n<ul>\n<li>lots of CUDA kernels on GPU</li>\n<li>cudaMemcpyAsyncs in different directions</li>\n<li>computation on the CPU</li>\n</ul>"
}
