{
  "expireTime": 9007200830739094000,
  "key": "transformer-remark-markdown-html-4925a4605a10dbb4cf0afea17c905e62-gatsby-remark-imagesgatsby-remark-responsive-iframegatsby-remark-prismjsgatsby-remark-copy-linked-filesgatsby-remark-smartypantsgatsby-remark-reading-time-",
  "val": "<p>There’s an intrinsic tradeoff in the use of device memories in CUDA: the <strong>global memory</strong> is large but slow, whereas the <strong>shared memory</strong> is small but fast.</p>\n<p>(To recap on the memory hierarchy: <a href=\"/cuda1\">The CUDA Parallel Programming Model - 1. Concepts</a>, on how to specify memories for variables: <a href=\"/cudaProg2-Variables\">CUDA Programming - 2. CUDA Variable Type Qualifiers</a>.)</p>\n<p>A common strategy to reduce memory traffic is to partition the <strong>data</strong> into subsets called <strong>tiles</strong> so that <strong>each tile fits into the shared memory</strong>. An important criterion is that kernel computation on these tiles can be performed independently of each other. Note that <strong>not</strong> all data structures can be partitioned into tiles given an arbitrary kernel function.</p>\n<p>The term “tile” draws on the analogy that a large wall (i.e., the global memory data) can be covered by tiles (i.e., subsets that each can fit into the shared memory).</p>\n<p>To illustrate the concept of tiling, we use the example from <a href=\"/cudaProg1-matrixmult\">CUDA Programming - 1. Matrix Multiplication</a>. </p>"
}
