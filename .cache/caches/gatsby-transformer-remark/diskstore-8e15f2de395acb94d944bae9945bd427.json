{
  "expireTime": 9007200830740932000,
  "key": "transformer-remark-markdown-html-e3a71c7b9d640cfc87ad9792a6572808-gatsby-remark-imagesgatsby-remark-responsive-iframegatsby-remark-prismjsgatsby-remark-copy-linked-filesgatsby-remark-smartypantsgatsby-remark-reading-time-",
  "val": "<p>There’s an intrinsic tradeoff in the use of device memories in CUDA: the <strong>global memory</strong> is large but slow, whereas the <strong>shared memory</strong> is small but fast.</p>\n<p>(To recap on the memory hierarchy: <a href=\"/cuda1\">The CUDA Parallel Programming Model - 1. Concepts</a>, on how to specify memories for variables: <a href=\"/cudaProg2-Variables\">CUDA Programming - 2. CUDA Variable Type Qualifiers</a>.)</p>\n<p>A common strategy to reduce memory traffic is to partition the <strong>data</strong> into subsets called <strong>tiles</strong> so that <strong>each tile fits into the shared memory</strong>. An important criterion is that kernel computation on these tiles can be performed independently of each other. Note that <strong>not</strong> all data structures can be partitioned into tiles given an arbitrary kernel function.</p>\n<p>The term “tile” draws on the analogy that a large wall (i.e., the global memory data) can be covered by tiles (i.e., subsets that each can fit into the shared memory).</p>\n<p>To illustrate the concept of tiling, we use the example from <a href=\"/cudaProg1-matrixmult\">CUDA Programming - 1. Matrix Multiplication</a>, assumes that we use four 2×2 blocks to compute the P matrix. The picture below highlights the computation performed by the four threads of block(0,0):</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 590px;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/8a6ac4a5ad16d10fb7057c2042b31ad6/e1ec8/block.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 97.9320531757755%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAUABQDASIAAhEBAxEB/8QAGQABAAIDAAAAAAAAAAAAAAAAAAEFAgMG/8QAFgEBAQEAAAAAAAAAAAAAAAAAAAEC/9oADAMBAAIQAxAAAAHoKTfjm2g1IkJB/8QAGxAAAQQDAAAAAAAAAAAAAAAAAAECAwQUIDL/2gAIAQEAAQUCL3ZlKTP1/8QAFREBAQAAAAAAAAAAAAAAAAAAASD/2gAIAQMBAT8BRj//xAAWEQADAAAAAAAAAAAAAAAAAAABAiD/2gAIAQIBAT8BDR//xAAdEAABAgcAAAAAAAAAAAAAAAABAAIQERIgMkGh/9oACAEBAAY/Ak2GPE00AzGxb//EABsQAAICAwEAAAAAAAAAAAAAAAABEbEQIDFh/9oACAEBAAE/IStePQJ26EPrGiP/2gAMAwEAAgADAAAAEAAIAP/EABURAQEAAAAAAAAAAAAAAAAAAAEg/9oACAEDAQE/EBR//8QAFREBAQAAAAAAAAAAAAAAAAAAASD/2gAIAQIBAT8QFSP/xAAgEAADAAAFBQAAAAAAAAAAAAAAATERIUFx8VFhkbHR/9oACAEBAAE/EDs57hays5l9HMJR4lN+Ba7iWVfnYayrnUS2n//Z'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"block\"\n        title=\"block\"\n        src=\"/static/8a6ac4a5ad16d10fb7057c2042b31ad6/88218/block.jpg\"\n        srcset=\"/static/8a6ac4a5ad16d10fb7057c2042b31ad6/7237a/block.jpg 148w,\n/static/8a6ac4a5ad16d10fb7057c2042b31ad6/0cfdf/block.jpg 295w,\n/static/8a6ac4a5ad16d10fb7057c2042b31ad6/88218/block.jpg 590w,\n/static/8a6ac4a5ad16d10fb7057c2042b31ad6/e1ec8/block.jpg 677w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>Another picture (below) is used to show the global memory accesses performed by block(0, 0). Among the four threads highlighted, a significant <strong>overlap</strong> occurs in the M and N elements they access.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 590px;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/383c503ca2b90562629741e19a053781/dfb83/block00.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 32.97985153764581%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAHABQDASIAAhEBAxEB/8QAFwABAAMAAAAAAAAAAAAAAAAAAAECBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAdiAsD//xAAXEAADAQAAAAAAAAAAAAAAAAAAARAh/9oACAEBAAEFAtFP/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAFBABAAAAAAAAAAAAAAAAAAAAEP/aAAgBAQAGPwJ//8QAGBAAAwEBAAAAAAAAAAAAAAAAAAEhYXH/2gAIAQEAAT8h6hDZdP/aAAwDAQACAAMAAAAQc8//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAaEAEBAAIDAAAAAAAAAAAAAAABEQAhMUGR/9oACAEBAAE/EKo3o0vOBioM7wUbfTn/2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"block00\"\n        title=\"block00\"\n        src=\"/static/383c503ca2b90562629741e19a053781/88218/block00.jpg\"\n        srcset=\"/static/383c503ca2b90562629741e19a053781/7237a/block00.jpg 148w,\n/static/383c503ca2b90562629741e19a053781/0cfdf/block00.jpg 295w,\n/static/383c503ca2b90562629741e19a053781/88218/block00.jpg 590w,\n/static/383c503ca2b90562629741e19a053781/77d57/block00.jpg 885w,\n/static/383c503ca2b90562629741e19a053781/dfb83/block00.jpg 943w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>If thread(0, 0) and thread(0, 1) can be made to collaborate so that these M elements are only loaded from the global memory once, the total number of accesses to the global memory can be reduced by half.</p>\n<p>(The potential reduction in global memory traffic in the matrix multiplication example is proportional to the dimension of the blocks used. With Width ×Width blocks, the potential reduction of global memory traffic would be Width. Thus, if we use 16 ×16 blocks, the global memory traffic can be potentially reduced to 1/16 through collaboration between threads)</p>\n<h2>Tiled Algorithms</h2>\n<h3>threads and their DRAM accesses</h3>\n<p>Tiled algorithms are highly similar to carpooling arrangements. We can consider <strong>threads</strong> accessing data values as commuters and <strong>DRAM access</strong> requests as vehicles.</p>\n<p>When the rate of DRAM requests exceeds the provisioned access <strong>bandwidth</strong> of the DRAM system, traffic congestion arises and the arithmetic units become idle.</p>\n<p>If multiple threads access data from the same DRAM location, they can potentially form a “carpool” and <strong>combine their accesses into one DRAM request</strong>.</p>\n<p>However, this process requires a <strong>similar execution schedule</strong> for the threads so that their data accesses can be combined.</p>\n<p>This scenario is shown in the picture below:</p>\n<ul>\n<li>the cells at the center represent DRAM locations</li>\n<li>an arrow from a DRAM location pointing to a thread represents an access by the thread to that location at the time marked by the head of the arrow</li>\n<li>the top portion shows two threads that access the same data elements with similar timing</li>\n<li>the bottom half shows two threads that access their common data at varying times i.e., the accesses by Thread 2 lag significantly behind their corresponding accesses by Thread 1.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 590px;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/8e1a2a97d68a8fbbc53aada41de8761a/d55e0/tiled.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 51.68792198049512%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAKABQDASIAAhEBAxEB/8QAGAAAAwEBAAAAAAAAAAAAAAAAAAEDBAX/xAAVAQEBAAAAAAAAAAAAAAAAAAABAP/aAAwDAQACEAMQAAAB60tSRFSP/8QAGhAAAgIDAAAAAAAAAAAAAAAAAAECERASQf/aAAgBAQABBQKbp7PDO0f/xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAbEAACAQUAAAAAAAAAAAAAAAAAIQEgMTKRof/aAAgBAQAGPwJPRbsGdH//xAAbEAEBAAIDAQAAAAAAAAAAAAABABFRMWGBsf/aAAgBAQABPyFwwfQ+3fsRBUOslyiYaL//2gAMAwEAAgADAAAAEMv/AP/EABURAQEAAAAAAAAAAAAAAAAAAAEQ/9oACAEDAQE/EGf/xAAWEQEBAQAAAAAAAAAAAAAAAAABEFH/2gAIAQIBAT8QEyf/xAAdEAEAAgICAwAAAAAAAAAAAAABABEhQTFRYbHx/9oACAEBAAE/EASxM1U9p1Xxcii1LXjiEHjZAIEEvc+NP//Z'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"tiled\"\n        title=\"tiled\"\n        src=\"/static/8e1a2a97d68a8fbbc53aada41de8761a/88218/tiled.jpg\"\n        srcset=\"/static/8e1a2a97d68a8fbbc53aada41de8761a/7237a/tiled.jpg 148w,\n/static/8e1a2a97d68a8fbbc53aada41de8761a/0cfdf/tiled.jpg 295w,\n/static/8e1a2a97d68a8fbbc53aada41de8761a/88218/tiled.jpg 590w,\n/static/8e1a2a97d68a8fbbc53aada41de8761a/77d57/tiled.jpg 885w,\n/static/8e1a2a97d68a8fbbc53aada41de8761a/5a917/tiled.jpg 1180w,\n/static/8e1a2a97d68a8fbbc53aada41de8761a/d55e0/tiled.jpg 1333w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>the reason the bottom is an undesirable arrangement is that data elements that are brought back from the DRAM need to be stored in the on-chip memory for an extended time, waiting to be consumed by Thread 2. A large number of data elements will need to be stored, resulting in an excessive on-chip memory requirement.</p>\n<h3>tiling as a parallel algorithm</h3>\n<p>In the context of parallel computing, tiling is a program transformation technique that <strong>localizes the memory locations accessed among threads and the timing of their accesses</strong>. </p>\n<p>It divides the long access sequences of each thread into <strong>phases</strong> and uses <strong>barrier synchronization</strong> to keep the timing of accesses to each section at close intervals. </p>\n<p>This technique controls the amount of on-chip memory required by localizing the accesses both in time and in space. In terms of our carpool analogy, we force the threads that form the “carpool” group to follow approximately the same execution timing.</p>\n<h3>example</h3>\n<p>A tiled matrix multiplication algorithm is shown in the picture below:</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 590px;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/3774b92e3dbb1f7ee4047d901d93eb16/dc940/example.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 96.97406340057638%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAATABQDASIAAhEBAxEB/8QAGQABAAMBAQAAAAAAAAAAAAAAAAIDBAUG/8QAFQEBAQAAAAAAAAAAAAAAAAAAAAL/2gAMAwEAAhADEAAAAfQcG+Mti1TLoCQP/8QAHhAAAgIABwAAAAAAAAAAAAAAAAECAwQREhMhMjP/2gAIAQEAAQUCMd6SXO7cW62ZIU2R6n//xAAVEQEBAAAAAAAAAAAAAAAAAAABIP/aAAgBAwEBPwFI/8QAFhEAAwAAAAAAAAAAAAAAAAAAAQIg/9oACAECAQE/AQ0f/8QAHxAAAgECBwAAAAAAAAAAAAAAAAECEXEQEiExMlKR/9oACAEBAAY/AhWOMRaS8IvLV06myI2Fh//EACEQAAIABAcBAAAAAAAAAAAAAAABIUGR8BExUXGhsdHx/9oACAEBAAE/ITjO2XrwTtSrZDLiJZHE+cTqjYaiYn//2gAMAwEAAgADAAAAEGfHAP/EABURAQEAAAAAAAAAAAAAAAAAAAEg/9oACAEDAQE/EFI//8QAFxEBAAMAAAAAAAAAAAAAAAAAAREgMf/aAAgBAgEBPxCRxp//xAAfEAACAgIBBQAAAAAAAAAAAAABEQAhMVGRQYGhwdH/2gAIAQEAAT8QgOgHb8IFKK9mMwrKG8WO6DSZAWixoseI7JIFrjq1BCc5LHkxn04n/9k='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"example\"\n        title=\"example\"\n        src=\"/static/3774b92e3dbb1f7ee4047d901d93eb16/88218/example.jpg\"\n        srcset=\"/static/3774b92e3dbb1f7ee4047d901d93eb16/7237a/example.jpg 148w,\n/static/3774b92e3dbb1f7ee4047d901d93eb16/0cfdf/example.jpg 295w,\n/static/3774b92e3dbb1f7ee4047d901d93eb16/88218/example.jpg 590w,\n/static/3774b92e3dbb1f7ee4047d901d93eb16/dc940/example.jpg 694w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>The basic idea is <em>for the threads to collaboratively load subsets of the M and N elements into the shared memory before they individually use these elements in their dot product calculation</em>. The size of the shared memory is quite small, and the capacity of the shared memory should not be exceeded when these M and N elements are loaded into the shared memory. This condition can be satisfied by dividing the M and N matrices into smaller tiles so that they can fit into the shared memory. In the simplest form, the tile dimensions equal those of the block.</p>\n<p>In the picture above, we divide M and N into 2 ×2 tiles. The dot product calculations performed by each thread are now divided into phases.</p>\n<p>In each phase, all threads in a block collaborate to load a tile of M and a tile of N into the shared memory. This collaboration can be accomplished by having every thread in a block to load <strong>one M element</strong> and <strong>one N element</strong> into the shared memory, as illustrated in the picture below:</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 590px;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/fab5ae5ba6de564028c7787eca4baf88/6f1be/phase.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 60.290350128095646%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAMABQDASIAAhEBAxEB/8QAFwABAAMAAAAAAAAAAAAAAAAAAAIEBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAdYgWAf/xAAXEAEBAQEAAAAAAAAAAAAAAAABEAAx/9oACAEBAAEFAoccs//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABQQAQAAAAAAAAAAAAAAAAAAACD/2gAIAQEABj8CX//EABkQAQADAQEAAAAAAAAAAAAAAAEAIUERMf/aAAgBAQABPyF12OuhLx7SOPKuFT//2gAMAwEAAgADAAAAEMAP/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPxA//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPxA//8QAGxABAAIDAQEAAAAAAAAAAAAAAQARITFxQVH/2gAIAQEAAT8QNVlCgc4wQ37gp+b+Tcae0xUXTXdRyRABUz7MdGez/9k='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"phase\"\n        title=\"phase\"\n        src=\"/static/fab5ae5ba6de564028c7787eca4baf88/88218/phase.jpg\"\n        srcset=\"/static/fab5ae5ba6de564028c7787eca4baf88/7237a/phase.jpg 148w,\n/static/fab5ae5ba6de564028c7787eca4baf88/0cfdf/phase.jpg 295w,\n/static/fab5ae5ba6de564028c7787eca4baf88/88218/phase.jpg 590w,\n/static/fab5ae5ba6de564028c7787eca4baf88/77d57/phase.jpg 885w,\n/static/fab5ae5ba6de564028c7787eca4baf88/6f1be/phase.jpg 1171w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>Each row in this picture shows the execution activities of a thread. We only need to show the activities of threads in block0,0; all of the other blocks have the same behavior. The shared memory array for the M elements is called Mds, and that for the N elements is called Nds. At the beginning of Phase 1, the four threads of block0,0 collaboratively load a tile of M into a shared memory: thread0,0 loads M0,0 into Mds0,0, thread0,1 loads M0,1 into Mds0,1, thread1,0 loads M1,0 into Mds1,0, and thread1,1 loads M1,1 into Mds1,1, as shown in the second column in Fig. 4.15. A tile of N is also similarly loaded, as presented in the third column in Fig. 4.15.</p>"
}
