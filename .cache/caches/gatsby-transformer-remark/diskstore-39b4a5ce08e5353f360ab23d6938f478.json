{
  "expireTime": 9007200830648082000,
  "key": "transformer-remark-markdown-html-ea3cb52d45f3ee3a0475c908e9a68874-gatsby-remark-imagesgatsby-remark-responsive-iframegatsby-remark-prismjsgatsby-remark-copy-linked-filesgatsby-remark-smartypantsgatsby-remark-reading-time-",
  "val": "<p>In the last post, we saw how full concurrency can be achieved amongst streams. Here Iâ€™d like to talk about how CUDA operations from different streams may also be <strong>interleaved</strong>, which is another programming model often used to effect concurrency.</p>\n<h3>Review CUDA Streams</h3>\n<p>A stream in CUDA is a sequence of operations that execute on the device in the order in which they are issued by the host code. While operations within a stream are guaranteed to execute in the prescribed order, operations in different streams can be interleaved and, when possible, they can even run concurrently.</p>\n<h3>Review CUDA Asynchronous Commands</h3>\n<p>Being asynchronous means that CUDA can perform these operations simultaneously</p>\n<ul>\n<li>\n<p>CUDA Kernel &#x3C;&#x3C;&#x3C;>>></p>\n</li>\n<li>\n<p>cudaMemcpyAsync (HostToDevice)</p>\n</li>\n<li>\n<p>cudaMemcpyAsync (DeviceToHost)</p>\n</li>\n<li>\n<p>Operations on the CPU</p>\n</li>\n</ul>\n<h3>ðŸŒŸ What Can Happen Simultaneously ðŸŒŸ</h3>\n<ul>\n<li>\n<p>computation on the device </p>\n<ul>\n<li>lots of CUDA kernels on GPU</li>\n</ul>\n</li>\n<li>\n<p>data transfers between the host and device </p>\n<ul>\n<li><code class=\"language-text\">cudaMemcpyAsyncs</code> both HostToDevice and DeviceToHost</li>\n</ul>\n</li>\n<li>\n<p>computation on the CPU</p>\n</li>\n</ul>\n<h2>Overlap Kernel Execution</h2>\n<h2>Overlap Computation and Data Transfers</h2>\n<p><a href=\"https://developer.download.nvidia.com/CUDA/training/StreamsAndConcurrencyWebinar.pdf\">https://developer.download.nvidia.com/CUDA/training/StreamsAndConcurrencyWebinar.pdf</a></p>"
}
