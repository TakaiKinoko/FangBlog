{
  "expireTime": 9007200830439965000,
  "key": "transformer-remark-markdown-html-11348dfeecd1b6f97873b7664d32d787-gatsby-remark-imagesgatsby-remark-responsive-iframegatsby-remark-prismjsgatsby-remark-copy-linked-filesgatsby-remark-smartypantsgatsby-remark-reading-time-",
  "val": "<p>This is the second post in a series about what I learnt in my GPU class at NYU this past fall. This will be mostly about <strong>warps and SIMD hardward</strong>.</p>\n<h2>Kernel threads hierarchy</h2>\n<p>Recall that launching a CUDA kernel will generate a grid of threads organized as a <strong>two-level</strong> hierarchy.</p>\n<ol>\n<li>\n<p>top level: a 1/2/3-dimensional array of blocks.</p>\n</li>\n<li>\n<p>bottom level: each block consists of a 1/2/3-dimensional array of threads.</p>\n</li>\n</ol>\n<h2>Synchronize threads?</h2>\n<p>Conceptually, threads in a block can execute in any order, just like blocks.</p>\n<p>When an algorithm needs to execute in <em>phases</em>, <strong>barrier synchronizations</strong> should be used to ensure that all threads have completed a common phase before they start the next one.</p>\n<p>But the correctness of executing a kernel should not depend on the synchrony amongst threads.</p>\n<h2>Warp</h2>\n<p>Due to hardware cost considerations, CUDA devices currently bundle multiple threads for execution, which leads to performance limitations.</p>\n<ul>\n<li>\n<p>_Each <strong>thread block</strong> is partitioned into warps_. </p>\n</li>\n<li>\n<p>The execution of warps is implemented by an SIMD hardware. </p>\n</li>\n</ul>\n<h3>why? reducing hardware manufacturing cost.</h3>"
}
