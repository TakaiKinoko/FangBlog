{
  "expireTime": 9007200830839333000,
  "key": "transformer-remark-markdown-html-f2f6b4dc5d3c8e4ab5ba5a169ae5580e-gatsby-remark-imagesgatsby-remark-responsive-iframegatsby-remark-prismjsgatsby-remark-copy-linked-filesgatsby-remark-smartypantsgatsby-remark-reading-time-",
  "val": "<p>Have you wondered if itâ€™s possible to launch nested kernels (i.e. a kernel calls another kernel) in CUDA? Well, this is where dynamic parallelism comes into play.</p>\n<p>Without dynamic parallelism, GPU is unable to create more work on itself dynamically depending on the data. Consequently data travels back and forth between the CPU and GPU many times. This was the case in older generations e.g. Fermi, see picture below:</p>\n<p><figure class=\"gatsby-resp-image-figure\" style=\"\">\n    <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 590px;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/50ab2214f80a3820049d777e1d662621/7a2f6/fermi.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 100.47281323877068%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAACXBIWXMAABYlAAAWJQFJUiTwAAACq0lEQVQ4y6WUS2/TQBSF7Rm/x/bEduI8nTRNnDZUolSFBlChLUSiou2CDQtWLJBYIbWqgA2PDa0qkJBY8VhVQurfPIwfpbUSpFQsjhx5PDfznXvPSLJmgbIARDwlSSpIliXwgEJRJai6DE0n0AwZQU0BcwkIkSb2SHq0Cq27DqN7F5JMC4u62Dx6yHB/x8H6Y4bRmGG4auDJCxf3dhlcj0wWVA2GsN6EabHJxVyqJoNSCVTJRMTv5HQJwcT3bqWBhTvbCIPSlGKKkPzPP5oq0/Hgt2Iw08x8IzQtxEoeDk8WsLxWTt9P9WuadMNEu9uDy+0UgZDMF9sxcXLWwWjL+9ugmQq6YQvx2iNELR+KJjyiNN1MKUE8rIJ7WfdlSb1kA8mtOH9Hc6XIJZRbfViGkZ/kAvngKEZ3wDHarGD/aABDfLN/1MftByEGS2W8+dqHbbt4fhhh71l0GbkPniCTBDlrAnMMfDrtYHFZjMh2gHff27AsCx9+RdjYDbC0Usbx7ya46+HVcQ1PX9azgvwcuZkgy1AUJfVRUTUMhjXhpQE/sNFbCFM7oi4XNphgtoFOj0NVVNQjG5WqnSPbXCD3YOpFZC8s4+PPGP2hj42dEK+/DEBkZYaxYTbm42vwfaeAzEsG3v9ooDfk2NzzhV8RKFELHS88z6eAV9uIb43RanppCjJkKnIrkLpVWMwU0gWSM9uQG8yFX4uga/olZIpaO8Tx6SKu54M9e1KYg7kEOSgil0MTb781MVzhWVLojBHktQ76N8doNhJk0WVVSeNniMuiN6jDcayrnVA3bdHRBjQxJhkySZHrc1V8PovFvPl5xmcsyLgnOnkDldBNryVKL5APTppizb1alksCeX5lC7VQeEWyyyHLMhWxYqkFV2uKGOxqqwsrv77+V38ASHzNIENPxjQAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"fermi\"\n        title=\"fermi\"\n        src=\"/static/50ab2214f80a3820049d777e1d662621/799d3/fermi.png\"\n        srcset=\"/static/50ab2214f80a3820049d777e1d662621/00d96/fermi.png 148w,\n/static/50ab2214f80a3820049d777e1d662621/0b23c/fermi.png 295w,\n/static/50ab2214f80a3820049d777e1d662621/799d3/fermi.png 590w,\n/static/50ab2214f80a3820049d777e1d662621/7a2f6/fermi.png 846w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span>\n    <figcaption class=\"gatsby-resp-image-figcaption\">fermi</figcaption>\n  </figure></p>\n<p>With dynamic parallelism, however:</p>\n<ul>\n<li>GPU can generate work on itself without involvement of CPU.</li>\n<li>Permits Dynamic Runtime decisions.</li>\n<li>Kernels can start new kernels</li>\n<li>Streams can spawn new streams.</li>\n</ul>\n<p><figure class=\"gatsby-resp-image-figure\" style=\"\">\n    <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 590px;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/24b92ace0dfe94ce81a5d0c6af27c43b/40275/kepler.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 98.82075471698114%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAACXBIWXMAABYlAAAWJQFJUiTwAAAChElEQVQ4y6VUW08TQRTe2Z2dvXa3u+32ynap2AtUgkUMFh5UKEaiaGNI0KgQ9cXERBM0GDEmPPioURLRBH+E8cVo4n/7nO3StKG2gD58e2bnnPnmXOYcgVAFcmIUshuAUAZBEPqg6gTJLIXMCCxHgm6K0AyCoMxQKDEYMbFrT+0c1OIctPIiqOMfKEgkSWTkZSWsrNu4ft/G1TULS6sxlCYVLN+JoXkrhmpd6bUXYDtJJFy3S9QDInbXoWdUJn06QnrOiPxTmL6M6pkpkL+Ee2JInN0r1ZHzA4jhTUSEKElcqcI/ncLD5yUkPLPfk8EQeIJNGIbRF0ptxsH+7wD+qH0ywj50ipHWsbyWQdzVTxL2MCXhaWBgCoWqRWBMgkTJMG+H36hqciRVGZTStlRVhROHknJiCkkKKy8OJuzcPlq2sb07AduO4ea6j7tPxjiJile7FYxPeag3kth6X4HCdDzaCnDtdmEA4UFRJqYtfPk1glTawcaLFDbf5WCaBj7/9HH+YgKNZhKffuShKga29zLY2MwODznh6Vi8EZLoqE27ODef4vlkWFjJIZOPIV+w2nrKW3b2Uhq1evLoHHa7J5TSoT2hJ3diZ//opyDLChRFbidf0/SeXEswTK291jSV27Hjefj0bQlXVnM8xDh29ivwi/G27sJCGm/2KmCyhsevi2htBMchFLHzdQStBx6KJRcfv+dxqhoRLrY8fPg2witv4OVuFveeHRRFYYy7rA30cGY+jWI5DsvWMdfMwo5HYecDC42FDC+KjLOzSZRribDLBART8yiPT0LTRZhW+FD5mKLiv0+b9ESDD4CxgeOrt80Ot1znvy0j8AHrpuA47v/PQo4/vpjNb2OTGgIAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"kepler\"\n        title=\"kepler\"\n        src=\"/static/24b92ace0dfe94ce81a5d0c6af27c43b/799d3/kepler.png\"\n        srcset=\"/static/24b92ace0dfe94ce81a5d0c6af27c43b/00d96/kepler.png 148w,\n/static/24b92ace0dfe94ce81a5d0c6af27c43b/0b23c/kepler.png 295w,\n/static/24b92ace0dfe94ce81a5d0c6af27c43b/799d3/kepler.png 590w,\n/static/24b92ace0dfe94ce81a5d0c6af27c43b/40275/kepler.png 848w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span>\n    <figcaption class=\"gatsby-resp-image-figcaption\">kepler</figcaption>\n  </figure></p>\n<h3>code example</h3>\n<div class=\"gatsby-highlight\" data-language=\"c\"><pre class=\"language-c\"><code class=\"language-c\">__global__ <span class=\"token function\">ChildKernel</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">void</span><span class=\"token operator\">*</span> data<span class=\"token punctuation\">)</span><span class=\"token punctuation\">{</span>\n    <span class=\"token comment\">// operate on data</span>\n<span class=\"token punctuation\">}</span>\n\n__global__ <span class=\"token function\">ParentKernel</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">void</span><span class=\"token operator\">*</span> data<span class=\"token punctuation\">)</span><span class=\"token punctuation\">{</span>\n    <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span>threadIdx<span class=\"token punctuation\">.</span>x <span class=\"token operator\">==</span> <span class=\"token number\">0</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span>\n        ChildKernel<span class=\"token operator\">&lt;&lt;</span><span class=\"token operator\">&lt;</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">32</span><span class=\"token operator\">>></span><span class=\"token operator\">></span><span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n        <span class=\"token function\">cudaThreadSynchronize</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token punctuation\">}</span>\n    <span class=\"token function\">syncthreads</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token comment\">//operate on data</span>\n<span class=\"token punctuation\">}</span>\n\n<span class=\"token comment\">// in host code</span>\nParentKernel<span class=\"token operator\">&lt;&lt;</span><span class=\"token operator\">&lt;</span><span class=\"token number\">8</span><span class=\"token punctuation\">,</span> <span class=\"token number\">32</span><span class=\"token operator\">>></span><span class=\"token operator\">></span><span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre></div>\n<h2>Some Special Notes</h2>\n<h3>asynchronous operation</h3>\n<p>As we have talked about before in <a href=\"/cuda8-Stream\">The CUDA Parallel Programming Model - 8. Concurrency by Stream</a>, device kernel launch is asynchronous.</p>\n<ul>\n<li>\n<p>Successful execution of a kernel launch means that the kernel is queued</p>\n<ul>\n<li>it may begin executing immediately</li>\n<li>or it may execute later when resources become available.</li>\n</ul>\n</li>\n<li>Note that every thread that encounters a kernel launch executes it. So be careful!</li>\n<li>Child grids always complete before the parent grids that launch them, even if there is no explicit synchronization.</li>\n</ul>\n<h3>sequential execution of thread grids</h3>\n<p>By default, grids launched within a thread block are executed sequentially.</p>\n<ul>\n<li>This happens even if grids are launched by different threads within the block.</li>\n<li>To deal with this drawback, use streams</li>\n<li>streams created on the host cannot be used on the device.</li>\n<li>Streams created in a block can be used by all threads in that block.</li>\n</ul>\n<h3>parent-child dependency</h3>\n<p>If the parent kernel needs results computed by the child kernel to do its own work, it must ensure that the <strong>child grid</strong> has finished execution before continuing. This is done by:</p>\n<ul>\n<li>\n<p>explicitly synchronizing using cudaDeviceSynchronize(void). </p>\n<ul>\n<li>This function waits for completion of all grids previously launched by the thread block from which it has been called.</li>\n</ul>\n</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"c\"><pre class=\"language-c\"><code class=\"language-c\"><span class=\"token keyword\">void</span> <span class=\"token function\">threadBlockDeviceSynchronize</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">void</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">{</span>\n<span class=\"token function\">__syncthreads</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token keyword\">if</span><span class=\"token punctuation\">(</span>threadIdx<span class=\"token punctuation\">.</span>x <span class=\"token operator\">==</span> <span class=\"token number\">0</span><span class=\"token punctuation\">)</span>\n<span class=\"token function\">cudaDeviceSynchronize</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token function\">__syncthreads</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token punctuation\">}</span></code></pre></div>"
}
