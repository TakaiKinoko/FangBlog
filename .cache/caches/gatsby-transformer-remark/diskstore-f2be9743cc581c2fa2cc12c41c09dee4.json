{
  "expireTime": 9007200830645884000,
  "key": "transformer-remark-markdown-html-d589d715cd0a65281233301bbb2d16b5-gatsby-remark-imagesgatsby-remark-responsive-iframegatsby-remark-prismjsgatsby-remark-copy-linked-filesgatsby-remark-smartypantsgatsby-remark-reading-time-",
  "val": "<p>In the previous posts, we have sometimes assumed that only one kernel is launched at a time. But this is not all that kernels can do. They can be launched <strong>sequentially</strong> or <strong>in parallel</strong>. It’s just that if multiple kernels are launched in parallel, CUDA streams must be used.</p>\n<h2>Motivation</h2>\n<p><strong>Heterogeneous computing</strong> is about efficiently using all processors in the system, including CPUs and GPUs. To do this, applications must <strong>execute functions concurrently</strong> on multiple processors.</p>\n<p>CUDA Applications manage concurrency by executing <strong>asynchronous</strong> commands in streams. </p>\n<ul>\n<li>\n<p>Each stream is a sequences of commands that execute <strong>in order</strong>. </p>\n</li>\n<li>\n<p>Different streams may execute their commands concurrently or <strong>out of order</strong> with respect to each other.</p>\n</li>\n</ul>\n<h2>What is a Stream?</h2>\n<p>In CUDA, stream refers to <strong>a single operation sequence</strong> on a GPU device.</p>\n<ul>\n<li>\n<p>Every CUDA kernel is invoked on an independent stream</p>\n</li>\n<li>\n<p>If only one kernel is invoked, the default stream, stream0, is used</p>\n</li>\n<li>\n<p>When you execute asynchronous CUDA commands without specifying a stream, the runtime uses the default stream</p>\n</li>\n<li>\n<p>If n kernels are invoked in parallel, n streams need to be used</p>\n</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"c\"><pre class=\"language-c\"><code class=\"language-c\">  kernel<span class=\"token operator\">&lt;&lt;</span><span class=\"token operator\">&lt;</span> blocks<span class=\"token punctuation\">,</span> threads<span class=\"token punctuation\">,</span> bytes <span class=\"token operator\">>></span><span class=\"token operator\">></span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>    <span class=\"token comment\">// default stream</span>\n  kernel<span class=\"token operator\">&lt;&lt;</span><span class=\"token operator\">&lt;</span> blocks<span class=\"token punctuation\">,</span> threads<span class=\"token punctuation\">,</span> bytes<span class=\"token punctuation\">,</span> <span class=\"token number\">0</span> <span class=\"token operator\">>></span><span class=\"token operator\">></span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span> <span class=\"token comment\">// also default stream</span></code></pre></div>\n<p>In CUDA, we can run multiple kernels on different streams concurrently. Typically, we can improve performance by increasing number of concurrent streams by setting a higher degree of parallelism.</p>\n<h2>Asynchronous Commands</h2>\n<h3>side note on blocking &#x26; non-blocking</h3>\n<p>A function is said to be blocking if it calls an operating system function that waits for an event to occur or a time period to elapse.</p>\n<p>Whilst a blocking call is waiting, the operating system can often remove that thread from the scheduler, so it takes no CPU time until the event has occurred or the time has elapsed. </p>\n<p>Once the event has occurred then the thread is placed back in the scheduler and can run when allocated a time slice. A thread that is running a blocking call is said to be blocked.</p>\n<p>Non-blocking threads meaning threads competing for a shared resource do not have their execution indefinitely postponed by mutual exclusion.</p>\n<h3>asynchronous commands, non-blocking</h3>\n<p>Asynchronous commands return control to the calling host thread before the device has finished the requested task (they are <strong>non-blocking</strong>). These commands are:</p>\n<ul>\n<li>Kernel launches</li>\n<li>Memory copies between two addresses to the same device memory</li>\n<li>Memory copies from host to device of a memory block of 64 KB or less</li>\n<li>Memory copies performed by functions with the Async suffix</li>\n<li>Memory set function calls</li>\n</ul>\n<p>CUDA 7 introduced the per-thread default stream, that has two effects:</p>\n<ol>\n<li>it gives each host thread its own default stream. This means that commands issued to the default stream by different host threads can run concurrently. </li>\n<li>these default streams are regular streams. This means that commands in the default stream may run concurrently with commands in non-default streams.</li>\n</ol>\n<p>To learn more about how to invoke these options in <code class=\"language-text\">nvcc</code>, check out the <em>CUDA C Programming Guide</em>.</p>\n<h2>Visualize a Multi-Stream Example</h2>\n<h3>a multi-stream example</h3>\n<div class=\"gatsby-highlight\" data-language=\"c\"><pre class=\"language-c\"><code class=\"language-c\"><span class=\"token keyword\">const</span> <span class=\"token keyword\">int</span> N <span class=\"token operator\">=</span> <span class=\"token number\">1</span> <span class=\"token operator\">&lt;&lt;</span> <span class=\"token number\">20</span><span class=\"token punctuation\">;</span>\n\n__global__ <span class=\"token keyword\">void</span> <span class=\"token function\">kernel</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">float</span> <span class=\"token operator\">*</span>x<span class=\"token punctuation\">,</span> <span class=\"token keyword\">int</span> n<span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">{</span>\n    <span class=\"token keyword\">int</span> tid <span class=\"token operator\">=</span> threadIdx<span class=\"token punctuation\">.</span>x <span class=\"token operator\">+</span> blockIdx<span class=\"token punctuation\">.</span>x <span class=\"token operator\">*</span> blockDim<span class=\"token punctuation\">.</span>x<span class=\"token punctuation\">;</span>\n    <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> i <span class=\"token operator\">=</span> tid<span class=\"token punctuation\">;</span> i <span class=\"token operator\">&lt;</span> n<span class=\"token punctuation\">;</span> i <span class=\"token operator\">+</span><span class=\"token operator\">=</span> blockDim<span class=\"token punctuation\">.</span>x <span class=\"token operator\">*</span> gridDim<span class=\"token punctuation\">.</span>x<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span>\n        x<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token function\">sqrt</span><span class=\"token punctuation\">(</span><span class=\"token function\">pow</span><span class=\"token punctuation\">(</span><span class=\"token number\">3.14159</span><span class=\"token punctuation\">,</span>i<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token punctuation\">}</span>\n<span class=\"token punctuation\">}</span>\n\n<span class=\"token keyword\">int</span> <span class=\"token function\">main</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">{</span>\n    <span class=\"token keyword\">const</span> <span class=\"token keyword\">int</span> num_streams <span class=\"token operator\">=</span> <span class=\"token number\">8</span><span class=\"token punctuation\">;</span>\n\n    cudaStream_t streams<span class=\"token punctuation\">[</span>num_streams<span class=\"token punctuation\">]</span><span class=\"token punctuation\">;</span>\n    <span class=\"token keyword\">float</span> <span class=\"token operator\">*</span>data<span class=\"token punctuation\">[</span>num_streams<span class=\"token punctuation\">]</span><span class=\"token punctuation\">;</span>\n\n    <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> i <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> i <span class=\"token operator\">&lt;</span> num_streams<span class=\"token punctuation\">;</span> i<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span>\n        <span class=\"token function\">cudaStreamCreate</span><span class=\"token punctuation\">(</span><span class=\"token operator\">&amp;</span>streams<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n \n        <span class=\"token function\">cudaMalloc</span><span class=\"token punctuation\">(</span><span class=\"token operator\">&amp;</span>data<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> N <span class=\"token operator\">*</span> <span class=\"token keyword\">sizeof</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">float</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n        \n        <span class=\"token comment\">// launch one worker kernel per stream</span>\n        kernel<span class=\"token operator\">&lt;&lt;</span><span class=\"token operator\">&lt;</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">64</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> streams<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token operator\">>></span><span class=\"token operator\">></span><span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> N<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n        <span class=\"token comment\">// launch a dummy kernel on the default stream</span>\n        kernel<span class=\"token operator\">&lt;&lt;</span><span class=\"token operator\">&lt;</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token operator\">>></span><span class=\"token operator\">></span><span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token punctuation\">}</span>\n\n    <span class=\"token function\">cudaDeviceReset</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n    <span class=\"token keyword\">return</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span>\n<span class=\"token punctuation\">}</span></code></pre></div>\n<h3>no concurrency</h3>\n<p>If the above code is run without concurrency:</p>\n<p><a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/272b7042291aad899de9779d6aeee7f2/9c6a9/no_concurr.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-wrapper\"\n    style=\"position: relative; display: block;  max-width: 590px; margin-left: auto; margin-right: auto;\"\n  >\n    <span\n      class=\"gatsby-resp-image-background-image\"\n      style=\"padding-bottom: 28.61328125%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAAAsSAAALEgHS3X78AAABFUlEQVQY021Qy06DQBSd//8TF267YGFME5PSEEyEFgpEah9RGmB4TSW6OJ47Tl25OLnvc8+9auX7yLMcfd+jaRrUdY1Wa1Rti4bWMK/p11JzaF0sVmYsnK/KskSapui6zqIn3lnwdjs8ZBmC/d7Gn8bAjCOmYYBmj0BE/M2JT6goirH215ZdCrLVDtCmhwMe4xheksDj0uR0wvFywTxNMCQeHaF2pAL1EkUk9C2RbBE7sLknDAe/rlf7greqgkfyBbHkVdH5jA/XKwtualVRFNhuNlah5uBNqfgCu4ixqJl5suS2VP7El9wHAZacf6byhj1CrvK8QBiG3EBF/JMkxf6HUU6l/Z7n358yfqXygIrvSO5R2A/Pxb5g6rg+PQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n    ></span>\n    <img\n        class=\"gatsby-resp-image-image\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;box-shadow:inset 0px 0px 0px 400px white;\"\n        alt=\"no_concurrency\"\n        title=\"\"\n        src=\"/static/272b7042291aad899de9779d6aeee7f2/d7711/no_concurr.png\"\n        srcset=\"/static/272b7042291aad899de9779d6aeee7f2/a695b/no_concurr.png 148w,\n/static/272b7042291aad899de9779d6aeee7f2/2f273/no_concurr.png 295w,\n/static/272b7042291aad899de9779d6aeee7f2/d7711/no_concurr.png 590w,\n/static/272b7042291aad899de9779d6aeee7f2/5e66f/no_concurr.png 885w,\n/static/272b7042291aad899de9779d6aeee7f2/9c6a9/no_concurr.png 1024w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n      />\n  </span>\n  </a></p>\n<h3>with concurrency</h3>\n<p>If the above code is run with full-concurrency (can be achieved by utilizing <code class=\"language-text\">nvcc</code>’s <code class=\"language-text\">--default-stream</code> option)</p>\n<p><a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/350898cf39e112f964ef921845ef2846/9c6a9/concurr.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-wrapper\"\n    style=\"position: relative; display: block;  max-width: 590px; margin-left: auto; margin-right: auto;\"\n  >\n    <span\n      class=\"gatsby-resp-image-background-image\"\n      style=\"padding-bottom: 27.05078125%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAAAsSAAALEgHS3X78AAABN0lEQVQY01VQS0/CYBDsvzQaEx/ozYTE+Lj6IB41Brx5M7bYYpRXVFAeFaoeWoHSFggcCBdDoSktnMf9GiTxMNnJzO5kd7mfoQ1V1dDv9+G6Loa2Ddmy8NxoIKfreJmD8Wq7DYVQMIyF9lef6nW0BgNwvu+h2TRgWS34vg97NMLN1ycuSyXEZDlAVC7jimpcVSFqGq4VBdFyeeEz77xYxEe3C24ymUDXm+h0OvA8D47j4CSXw4ogYFOSsCGKWCcwfvSaR6TwhnAqiTXx7p+3xN/ivlYD51Jgg84zTRMs3BmPEcnnsRoXsEWNIYkGaIjxY9LPKHA3nQrCQnN/OyFhmeeR+NbATadTtOkvvV4Ps9ksCI1V3hFOJnGYzWI/kwlwQLhgJ1YrOKVN9zLpQGMe69t5fECafvkLf3o+r50mdFUAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n    ></span>\n    <img\n        class=\"gatsby-resp-image-image\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;box-shadow:inset 0px 0px 0px 400px white;\"\n        alt=\"with_concurrency\"\n        title=\"\"\n        src=\"/static/350898cf39e112f964ef921845ef2846/d7711/concurr.png\"\n        srcset=\"/static/350898cf39e112f964ef921845ef2846/a695b/concurr.png 148w,\n/static/350898cf39e112f964ef921845ef2846/2f273/concurr.png 295w,\n/static/350898cf39e112f964ef921845ef2846/d7711/concurr.png 590w,\n/static/350898cf39e112f964ef921845ef2846/5e66f/concurr.png 885w,\n/static/350898cf39e112f964ef921845ef2846/9c6a9/concurr.png 1024w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n      />\n  </span>\n  </a></p>\n<h2>Between Streams</h2>\n<p>The concurrent streams are <strong>independent</strong>, which means that streams neither communicate nor have any inter-stream dependency.</p>\n<p>However, different streams may have different execution times (<strong>asynchronous</strong>), and we cannot guarantee that all streams will complete at the same time. To ensure all streams are synchronized, use a synchronization barrier.</p>\n<h2>Synchronize Streams</h2>\n<p>There are two types of stream synchronization in CUDA. A programmer can place the synchronization barrier explicitly, to synchronize tasks such as memory operations. Some functions are implicitly synchronized, which means one or all streams must complete before proceeding to the next section.</p>\n<h3>explicit synchronization</h3>\n<p>Invoking multiple concurrent kernels requires slightly more programming than invoking one kernel at a time:</p>\n<ul>\n<li>Kernels need to be assigned to different streams and invoked asynchronously</li>\n<li>Kernel launch and memory transfer functions need to be assigned to the same stream</li>\n<li>pinned memory must be used</li>\n<li>Asynchronous memory transfer API functions must be used</li>\n<li>the synchronization barrier cudaStreamSynchronize() must be used to ensure all tasks are synchronized</li>\n</ul>\n<h3>implicit synchronization</h3>\n<p>The following operations are implicitly synchronized; therefore, no barrier is needed:</p>\n<ul>\n<li>\n<p>page-locked memory allocation </p>\n<ul>\n<li>cudaMallocHost</li>\n<li>cudaHostAlloc</li>\n</ul>\n</li>\n<li>\n<p>device memory allocation</p>\n<ul>\n<li>cudaMalloc</li>\n</ul>\n</li>\n<li>\n<p>Non-asynchronized memory operation</p>\n<ul>\n<li>cudaMemcpy</li>\n</ul>\n</li>\n<li>\n<p>Memory configuration</p>\n<ul>\n<li>cudaDeviceSetCacheConfig</li>\n</ul>\n</li>\n</ul>"
}
