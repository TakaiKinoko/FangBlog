{
  "expireTime": 9007200830738555000,
  "key": "transformer-remark-markdown-html-bb89bbd780e6ea9977942259a80f3e0a-gatsby-remark-imagesgatsby-remark-responsive-iframegatsby-remark-prismjsgatsby-remark-copy-linked-filesgatsby-remark-smartypantsgatsby-remark-reading-time-",
  "val": "<p>There’s an intrinsic tradeoff in the use of device memories in CUDA: the <strong>global memory</strong> is large but slow, whereas the <strong>shared memory</strong> is small but fast.</p>\n<p>(To recap on the memory hierarchy: <a href=\"/cuda1\">The CUDA Parallel Programming Model - 1. Concepts</a>, on how to specify memories for variables: <a href=\"/cudaProg2-Variables\">CUDA Programming - 2. CUDA Variable Type Qualifiers</a>.)</p>\n<p>A common strategy is to partition the <strong>data</strong> into subsets called <strong>tiles</strong> so that <strong>each tile fits into the shared memory</strong>.</p>\n<p>The term “tile” draws on the analogy that a large wall (i.e., the global memory data) can be covered by tiles (i.e., subsets that each can fit into the shared memory). An important criterion is that kernel computation on these tiles can be performed independently of each other. Note that not all data structures can be partitioned into tiles given an arbitrary kernel function.</p>"
}
