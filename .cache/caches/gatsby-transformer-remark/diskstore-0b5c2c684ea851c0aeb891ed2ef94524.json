{
  "expireTime": 9007200830838267000,
  "key": "transformer-remark-markdown-html-2ede7c84db3de1127a30280c2b3b5408-gatsby-remark-imagesgatsby-remark-responsive-iframegatsby-remark-prismjsgatsby-remark-copy-linked-filesgatsby-remark-smartypantsgatsby-remark-reading-time-",
  "val": "<h2>Unified Virtual Address Space (UVA)</h2>\n<p>From CUDA 4.0 and on, UVA has been an important feature. It</p>\n<ul>\n<li>puts all CUDA execution, host and GPUs, in the same address space</li>\n<li>Requires Fermi-class GPU and above</li>\n<li>Requires 64-bit application</li>\n<li>Call <code class=\"language-text\">cudaGetDeviceProperties()</code> for all participating devices and check <code class=\"language-text\">unifiedAddressing</code> flag</li>\n</ul>\n<p><figure class=\"gatsby-resp-image-figure\" style=\"\">\n    <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 590px;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/716ff7cbec1625aebc2078e7c552f7a4/d4c84/uva.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 53.36481700118064%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAABYlAAAWJQFJUiTwAAACjklEQVQoz1WSzU8TYRDGty20XbbsLsu23e1+dLctsdCCRQ0BkaC2ioIgKBoVMH5BCCdNIMbEiCbIxYMHNSZ+HPyKMYaLdy8m/l0/t1sFOUzmmbwzzzvPzAiCIPDPIpFI6NsSEbIFiYzfQbct0u2IaJZIV04kW5TQnQ50VyKZiv2t2+MQ/idsWsZU6alY+H1Z+oddKkcsyrUctZECh0aLVIccBoY9KoEv12wMS9tXL0iShGmaZDKZ0DuujefnMbJGEBtBbNGtdRGLxVC7ZBzHJpdr5jkBtsh7DpqmhfWJRAJBVVUs20FPpykUCnheHssKOvR9DMMIYo90OhP+LopimKfpaYqlUvih67rkLBO/4JFKpVqSNbkTz8y2JOsGKUkOsWXmg/lEW3P6K8k2stha691zeohHRaJCO07O25vhyYXbXHn8HEVWuLcZ4FszgRyXrZf3GG8Mtwij0ZB05s4a9UevObO2yPrTS9y+P83NjUnWt29Q6ikgRJtJ649Z/fqTtJbm1bcnbGytUOnrZ+f3Cy5fn2kRBjOMBX7q7kPG3vxi9e02zz7d5OnHVbY/L/Pi+wNqg7VWh37eonFiNCw83jhK9eCBEE9fmMC0MvtOqtJb4uhQjVjQ8bnZ0+FSFEVl8nwdKSUiJOMC41Ob1Od3KPZajM/lGZl0sQci1C/20jeq7p5Ep9TOxOV3nJr/gNUTpzFfpjqq4w8mmFrsx691IIjJCLNXt1hY/UF5wKdxzWdsNk/xsMjUUh9DZ3K7i1HkJFeX37Ow8oVSVeXsUplDdZPeEYW5lQGqx3SEtrZ2cqZOwTeQZRnT6SZtqIEMBdPVyPs2YlIMCdvjcWxLx3UypDo7w3dNV5CVJu4Kzs/kD/8hOeOAj+GnAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"uva\"\n        title=\"uva\"\n        src=\"/static/716ff7cbec1625aebc2078e7c552f7a4/799d3/uva.png\"\n        srcset=\"/static/716ff7cbec1625aebc2078e7c552f7a4/00d96/uva.png 148w,\n/static/716ff7cbec1625aebc2078e7c552f7a4/0b23c/uva.png 295w,\n/static/716ff7cbec1625aebc2078e7c552f7a4/799d3/uva.png 590w,\n/static/716ff7cbec1625aebc2078e7c552f7a4/2a3d6/uva.png 885w,\n/static/716ff7cbec1625aebc2078e7c552f7a4/ae92f/uva.png 1180w,\n/static/716ff7cbec1625aebc2078e7c552f7a4/d4c84/uva.png 1694w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span>\n    <figcaption class=\"gatsby-resp-image-figcaption\">uva</figcaption>\n  </figure></p>\n<h3>easier memory access with UVA</h3>\n<p>Zero-copy</p>\n<ul>\n<li>UVA provides a <strong>single virtual memory address space</strong> for all memory in the system, and enables pointers to be accessed from GPU code no matter where in the system they reside.</li>\n<li>Pointers returned by cudaHostAlloc() can be used directly from within kernels running on UVA enabled devices\n– Data cache in L2 of target device.</li>\n</ul>\n<h3>easier memory copy with UVA</h3>\n<h4>between host and multiple devices</h4>\n<div class=\"gatsby-highlight\" data-language=\"c\"><pre class=\"language-c\"><code class=\"language-c\"><span class=\"token function\">cudaMemcpy</span><span class=\"token punctuation\">(</span>gpu0_buf<span class=\"token punctuation\">,</span> host_buf<span class=\"token punctuation\">,</span> buf_size<span class=\"token punctuation\">,</span> cudaMemcpyDefault<span class=\"token punctuation\">)</span>\n<span class=\"token function\">cudaMemcpy</span><span class=\"token punctuation\">(</span>gpu1_buf<span class=\"token punctuation\">,</span> host_buf<span class=\"token punctuation\">,</span> buf_size<span class=\"token punctuation\">,</span> cudaMemcpyDefault<span class=\"token punctuation\">)</span>\n<span class=\"token function\">cudaMemcpy</span><span class=\"token punctuation\">(</span>host_buf<span class=\"token punctuation\">,</span> gpu0_buf<span class=\"token punctuation\">,</span> buf_size<span class=\"token punctuation\">,</span> cudaMemcpyDefault<span class=\"token punctuation\">)</span>\n<span class=\"token function\">cudaMemcpy</span><span class=\"token punctuation\">(</span>host_buf<span class=\"token punctuation\">,</span> gpu1_buf<span class=\"token punctuation\">,</span> buf_size<span class=\"token punctuation\">,</span> cudaMemcpyDefault<span class=\"token punctuation\">)</span></code></pre></div>\n<h4>between two devices:</h4>\n<div class=\"gatsby-highlight\" data-language=\"c\"><pre class=\"language-c\"><code class=\"language-c\"><span class=\"token function\">cudaMemcpy</span><span class=\"token punctuation\">(</span>gpu0_buf<span class=\"token punctuation\">,</span> gpu1_buf<span class=\"token punctuation\">,</span> buf_size<span class=\"token punctuation\">,</span> cudaMemcpyDefault<span class=\"token punctuation\">)</span></code></pre></div>\n<p>cudaMemcpy() knows that our buffers are on different devices.</p>\n<h2>Unified Memory</h2>\n<p>Available for CUDA 6.0 and up.</p>\n<p><figure class=\"gatsby-resp-image-figure\" style=\"\">\n    <span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 590px;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/880600ba85b509ae39620a262a6d5a22/c9629/unifiedMem.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 64.60905349794238%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAABYlAAAWJQFJUiTwAAACxElEQVQ4y62SaUhUURTHX30riMC+FEUrVBRKEBQIERVIywcJigiKINujQinJ1GxxVKyxpJo0l7TNFM1MHU2n3FPTNGksc8GFlrEsMY1p5r13f93xVVafPXC4/3vOuf97zp+jIE0IME6BrhtY1wVOp5HQNIHL5cG/Cv8zIcbiyhihQFWh1S4otgpKHwtycwTVlYLaZ4L8PEFqssBaIHhSKqgoF9hKBP0O8Q+p4gG/3UM6MCDo6xM4PsrifkFHu3Ef/Grg162C7m5Bj/SODsHQkPHO89zDoTBu9leHulDRdPfoqUsRVc2Nqv/GKm7NJe8ypun8cBuuenRVJVb10bgxpSRU9R+kvfAntnIhGS07qOu7wcWqJcRVe9P2uYj8N0FEPp3LnZdrSalvZWlMC4tMjVyueM/6BDvzzzewN7Pjj46KSxshqnwOwUUTJIkPhW3Bo/i4VaGqy0Jyw0aCCj05L0KttUwMrEc5WsHh7E5mRTxHOVbFirhmySYn8hBqskNLnS+hxZNIaVxPdU8cp0umctbmhd2RQ27rPkKKJpPauAxLjZ3pYU14hdQSbXsniVqYEvyMbWltY2vjkhoNOQext7/ky/AATs1JW89ruj90Mez6zjfXCE1vmnAMOhh2u+jt/yTzvTh1jY9fB0fxiNsttXRLrTWUtIYyTLZcThVmcORhKrszr7El3cyGpEhWW8JZfvkEi2OO4mMOwvdqCH6JEWxOi2ZnRjyHchI5UZDOudIszJV5PH7bhBJb9ogD2YnsuBfPppQoVl0Nw9scyOzIvSyIOshc036mhe1k5rkA5pkOMONsAItij7Ay/iR+N86z5baZPZkWjhfcJKulCuV+cw2XKvKJsuVwynqXY7LLgCwL22Whf2o06xIi8L0Swprr4fJDE1vTL7Dr/hUOP0giuOAWZ0qyuFieR0JtMeWdr8ZzsQ37Cav0Y2imXCJXAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"unified memory\"\n        title=\"unified memory\"\n        src=\"/static/880600ba85b509ae39620a262a6d5a22/799d3/unifiedMem.png\"\n        srcset=\"/static/880600ba85b509ae39620a262a6d5a22/00d96/unifiedMem.png 148w,\n/static/880600ba85b509ae39620a262a6d5a22/0b23c/unifiedMem.png 295w,\n/static/880600ba85b509ae39620a262a6d5a22/799d3/unifiedMem.png 590w,\n/static/880600ba85b509ae39620a262a6d5a22/2a3d6/unifiedMem.png 885w,\n/static/880600ba85b509ae39620a262a6d5a22/ae92f/unifiedMem.png 1180w,\n/static/880600ba85b509ae39620a262a6d5a22/c9629/unifiedMem.png 1458w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span>\n    <figcaption class=\"gatsby-resp-image-figcaption\">unified memory</figcaption>\n  </figure></p>\n<p>Creates a pool of managed memory that is shared between the CPU and GPU.</p>\n<ul>\n<li>Managed memory is accessible to CPU and GPU with single pointers.</li>\n<li>\n<p>Under the hood: data (granularity = pages) automatically migrates from CPU to GPU and among GPUs. </p>\n<ul>\n<li>Pascal GPU architecture is the first with hardware support for virtual memory page faulting and migration.</li>\n</ul>\n</li>\n</ul>\n<h3>usage</h3>\n<div class=\"gatsby-highlight\" data-language=\"c\"><pre class=\"language-c\"><code class=\"language-c\">cudaError_t <span class=\"token function\">cudaMallocManaged</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">void</span><span class=\"token operator\">*</span><span class=\"token operator\">*</span> ptr<span class=\"token punctuation\">,</span> size_t size<span class=\"token punctuation\">)</span></code></pre></div>\n<p>ptr can be used by any GPU and CPU in the system.</p>\n<h4>Pascal GPU:</h4>\n<ul>\n<li>Pages may not be created until they are accessed by the GPU or the CPU. </li>\n<li>Pages automatically migrate to the device (or host) that access them.</li>\n</ul>\n<h4>Pre-PASCAL (i.e. Kepler and Maxwell)</h4>\n<ul>\n<li>With single GPU, data will be allocated on the GPU device that is active when the call is made.</li>\n<li>On multi-GPU systems, if some of the GPUs have peer-to-peer access disabled, the memory will be allocated so it is initially resident on the CPU.</li>\n</ul>\n<h3>difference between UVA and unified memory</h3>\n<ul>\n<li>Unified memory depends on UVA.</li>\n<li>UVA does NOT move data automatically between CPU and GPU.</li>\n<li>Unified memory gives higher performance than UVA.</li>\n</ul>\n<h3>pros and cons of unified memory</h3>\n<h4>advantages</h4>\n<ul>\n<li>Ease of programming</li>\n<li>Data is migrated on demand\n– offer the performance of local data on the GPU\n– while providing the ease of use of globally shared data</li>\n<li>Very efficient with complex data structures (e.g. linked lists, structures with pointers, … ).</li>\n</ul>\n<h4>disadvantage</h4>\n<p>Carefully tuned CUDA program that uses streams to efficiently overlap execution with data transfers may\nperform better than a CUDA program\nthat only uses Unified Memory.</p>"
}
