{
  "expireTime": 9007200830737027000,
  "key": "transformer-remark-markdown-ast-7f42d3b5cc35a9013f7f695038faa3c3-gatsby-remark-imagesgatsby-remark-responsive-iframegatsby-remark-prismjsgatsby-remark-copy-linked-filesgatsby-remark-smartypantsgatsby-remark-reading-time-",
  "val": {
    "type": "root",
    "children": [
      {
        "type": "paragraph",
        "children": [
          {
            "type": "text",
            "value": "I briefly talked about how CUDA processors hide long-latency operations such as global memory accesses through their warp-scheduling mechanism in ",
            "position": {
              "start": { "line": 2, "column": 1, "offset": 1 },
              "end": { "line": 2, "column": 147, "offset": 147 },
              "indent": []
            }
          },
          {
            "type": "link",
            "title": null,
            "url": "/cuda2-warp",
            "children": [
              {
                "type": "text",
                "value": "The CUDA Parallel Programming Model - 2. Warps",
                "position": {
                  "start": { "line": 2, "column": 148, "offset": 148 },
                  "end": { "line": 2, "column": 194, "offset": 194 },
                  "indent": []
                }
              }
            ],
            "position": {
              "start": { "line": 2, "column": 147, "offset": 147 },
              "end": { "line": 2, "column": 208, "offset": 208 },
              "indent": []
            }
          },
          {
            "type": "text",
            "value": ".",
            "position": {
              "start": { "line": 2, "column": 208, "offset": 208 },
              "end": { "line": 2, "column": 209, "offset": 209 },
              "indent": []
            }
          }
        ],
        "position": {
          "start": { "line": 2, "column": 1, "offset": 1 },
          "end": { "line": 2, "column": 209, "offset": 209 },
          "indent": []
        }
      },
      {
        "type": "heading",
        "depth": 2,
        "children": [
          {
            "type": "text",
            "value": "Recap",
            "position": {
              "start": { "line": 4, "column": 4, "offset": 214 },
              "end": { "line": 4, "column": 9, "offset": 219 },
              "indent": []
            }
          }
        ],
        "position": {
          "start": { "line": 4, "column": 1, "offset": 211 },
          "end": { "line": 4, "column": 9, "offset": 219 },
          "indent": []
        }
      },
      {
        "type": "paragraph",
        "children": [
          {
            "type": "text",
            "value": "Unlike CPUs, GPUs do not dedicate as much chip area to cache memories and branch prediction mechanisms. This is because GPUs have the ability to tolerate long-latency operations.",
            "position": {
              "start": { "line": 6, "column": 1, "offset": 221 },
              "end": { "line": 6, "column": 179, "offset": 399 },
              "indent": []
            }
          }
        ],
        "position": {
          "start": { "line": 6, "column": 1, "offset": 221 },
          "end": { "line": 6, "column": 179, "offset": 399 },
          "indent": []
        }
      },
      {
        "type": "paragraph",
        "children": [
          {
            "type": "text",
            "value": "GPU SMs are designed in the way that each SM can execute only a small number of warps at any given time. Normally the number of warps residing on the SM is much bigger than what can actually be executed. The reason for this is, when a warp is currently waiting for result from a long-latency operation, such as: ",
            "position": {
              "start": { "line": 8, "column": 1, "offset": 401 },
              "end": { "line": 8, "column": 313, "offset": 713 },
              "indent": []
            }
          }
        ],
        "position": {
          "start": { "line": 8, "column": 1, "offset": 401 },
          "end": { "line": 8, "column": 313, "offset": 713 },
          "indent": []
        }
      },
      {
        "type": "paragraph",
        "children": [
          {
            "type": "text",
            "value": "*\n*\n*",
            "position": {
              "start": { "line": 10, "column": 1, "offset": 715 },
              "end": { "line": 12, "column": 2, "offset": 720 },
              "indent": [1, 1]
            }
          }
        ],
        "position": {
          "start": { "line": 10, "column": 1, "offset": 715 },
          "end": { "line": 12, "column": 2, "offset": 720 },
          "indent": [1, 1]
        }
      },
      {
        "type": "paragraph",
        "children": [
          {
            "type": "text",
            "value": "Assume that a CUDA device allows up to 8 blocks and 1024 threads per SM, whichever becomes a limitation first. Furthermore, it allows up to 512 threads in each block. For image blur, should we use 8 ×8, 16 ×16, or 32 ×32 thread blocks? To answer the question, we can analyze the pros and cons of each choice. If we use 8 ×8 blocks, each block would have only 64 threads. We will need 1024/64 =12 blocks to fully occupy an SM. However, each SM can only allow up to 8 blocks; thus, we will end up with only 64 ×8 =512 threads in each SM. This limited number implies that the SM execution resources will likely be underutilized because fewer warps will be available to schedule around long-latency operations.",
            "position": {
              "start": { "line": 14, "column": 1, "offset": 722 },
              "end": { "line": 14, "column": 707, "offset": 1428 },
              "indent": []
            }
          }
        ],
        "position": {
          "start": { "line": 14, "column": 1, "offset": 722 },
          "end": { "line": 14, "column": 707, "offset": 1428 },
          "indent": []
        }
      },
      {
        "type": "paragraph",
        "children": [
          {
            "type": "text",
            "value": "The 16 ×16 blocks result in 256 threads per block, implying that each SM can take 1024/256 =4 blocks. This number is within the 8-block limitation and is a good configuration as it will allow us a full thread capacity in each SM and a maximal number of warps for scheduling around the long-latency operations. The 32 ×32 blocks would give 1024 threads in each block, which exceeds the 512 threads per block limitation of this device. Only 16 ×16 blocks allow a maximal number of threads assigned to each SM.",
            "position": {
              "start": { "line": 16, "column": 1, "offset": 1430 },
              "end": { "line": 16, "column": 508, "offset": 1937 },
              "indent": []
            }
          }
        ],
        "position": {
          "start": { "line": 16, "column": 1, "offset": 1430 },
          "end": { "line": 16, "column": 508, "offset": 1937 },
          "indent": []
        }
      }
    ],
    "position": {
      "start": { "line": 1, "column": 1, "offset": 0 },
      "end": { "line": 16, "column": 508, "offset": 1937 }
    }
  }
}
