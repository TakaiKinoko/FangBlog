{
  "expireTime": 9007200830644120000,
  "key": "transformer-remark-markdown-html-40221d1870ec860b9358d962bd38fbb1-gatsby-remark-imagesgatsby-remark-responsive-iframegatsby-remark-prismjsgatsby-remark-copy-linked-filesgatsby-remark-smartypantsgatsby-remark-reading-time-",
  "val": "<p>In the previous posts, we have sometimes assumed that only one kernel is launched at a time. But this is not all that kernels can do. They can be launched <strong>sequentially</strong> or <strong>in parallel</strong>. Itâ€™s just that if multiple kernels are launched in parallel, CUDA streams must be used.</p>\n<h2>Motivation</h2>\n<p><strong>Heterogeneous computing</strong> is about efficiently using all processors in the system, including CPUs and GPUs. To do this, applications must <strong>execute functions concurrently</strong> on multiple processors.</p>\n<p>CUDA Applications manage concurrency by executing asynchronous** commands in streams, sequences of commands that execute in order. Different streams may execute their commands concurrently or out of order with respect to each other</p>\n<h2>What is a Stream?</h2>\n<p>In CUDA, stream refers to <strong>a single operation sequence</strong> on a GPU device.</p>\n<ul>\n<li>\n<p>Every CUDA kernel is invoked on an independent stream</p>\n</li>\n<li>\n<p>If only one kernel is invoked, the default stream, stream0, is used</p>\n</li>\n<li>\n<p>If n kernels are invoked in parallel, n streams need to be used</p>\n</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"c\"><pre class=\"language-c\"><code class=\"language-c\">  kernel<span class=\"token operator\">&lt;&lt;</span><span class=\"token operator\">&lt;</span> blocks<span class=\"token punctuation\">,</span> threads<span class=\"token punctuation\">,</span> bytes <span class=\"token operator\">>></span><span class=\"token operator\">></span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>    <span class=\"token comment\">// default stream</span>\n  kernel<span class=\"token operator\">&lt;&lt;</span><span class=\"token operator\">&lt;</span> blocks<span class=\"token punctuation\">,</span> threads<span class=\"token punctuation\">,</span> bytes<span class=\"token punctuation\">,</span> <span class=\"token number\">0</span> <span class=\"token operator\">>></span><span class=\"token operator\">></span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span> <span class=\"token comment\">// stream 0</span></code></pre></div>\n<p>In CUDA, we can run multiple kernels on different streams concurrently. Typically, we can improve performance by increasing number of concurrent streams by setting a higher degree of parallelism.</p>\n<h2>Between Streams</h2>\n<p>The concurrent streams are independent, which means that streams neither communicate nor have any inter-stream dependency.</p>\n<p>However, different streams may have different execution times, and we cannot guarantee that all streams will complete at the same time. To ensure all streams are synchronized, use a synchronization barrier.</p>\n<p>There are two types of stream synchronization in CUDA. A programmer can place the synchronization barrier explicitly, to synchronize tasks such as memory operations. Some functions are implicitly synchronized, which means one or all streams must complete before proceeding to the next section.</p>"
}
