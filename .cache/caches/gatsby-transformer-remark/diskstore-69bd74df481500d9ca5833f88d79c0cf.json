{"expireTime":9007200830664054000,"key":"transformer-remark-markdown-html-17925ad4b6c23c1dc9b65a58f4cee4a4-gatsby-remark-imagesgatsby-remark-responsive-iframegatsby-remark-prismjsgatsby-remark-copy-linked-filesgatsby-remark-smartypantsgatsby-remark-reading-time-","val":"<p>Motivated by a CUDA puzzle I tried to solve today, I’d like to talk more about resource assignment. </p>\n<h2>A Puzzle</h2>\n<h3>problem</h3>\n<p>Adding two big arrays element-wise.</p>\n<h3>settings</h3>\n<ul>\n<li>Suppose a GPU has 8 SMs</li>\n<li>Each SM has 32 SPs</li>\n<li>Warp size is 16, instead of 32</li>\n<li>kernel adds one element from each array together</li>\n</ul>\n<h3>questions</h3>\n<p>Say if we were to compute everything sequentially, it takes time <code class=\"language-text\">t</code> to finish.</p>\n<h4>scenario 1</h4>\n<p>Assume that 256 threads are enough to keep all SPs in the SM busy all the time. What is the amount of time it’d take to perform the computation for <strong>one block of 1024</strong> threads? </p>\n<h4>scenario 2</h4>\n<p>What if we use <strong>two blocks</strong> of <strong>512 threads</strong> each?</p>\n<h2>CUDA Resource Assignment</h2>\n<p>In order to see how the two scenarios differ, I thougth it was necessary to take another look at how resources are assigned to blocks and warps.</p>\n<h3>sequence of events</h3>\n<ol>\n<li>kernel launch</li>\n<li>CUDA runtime generates the corresponding grid of threads.</li>\n<li>threads are assigned to execution resources on a block-to-block basis</li>\n</ol>"}