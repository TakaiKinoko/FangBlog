{
  "expireTime": 9007200830838481000,
  "key": "transformer-remark-markdown-html-ceb4e65a8f6c7da6933e1575aa8b71f1-gatsby-remark-imagesgatsby-remark-responsive-iframegatsby-remark-prismjsgatsby-remark-copy-linked-filesgatsby-remark-smartypantsgatsby-remark-reading-time-",
  "val": "<h2>Unified Virtual Address Space (UVA)</h2>\n<p>From CUDA 4.0 and on, UVA has been an important feature. It</p>\n<ul>\n<li>puts all CUDA execution, host and GPUs, in the same address space</li>\n<li>Requires Fermi-class GPU and above</li>\n<li>Requires 64-bit application</li>\n<li>Call <code class=\"language-text\">cudaGetDeviceProperties()</code> for all participating devices and check <code class=\"language-text\">unifiedAddressing</code> flag</li>\n</ul>\n<p><img src=\"./uva.png\" alt=\"uva\"></p>\n<h3>easier memory access with UVA</h3>\n<p>Zero-copy</p>\n<ul>\n<li>UVA provides a <strong>single virtual memory address space</strong> for all memory in the system, and enables pointers to be accessed from GPU code no matter where in the system they reside.</li>\n<li>Pointers returned by cudaHostAlloc() can be used directly from within kernels running on UVA enabled devices\n– Data cache in L2 of target device.</li>\n</ul>\n<h3>easier memory copy with UVA</h3>\n<h4>between host and multiple devices</h4>\n<div class=\"gatsby-highlight\" data-language=\"c\"><pre class=\"language-c\"><code class=\"language-c\"><span class=\"token function\">cudaMemcpy</span><span class=\"token punctuation\">(</span>gpu0_buf<span class=\"token punctuation\">,</span> host_buf<span class=\"token punctuation\">,</span> buf_size<span class=\"token punctuation\">,</span> cudaMemcpyDefault<span class=\"token punctuation\">)</span>\n<span class=\"token function\">cudaMemcpy</span><span class=\"token punctuation\">(</span>gpu1_buf<span class=\"token punctuation\">,</span> host_buf<span class=\"token punctuation\">,</span> buf_size<span class=\"token punctuation\">,</span> cudaMemcpyDefault<span class=\"token punctuation\">)</span>\n<span class=\"token function\">cudaMemcpy</span><span class=\"token punctuation\">(</span>host_buf<span class=\"token punctuation\">,</span> gpu0_buf<span class=\"token punctuation\">,</span> buf_size<span class=\"token punctuation\">,</span> cudaMemcpyDefault<span class=\"token punctuation\">)</span>\n<span class=\"token function\">cudaMemcpy</span><span class=\"token punctuation\">(</span>host_buf<span class=\"token punctuation\">,</span> gpu1_buf<span class=\"token punctuation\">,</span> buf_size<span class=\"token punctuation\">,</span> cudaMemcpyDefault<span class=\"token punctuation\">)</span></code></pre></div>\n<h4>between two devices:</h4>\n<div class=\"gatsby-highlight\" data-language=\"c\"><pre class=\"language-c\"><code class=\"language-c\"><span class=\"token function\">cudaMemcpy</span><span class=\"token punctuation\">(</span>gpu0_buf<span class=\"token punctuation\">,</span> gpu1_buf<span class=\"token punctuation\">,</span> buf_size<span class=\"token punctuation\">,</span> cudaMemcpyDefault<span class=\"token punctuation\">)</span></code></pre></div>\n<p>cudaMemcpy() knows that our buffers are on different devices.</p>\n<h2>Unified Memory</h2>\n<p>Available for CUDA 6.0 and up.</p>\n<p><img src=\"./unifiedMem.png\" alt=\"unified memory\"></p>\n<p>Creates a pool of managed memory that is shared between the CPU and GPU.</p>\n<ul>\n<li>Managed memory is accessible to CPU and GPU with single pointers.</li>\n<li>\n<p>Under the hood: data (granularity = pages) automatically migrates from CPU to GPU and among GPUs. </p>\n<ul>\n<li>Pascal GPU architecture is the first with hardware support for virtual memory page faulting and migration.</li>\n</ul>\n</li>\n</ul>\n<h3>usage</h3>\n<div class=\"gatsby-highlight\" data-language=\"c\"><pre class=\"language-c\"><code class=\"language-c\">cudaError_t <span class=\"token function\">cudaMallocManaged</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">void</span><span class=\"token operator\">*</span><span class=\"token operator\">*</span> ptr<span class=\"token punctuation\">,</span> size_t size<span class=\"token punctuation\">)</span></code></pre></div>\n<p>ptr can be used by any GPU and CPU in the system.</p>\n<h4>Pascal GPU:</h4>\n<ul>\n<li>Pages may not be created until they are accessed by the GPU or the CPU. </li>\n<li>Pages automatically migrate to the device (or host) that access them.</li>\n</ul>\n<h4>Pre-PASCAL (i.e. Kepler and Maxwell)</h4>\n<ul>\n<li>With single GPU, data will be allocated on the GPU device that is active when the call is made.</li>\n<li>On multi-GPU systems, if some of the GPUs have peer-to-peer access disabled, the memory will be allocated so it is initially resident on the CPU.</li>\n</ul>\n<h4>example</h4>\n<div class=\"gatsby-highlight\" data-language=\"c\"><pre class=\"language-c\"><code class=\"language-c\"><span class=\"token keyword\">int</span> <span class=\"token function\">main</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span>\n    <span class=\"token keyword\">int</span> <span class=\"token operator\">*</span>ret<span class=\"token punctuation\">;</span>\n\n    <span class=\"token function\">cudaMallocManaged</span><span class=\"token punctuation\">(</span><span class=\"token operator\">&amp;</span>ret<span class=\"token punctuation\">,</span> <span class=\"token number\">1000</span> <span class=\"token operator\">*</span> <span class=\"token keyword\">sizeof</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n    AplusB<span class=\"token operator\">&lt;&lt;</span><span class=\"token operator\">&lt;</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1000</span> <span class=\"token operator\">>></span><span class=\"token operator\">></span><span class=\"token punctuation\">(</span>ret<span class=\"token punctuation\">,</span> <span class=\"token number\">10</span><span class=\"token punctuation\">,</span> <span class=\"token number\">100</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token function\">cudaDeviceSynchronize</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n    <span class=\"token keyword\">for</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> i<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">;</span> i<span class=\"token operator\">&lt;</span><span class=\"token number\">1000</span><span class=\"token punctuation\">;</span> i<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span>   \n        <span class=\"token function\">printf</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"%d: A+B = %d\\n\"</span><span class=\"token punctuation\">,</span> i<span class=\"token punctuation\">,</span> ret<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n    <span class=\"token function\">cudaFree</span><span class=\"token punctuation\">(</span>ret<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token keyword\">return</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span>\n<span class=\"token punctuation\">}</span></code></pre></div>\n<h3>difference between UVA and unified memory</h3>\n<ul>\n<li>Unified memory depends on UVA.</li>\n<li>UVA does NOT move data automatically between CPU and GPU.</li>\n<li>Unified memory gives higher performance than UVA.</li>\n</ul>\n<h3>pros and cons of unified memory</h3>\n<h4>advantages</h4>\n<ul>\n<li>Ease of programming</li>\n<li>Data is migrated on demand\n– offer the performance of local data on the GPU\n– while providing the ease of use of globally shared data</li>\n<li>Very efficient with complex data structures (e.g. linked lists, structures with pointers, … ).</li>\n</ul>\n<h4>disadvantage</h4>\n<p>Carefully tuned CUDA program that uses streams to efficiently overlap execution with data transfers may perform better than a CUDA program that only uses Unified Memory.</p>"
}
