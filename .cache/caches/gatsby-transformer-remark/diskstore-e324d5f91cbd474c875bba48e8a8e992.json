{
  "expireTime": 9007200830565980000,
  "key": "transformer-remark-markdown-html-84cd8335395c0f4438e169e9ca6ec260-gatsby-remark-imagesgatsby-remark-responsive-iframegatsby-remark-prismjsgatsby-remark-copy-linked-filesgatsby-remark-smartypantsgatsby-remark-reading-time-",
  "val": "<p>This is the second post in a series about what I learnt in my GPU class at NYU this past fall. This will be mostly about warps, why using warps from a SIMD hardware standpoint, and how warps can be a dangerous thing to deal with.</p>\n<h2>Table of Contents</h2>\n<ol>\n<li>\n<p>More About Threads</p>\n<ol>\n<li>Kernel threads hierarchy</li>\n<li>Synchronization of threads?</li>\n</ol>\n</li>\n<li>\n<p>Warp</p>\n<ol>\n<li>practical reasons</li>\n<li><strong>SIMD hardware</strong></li>\n<li>practical reasons to share control unit amongst processing units</li>\n</ol>\n</li>\n<li>\n<p>How Are Blocks Partitioned?</p>\n<ol>\n<li>1D block</li>\n<li>2D block</li>\n<li>3D block</li>\n</ol>\n</li>\n<li>\n<p>Warp Execution</p>\n<ol>\n<li>when is it good?</li>\n<li>when is it bad? <strong>thread divergence</strong></li>\n<li>multipass aproach &#x26; divergence</li>\n<li>if-else example</li>\n<li>for-loop example</li>\n<li>other scenarios</li>\n</ol>\n</li>\n</ol>\n<h2>More About Threads</h2>\n<h3>Kernel threads hierarchy</h3>\n<p>Recall that launching a CUDA kernel will generate a grid of threads organized as a <strong>two-level</strong> hierarchy.</p>\n<ol>\n<li>\n<p>top level: a 1/2/3-dimensional array of blocks.</p>\n</li>\n<li>\n<p>bottom level: each block consists of a 1/2/3-dimensional array of threads.</p>\n</li>\n</ol>\n<h3>Synchronization of threads?</h3>\n<p>Conceptually, threads in a block can execute in any order, just like blocks.</p>\n<p>When an algorithm needs to execute in <em>phases</em>, <strong>barrier synchronizations</strong> should be used to ensure that all threads have completed a common phase before they start the next one.</p>\n<p>But the correctness of executing a kernel should not depend on the synchrony amongst threads.</p>\n<h2>Warp</h2>\n<p>Due to hardware cost considerations, CUDA devices currently bundle multiple threads for execution, which leads to performance limitations.</p>\n<ul>\n<li>\n<p>üßê<strong>Each thread block is partitioned into warps</strong>.</p>\n</li>\n<li>\n<p>The execution of warps is implemented by an SIMD hardware.</p>\n</li>\n</ul>\n<h4>practical reasons</h4>\n<p>This implementation (doesn‚Äôt just exist to annoy programmers) helps:</p>\n<ol>\n<li>\n<p>reducing hardware manufacturing cost</p>\n</li>\n<li>\n<p>lower run0time operation electricity cost</p>\n</li>\n<li>\n<p>enable coalescing of memory accesses (which will be the topic of some later post)</p>\n</li>\n</ol>\n<h4>SIMD hardware</h4>\n<p><a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/dd7852cc44b3614e54dbf83a6304f750/bb5d7/processor.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-wrapper\"\n    style=\"position: relative; display: block;  max-width: 590px; margin-left: auto; margin-right: auto;\"\n  >\n    <span\n      class=\"gatsby-resp-image-background-image\"\n      style=\"padding-bottom: 65.51155115511551%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAABYlAAAWJQFJUiTwAAACDElEQVQ4y21T2ZLaQAzk/z9nK8kmu7BcAZbl8jFg4wNMFbzYYK4qjqeOWsSbEPIgjy1pWq2WXLper9jvD2rb7Q55vlXbbHL9PhyO/43tdnuN8SzscrmgROd8vkAQRHAcg+k0hO8HCMMYtuWiXm+i3X6H6441hzGeo5GNXm8Ay3IwHFqaz6IlVve8qYINBiOMx55e5oVatY5vX5/xVqnpZfpt21X7+Oij0+nqnX5/qEXyXABJ37aNAhVgPBuNpiQOEIWRsA7gTTwp7GMiZxzP9DRmomRYII7nKkeJDzrJgNTJjNUtOanv6XRCmqZYrVZYJAlWy6XotUOSLLR9ykUCD4AvL2U8PX1BufwmujWklQG2eY4szQRoIYPZK7BxDcIglBZvWhOITB8Au92eMlOWYo7jIssyHI9HZXcQwL0wC6T9pbAMVYoQs1nyCDgRQM+fwgj1MJohkIndtJyoBKORJQN4F60c9RkzFg19NQLdtbxeb7QC6ZMpjYn0VWXKtEqlitbPjsaM8TTX90O4v4fC/EiIKGCWbT7FJX0a2242W5qs/nmil1qtjq4Xc4rd/QQsGHJtSPnWwm11uAY90ZO6DmXPWKAjy90WwGKtmM93XzS9Y8gHK1HgwhgkKBf69bWCWq2B788/tH0y+TuXfwgLkLUCns9nnWaa3luWrfWfpcbr9Z/3f/NojHM3ifULGb2fyrgSzdgAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n    ></span>\n    <img\n        class=\"gatsby-resp-image-image\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;box-shadow:inset 0px 0px 0px 400px white;\"\n        alt=\"processor\"\n        title=\"\"\n        src=\"/static/dd7852cc44b3614e54dbf83a6304f750/d7711/processor.png\"\n        srcset=\"/static/dd7852cc44b3614e54dbf83a6304f750/a695b/processor.png 148w,\n/static/dd7852cc44b3614e54dbf83a6304f750/2f273/processor.png 295w,\n/static/dd7852cc44b3614e54dbf83a6304f750/d7711/processor.png 590w,\n/static/dd7852cc44b3614e54dbf83a6304f750/5e66f/processor.png 885w,\n/static/dd7852cc44b3614e54dbf83a6304f750/466de/processor.png 1180w,\n/static/dd7852cc44b3614e54dbf83a6304f750/bb5d7/processor.png 1212w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n      />\n  </span>\n  </a></p>\n<ul>\n<li>\n<p>The processor has only <em>one</em> <strong>control unit</strong> that fetches and decodes instruction.</p>\n</li>\n<li>\n<p>The same control signal goes to <em>multiple</em> <strong>processing units</strong>, each of which executes one of the threads in a warp.</p>\n</li>\n<li>\n<p>Since all <strong>processing units</strong> are controlled by the same instruction, their execution differences are due to the different operands in the <strong>register files</strong>.</p>\n</li>\n</ul>\n<p>This is called Single-Instruction-Multiple-Data in processor design.</p>\n<h5>practical reasons to share control unit amongst processing units</h5>\n<ul>\n<li>\n<p>Control units are quite complex:</p>\n<ul>\n<li>\n<p>sophisticated logic for fetching instructions</p>\n</li>\n<li>\n<p>access ports to the instruction memory</p>\n</li>\n<li>\n<p>on-chip instruction caches to reduce the latency of instruction fetch.</p>\n</li>\n</ul>\n</li>\n<li>\n<p>Having multiple processing units share a control unit can result in significant reduction in hardware manufacturing cost and power consumption.</p>\n</li>\n</ul>\n<h2>How Are Blocks Partitioned?</h2>\n<p>Based on thread indices. Thread IDs within a warp are consecutive and increasing.</p>\n<ol>\n<li>\n<p>Example with <em>1D thread block</em>:</p>\n<ul>\n<li>\n<p>only threadIdx.x is used, threadIdx.x values within a warp are consecutive and increasing.</p>\n</li>\n<li>\n<p>for a warp size of 32:</p>\n</li>\n<li>\n<p>warp 0: thread 0 ~ thread 31</p>\n</li>\n<li>\n<p>warp 1: thread 32 ~ thread 63</p>\n</li>\n<li>\n<p>warp n: thread 32 √ó n ~ thread 32(n + 1) - 1</p>\n</li>\n<li>\n<p>for a block of which the size is not a multiple of 32:</p>\n</li>\n<li>\n<p>the last warp will be <strong>padded with extra threads</strong> to fill up the 32 threads.</p>\n</li>\n</ul>\n</li>\n<li>\n<p><em>2D thread block</em>:</p>\n<ul>\n<li>the dimensions will be <em>projected into a linear order</em> <strong>before</strong> partitioning into warps</li>\n</ul>\n<p><a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/1b336777349cc0288199bf1472bf368c/f6b36/2D.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-wrapper\"\n    style=\"position: relative; display: block;  max-width: 590px; margin-left: auto; margin-right: auto;\"\n  >\n    <span\n      class=\"gatsby-resp-image-background-image\"\n      style=\"padding-bottom: 49.479166666666664%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAABYlAAAWJQFJUiTwAAAB+UlEQVQoz32Sy2tTQRSHxyLiQhe6kyLYxLQWi48/wY1QEP+Agi7cdFOJq+qiK6la3ChUDD7y2CiIYCiCj4WF4KO2AUVFTaBJm9uY9BHyvHndeyefc29aTG31wMecYc785jxGsG6tVsthw7fNtEroxivFS8VrxXMa1hcV4ETReXfDxPZi0vHXshHS0R0szQmWol1oM4Lc/BCyZcdZSCm3Cm6XmWUYVBS59+9Y3reHzN7dZNSa7RKsXjjPT01jPh4nkUjQbDY33RVbyjRNCvmC489GvnFy1yUGxEVO7PTiFsOMDgVZWcuQTC6gKeFarbZZsHNjqKxKpRLWeinTnz4gTrsUBxFnDiFO7efsxAh/W2dSKkPp9KzRqCqxvOqPoU4s6uUq2mqCqy9GufJ0hMtTXrzhYZ58fuTMo2mYGKahBmdiyjaWtNoZNhpQrf550X5C53/W+ueJ0PUY2eVbSjBEpRxQwwiSm/ZRnLyHHgpS9j+gEvTz+PYz7j6cIRD4iM/3lsk7b7gfCRCMBgjNhfDP+gl/DSOKxSkWF7tJpw+TSrpZyLtIj/WycqCPtMdF1uOhcPwY54546XbfxOMeV1yn7+g1PGMD9N5w0z/RT894D4O+QbtkiaX6IGUHqjf21zHqdX6pScZ+fCcWi5NKaVQquhqe2UbFdWL38zfzOrIVqOrk/wAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n    ></span>\n    <img\n        class=\"gatsby-resp-image-image\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;box-shadow:inset 0px 0px 0px 400px white;\"\n        alt=\"linearized2D\"\n        title=\"\"\n        src=\"/static/1b336777349cc0288199bf1472bf368c/d7711/2D.png\"\n        srcset=\"/static/1b336777349cc0288199bf1472bf368c/a695b/2D.png 148w,\n/static/1b336777349cc0288199bf1472bf368c/2f273/2D.png 295w,\n/static/1b336777349cc0288199bf1472bf368c/d7711/2D.png 590w,\n/static/1b336777349cc0288199bf1472bf368c/5e66f/2D.png 885w,\n/static/1b336777349cc0288199bf1472bf368c/466de/2D.png 1180w,\n/static/1b336777349cc0288199bf1472bf368c/f6b36/2D.png 1536w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n      />\n  </span>\n  </a></p>\n<ul>\n<li>determine the linear order: place the rows with larger y and z coordinates after those with lower ones.</li>\n</ul>\n</li>\n<li>\n<p><em>3D thread block</em>:</p>\n<ul>\n<li>\n<p>first place all threads of which the threadIdx.z value is 0 into the linear order. Among these threads, they are treated as a 2D block as shown in above picture</p>\n</li>\n<li>\n<p>example: a 3D thread block of dimensions 2 √ó 8 √ó 4</p>\n</li>\n<li>\n<p>total 64 threads</p>\n</li>\n<li>\n<p>warp 0: T(0,0,0) ~ T(0,7,3)</p>\n</li>\n<li>\n<p>warp 1: T(1,0,0) ~ T(1,7,3)</p>\n</li>\n</ul>\n</li>\n</ol>\n<h2>Warp Execution</h2>\n<p>All threads of a warp are executed by the SIMD hardware as a bundle, where the same instruction is run for all threads.</p>\n<h3>when is it good?</h3>\n<p>When all threads within a warp follow the same control flow.</p>\n<p>For example, for an if-else construct, the execution works well when either all threads execute the if part or all execute the else part.</p>\n<h3>when is it bad?</h3>\n<p>When threads within a warp take different control flow paths, the SIMD hardware will take <em>multiple passes</em> through these divergent paths. During each pass, the threads that follow the other path are not allowed to take effect.</p>\n<p>These passes are <em>sequential</em> to each other, thus they will add to the execution time.</p>\n<h4>multipass aproach &#x26; divergence</h4>\n<ul>\n<li>\n<p>The <strong>multipass approach</strong> to <strong>divergent warp execution</strong> extends the SIMD hardware‚Äôs ability to implement the full semantics of CUDA threads. While the hardware executes the same instruction for all threads in a warp, it selectively lets the threads take effect in each pass only, allowing every thread to take its own control flow path. This preserves the independence of threads while taking advantage of the reduced cost of SIMD hardware.</p>\n</li>\n<li>\n<p>When threads <em>in the same warp</em> follow different paths of control flow, we say that these threads <strong>diverge</strong> in their execution.</p>\n</li>\n</ul>\n<h4>if-else example</h4>\n<p>In the if-else example, divergence arises if some threads in a warp take the then path and some the else path. The cost of divergence is the <em>extra pass</em> the hardware needs to take to allow the threads in a warp to make their own decisions.</p>\n<h4>for-loop example</h4>\n<p>If threads in a warp execute a for loop that can iterate <strong>six, seven, or eight</strong> times for different threads:</p>\n<ul>\n<li>All threads will finish the first six iterations together.</li>\n<li>Two passes will be used to execute the seventh iteration, one for those that take the iteration and one for those that do not.</li>\n<li>Two passes will be used to execute the eighth iteration, one for those that take the iteration and one for those that do not.</li>\n</ul>\n<h4>other scenarios</h4>\n<p>In terms of source statements, a control construct can result in thread divergence when its decision condition is based on threadIdx values.</p>\n<p>For example, the statement <code class=\"language-text\">if (threadIdx.x. &gt; 2) {}</code> causes the threads to follow two divergent control flow paths. Threads 0, 1, and 2 follow a different path than threads 3, 4, 5, etc.</p>\n<p>Similarly, a loop can cause thread divergence if its loop condition is based on thread index values.</p>"
}
