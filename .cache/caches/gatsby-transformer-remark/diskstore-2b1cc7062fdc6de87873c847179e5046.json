{
  "expireTime": 9007200830440491000,
  "key": "transformer-remark-markdown-html-a8ac11cb49c5f67a789a80ba77d7f942-gatsby-remark-imagesgatsby-remark-responsive-iframegatsby-remark-prismjsgatsby-remark-copy-linked-filesgatsby-remark-smartypantsgatsby-remark-reading-time-",
  "val": "<p>This is the second post in a series about what I learnt in my GPU class at NYU this past fall. This will be mostly about <strong>warps and SIMD hardward</strong>.</p>\n<h2>Kernel threads hierarchy</h2>\n<p>Recall that launching a CUDA kernel will generate a grid of threads organized as a <strong>two-level</strong> hierarchy.</p>\n<ol>\n<li>\n<p>top level: a 1/2/3-dimensional array of blocks.</p>\n</li>\n<li>\n<p>bottom level: each block consists of a 1/2/3-dimensional array of threads.</p>\n</li>\n</ol>\n<h2>Synchronize threads?</h2>\n<p>Conceptually, threads in a block can execute in any order, just like blocks.</p>\n<p>When an algorithm needs to execute in <em>phases</em>, <strong>barrier synchronizations</strong> should be used to ensure that all threads have completed a common phase before they start the next one.</p>\n<p>But the correctness of executing a kernel should not depend on the synchrony amongst threads.</p>\n<h2>Warp</h2>\n<p>Due to hardware cost considerations, CUDA devices currently bundle multiple threads for execution, which leads to performance limitations.</p>\n<ul>\n<li>\n<p>üßê<strong>Each thread block is partitioned into warps</strong>. </p>\n</li>\n<li>\n<p>The execution of warps is implemented by an SIMD hardware. </p>\n</li>\n</ul>\n<h4>practical reasons</h4>\n<p>This implementation (doesn‚Äôt just exist to annoy programmers) helps:</p>\n<ol>\n<li>\n<p>reducing hardware manufacturing cost</p>\n</li>\n<li>\n<p>lower run0time operation electricity cost</p>\n</li>\n<li>\n<p>enable coalescing of memory accesses (which will be the topic of some later post)</p>\n</li>\n</ol>\n<h4>SIMD hardware</h4>\n<ul>\n<li>\n<p>The processor has only <em>one</em> <strong>control unit</strong> that fetches and decodes instruction.</p>\n</li>\n<li>\n<p>The same control signal goes to <em>multiple</em> <strong>processing units</strong>, each of which executes one of the threads in a warp. </p>\n</li>\n<li>\n<p>Since all <strong>processing units</strong> are controlled by the same instruction, their execution differences are due to the different operands in the <strong>register files</strong>.</p>\n</li>\n</ul>\n<p>This is called Single-Instruction-Multiple-Data in processor design.</p>"
}
