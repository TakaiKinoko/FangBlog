{
  "expireTime": 9007200830432484000,
  "key": "transformer-remark-markdown-html-793533645850a55f8f3f6d2e2dbede31-gatsby-remark-imagesgatsby-remark-responsive-iframegatsby-remark-prismjsgatsby-remark-copy-linked-filesgatsby-remark-smartypantsgatsby-remark-reading-time-",
  "val": "<h2>TABLE OF CONTENTS</h2>\n<ol>\n<li>\n<p>Concepts</p>\n<ol>\n<li>key abstractions</li>\n<li>granularity</li>\n</ol>\n</li>\n<li>\n<p>CUDA Architecture</p>\n<ol>\n<li>kernel execution</li>\n<li>thread organization</li>\n<li>blocks</li>\n<li>SMs</li>\n<li>warp</li>\n<li>execution picture</li>\n</ol>\n</li>\n<li>Thread ID</li>\n<li>Memory Hierarchy</li>\n</ol>\n<h2>Some Concepts</h2>\n<h3>three key abstractions</h3>\n<ul>\n<li>\n<p>a hierarchy of thread groups</p>\n</li>\n<li>\n<p>shared memories</p>\n</li>\n<li>\n<p>barrier synchronization</p>\n</li>\n</ul>\n<h3>granularity</h3>\n<ul>\n<li>\n<p>In parallel computing, granularity means the amount of <strong>computation</strong> in relation to <strong>communication (or transfer) of data</strong>.</p>\n<ul>\n<li>\n<p>fine-grained: individual tasks are small in terms of code size and execution time.</p>\n</li>\n<li>\n<p>coarse-grained: larger amounts of computation, infrequent data communication.</p>\n</li>\n</ul>\n</li>\n<li>\n<p>CUDA abstraction:</p>\n<ul>\n<li>\n<p><strong>fine-grained</strong> data parallelism and thread parallelism nested within <strong>roarse-grained</strong> data parallelism and task parallelism.</p>\n</li>\n<li>\n<p>programmers partition the problem into <strong>coarse sub-problems</strong> that can be solved independently in prallel by <strong>blocks of threads</strong> and each sub-problem into finer pieces that can be solved cooperatively in parallel by <strong>threads within the block</strong>.</p>\n</li>\n</ul>\n</li>\n</ul>\n<h2>CUDA Architecture</h2>\n<h3>kernel execution</h3>\n<ul>\n<li>\n<p>Executed in parallel by an array of threads, all of which run the same code.</p>\n</li>\n<li>\n<p>Each thread has an ID which is used to compute memory addresses and make control decisions.</p>\n</li>\n</ul>\n<h3>thread organization - grid of blocks</h3>\n<ul>\n<li>\n<p>Threads are arranged as a <strong>grid</strong> of <strong>blocks</strong>.</p>\n</li>\n<li>\n<p>Blocks in a grid are completely <strong>independent</strong> which means they can be executed in any order, in parallel or in series.</p>\n</li>\n<li>\n<p>The independence allows thread blocks to be scheduled in <em>any order</em> across <em>any number of cores</em>.</p>\n</li>\n</ul>\n<h3>block</h3>\n<ul>\n<li>\n<p>Threads from the same block have access to a <strong>shared memory</strong> .</p>\n</li>\n<li>\n<p>Execution of threads from the same block can be <strong>synchronized</strong> (to coordinate memory accesses).</p>\n</li>\n</ul>\n<h3>SMs</h3>\n<p>The CUDA architecture is built around a scalable array of multithreaded Streaming Multiprocessors. </p>\n<p>Each SM has:</p>\n<ul>\n<li>\n<p>a set of execution units</p>\n</li>\n<li>\n<p>a set of registers </p>\n</li>\n<li>\n<p>a chunch of shared memory.</p>\n</li>\n</ul>\n<h3>üßêwarp</h3>\n<p>Warp is the <strong>basic unit of execution</strong> in an NVIDIA GPU.</p>\n<p>A warp is a collection of threads, 32 in current NVIDIA implementations. </p>\n<ul>\n<li>\n<p>threads within a warp a executed simultaneously by an SM. </p>\n</li>\n<li>\n<p>multiple warps can be executed on an SM at once.</p>\n</li>\n</ul>\n<p>The mapping between warps and thread blocks can affect the performance of the kernel.\n<strong>It‚Äôs usually good the keep the size of a block a multiple of 32</strong>.</p>\n<h3>picture the process of execution</h3>\n<ol>\n<li>\n<p>CUDA program on the host CPU invokes a <strong>kernel grid</strong></p>\n</li>\n<li>\n<p>blocks in the grid are enumerated and distributed to SMs with available execution capacity</p>\n</li>\n<li>\n<p>the threads of a block execute concurrently on one SM </p>\n</li>\n<li>\n<p>as thread blocks terminate, new blocks are launched on the vacated SMs</p>\n</li>\n</ol>\n<h2>Thread ID</h2>\n<p>TODO</p>\n<h2>Memory Hierarchy</h2>\n<h3>between CPU and GPU</h3>\n<p>CPU and GPU has <strong>separate memory spaces</strong> => data must be moved from CPU to GPU before computation starts, as well as moved back to CPU once processing has completed.</p>\n<h3>global memory</h3>\n<ol>\n<li>\n<p>accessible to all threads as well as the host (CPU)</p>\n</li>\n<li>\n<p><strong>host</strong> allocate and deallocate the global memory</p>\n</li>\n<li></li>\n</ol>"
}
